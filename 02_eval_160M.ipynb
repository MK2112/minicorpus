{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking Pythia $160\\text{M}$ pre-trained on The Pile vs. Pythia $160\\text{M}$ trained on MiniPile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objectives:\n",
    "- [x] Prepare (Download) two models - **Pythia $160M$ Untrained** and **Pythia $160M$ fully Pile-trained**\n",
    "- [x] Load MiniPile Dataset from disk\n",
    "- [x] Train **Pythia $160M$ Untrained** on MiniPile (according to the MiniPile paper) *and save the model* (`pythia160m_minipile_trained`)\n",
    "- [x] Evaluate the performance of **Pythia $160M$ Pile-trained** on MMLU, ARC-Challenge benchmarks\n",
    "- [x] Evaluate the performance of **Pythia $160M$ Untrained** on MMLU, ARC-Challenge benchmarks\n",
    "\n",
    "I extended the benchmark efforts to make results more reliable, representative and nuanced. Models from here on will be evaluated on the following benchmarks:<br>\n",
    "ARC-Challenge, MMLU, HellaSwag, WinoGrande, Lambada (OpenAI) acc+perplexity, Lambada (Std) acc+perplexity, BLiMP, and for 1.4B models we also additionally evaluate on ARC-Easy.\n",
    "\n",
    "To avoid confusion regarding model naming, I use the updated nomenclature for the models as per:\n",
    "\n",
    "![](./img/pythia_nomenclature.png)<br>\n",
    "Source: [Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling (Biderman, et al. 2023) (p. 18)](https://arxiv.org/abs/2304.01373)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the now downloaded MiniPile, we can proceed with the training and evaluation of a Pythia 160M decoder-only model.<br>\n",
    "The model version `pythia-160m-deduped`, which we will use here, has already been trained on the deduplicated Pile. This allows us to skip the large-scale training on the deduplicated Pile.<br>\n",
    "The Pythia 160M model series has been released with intermediary checkpoints throughout.<br>\n",
    "Therefore, we also have access to a fully untrained Pythia 160M, which we will use to train on MiniPile, for direct comparison.\n",
    "\n",
    "To compare the two trained Pythia 160M models, we will setup and use the [EleutherAI LM Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install transformers datasets torch accelerate evaluate wandb\n",
    "#! pip install lm-eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from torch.optim import Adam\n",
    "from datasets import load_dataset\n",
    "from lm_eval import tasks, evaluator, utils\n",
    "from huggingface_hub import snapshot_download\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, get_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/vol/tmp/koppelmm\"\n",
    "base_path = Path(base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Download Pythia $160\\text{M}$ Untrained and Pythia $160\\text{M}$ Pile-Trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to download the two Pythia 160M models: The empty one for training on MiniPile, the fully Pile-trained one for benchmark comparison.<br>\n",
    "To avoid the same \"default directory download\" behavior from HuggingFace as in the previous notebook, we will use another custom download function, again based on the `snapshot_download` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_model(down_dir: str, target_folder: str, cache_folder: str, repo_id: str, branch: str = \"main\") -> None:\n",
    "    down_dir = Path(down_dir)\n",
    "    target_dir = down_dir / target_folder\n",
    "    cache_dir = down_dir / cache_folder\n",
    "\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"Downloading {repo_id}/{branch}...\")\n",
    "\n",
    "    # Hinsight is 20/20, snapshot_download has a retry mechanism. \n",
    "    # I built that here myself. Works too.\n",
    "    while True:\n",
    "        try:\n",
    "            snapshot_download(repo_id,\n",
    "                              repo_type=\"model\",\n",
    "                              revision=branch,\n",
    "                              cache_dir=str(cache_dir),\n",
    "                              local_dir=str(target_dir))\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Download attempt failed: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading the untrained, initial Pythia 160M model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the untrained Pythia 160M model as clean slate\n",
    "download_model(down_dir=base_dir, target_folder=\"pythia160m_dedup_untrained\", \n",
    "               cache_folder=\"pythia160m_dedup_untrained_Cache\",\n",
    "               repo_id=\"EleutherAI/pythia-160m-deduped\", branch=\"step0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the Pythia 160M model, fully trained on Pile deduplicated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the Pythia 160M model trained on the Pile dataset\n",
    "# https://huggingface.co/EleutherAI/pythia-160m/blob/main/README.md states:\n",
    "# \"[...] final step 143000 corresponds exactly to the model checkpoint on the main branch of each model.\"\n",
    "download_model(down_dir=base_dir, target_folder=\"pythia160m_dedup_pile\", \n",
    "               cache_folder=\"pythia160m_dedup_pile_Cache\",\n",
    "               repo_id=\"EleutherAI/pythia-160m-deduped\", branch=\"main\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Load MiniPile from Disk\n",
    "\n",
    "We downloaded MiniPile with the previous Jupyter Notebook `01_get_piles.ipynb`.<br>\n",
    "I at this point expect the MiniPile dataset to have been downloaded, ready to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading minipile train + val splits from the local directory \n",
    "# https://stackoverflow.com/questions/77020278/how-to-load-a-huggingface-dataset-from-local-path\n",
    "# https://github.com/MK2112/mobileYOLOv3/blob/main/mobileyolov3-cocotext.ipynb\n",
    "# Split is named exactly like with the original dataset https://huggingface.co/datasets/JeanKaddour/minipile\n",
    "minipile_train = load_dataset(\"parquet\",\n",
    "                              data_files={\n",
    "                                  \"train\": str(base_path / \"MiniPile\" / \"data\" / \"train-*.parquet\"),\n",
    "                                  \"validation\": str(base_path / \"MiniPile\" / \"data\" / \"validation-*.parquet\"),\n",
    "                                  \"test\": str(base_path / \"MiniPile\" / \"data\" / \"test-*.parquet\")\n",
    "                              },\n",
    "                              cache_dir=str(base_path / \"MiniPile_Cache\"),\n",
    "                              split=\"train\")\n",
    "\n",
    "minipile_val = load_dataset(\"parquet\",\n",
    "                            data_files={\n",
    "                                \"train\": str(base_path / \"MiniPile\" / \"data\" / \"train-*.parquet\"),\n",
    "                                \"validation\": str(base_path / \"MiniPile\" / \"data\" / \"validation-*.parquet\"),\n",
    "                                \"test\": str(base_path / \"MiniPile\" / \"data\" / \"test-*.parquet\")\n",
    "                            },\n",
    "                            cache_dir=str(base_path / \"MiniPile_Cache\"),\n",
    "                            split=\"validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Hyperparameters for Pythia $160\\text{M}$ Untrained on MiniPile\n",
    "\n",
    "Training the Pythia 160M model on MiniPile requires a set of hyperparameters.<br>\n",
    "The [MiniPile paper](https://arxiv.org/abs/2304.08442) did train models, too.<br>\n",
    "But it was BERT and T5, not Pythia. We can't rely on training arguments being provided by the original paper. Instead, we have to scale them from the [Pythia paper](https://arxiv.org/abs/2304.01373).<br>\n",
    "\n",
    "**Training Parameters for Pythia $160\\text{M}$ for The deduplicated Pile (*not* MiniPile)**<br>\n",
    "See the [Pythia Paper](https://arxiv.org/abs/2304.01373) (p. 22) and the [Pythia GitHub](https://github.com/EleutherAI/pythia/blob/main/models/160M/pythia-160m-deduped.yml):\n",
    "\n",
    "![](./img/pythia_train_params.png)\n",
    "\n",
    "- Each model gets exposed to $299,892,736,000 \\approx 300B$ tokens through training ($\\approx 1.5$ epochs on The deduplicated Pile)\n",
    "- Batch size of $1024$ samples\n",
    "- Sequence length of $2048$\n",
    "- Adam optimizer with $\\beta_1 = 0.9$, $\\beta_2 = 0.95$, $\\epsilon = 1 \\times 10^{-8}$\n",
    "- Learning rates vary by model size:\n",
    "    - $70\\text{M}$ model:  $10.0 \\times 10^{-4}$\n",
    "    - $160\\text{M}$ model: $6.0 \\times 10^{-4}$\n",
    "    - $410\\text{M}$ model: $3.0 \\times 10^{-4}$\n",
    "    - $1.0\\text{B}$ model: $3.0 \\times 10^{-4}$\n",
    "    - $1.4\\text{B}$ model: $2.0 \\times 10^{-4}$\n",
    "    - $2.8\\text{B}$ model: $1.6 \\times 10^{-4}$\n",
    "    - $6.9\\text{B}$ model: $1.2 \\times 10^{-4}$\n",
    "    - $12\\text{B}$ model:  $1.2 \\times 10^{-4}$\n",
    "- train-iters $143,000$\n",
    "- lr-decay-iters $143,000$\n",
    "- lr-decay-style $\\text{cosine}$\n",
    "- lr-warmup $0.01$\n",
    "- weight-decay $0.01$\n",
    "- gradient-clipping $1.0$\n",
    "- lr-min $0.1 \\times$ `optimizer.params.lr` (i.e. the model-size-specific learning rate)\n",
    "- synchronize-each-layer $\\text{True}$ (i.e. gradients across all GPUs after each layer synced)\n",
    "- LR Scheduling: Decays to a minimum of $0.1\\times$ the maximum learning rate for all models\n",
    "- (Tokenizer is loaded as the same as for GPT-NeoX-20B)\n",
    "\n",
    "**Training Parameters for $160\\text{M}$ for MiniPile**<br>\n",
    "See [MiniPile paper](https://arxiv.org/abs/2304.08442)\n",
    "- $1M/500/10k$ training/validation/test examples\n",
    "    - Vocab size: $32,309,614$\n",
    "    - Median document length: $294$\n",
    "    - Longest document length: $929,633$\n",
    "\n",
    "For the sake of completeness, let's also watch the training arguments that the [MiniPile paper](https://arxiv.org/abs/2304.08442) used for training BERT and T5.<br>\n",
    "This can at least help validate our claim/idea that scaling down training arguments is generally on the right track:\n",
    "\n",
    "**BERT Training Parameters for MiniPile**\n",
    "- Adam, $\\beta_1 = 0.9$, $\\beta_2 = 0.98$, $\\epsilon = 1 \\times 10^{-12}$\n",
    "- weight-decay $0.001$\n",
    "- One cycle policy with peak learning rate of $1 \\times 10^{-3}$\n",
    "- gradient-clipping $0.5$\n",
    "- Progressive batch size from $128$ to $4096$ with a linear increase over the course of training up to $300\\text{k}$ steps, no warmup\n",
    "- $800\\text{k}$ total training steps\n",
    "- weight averaging of the $k = 5$ latest checkpoints and $1k$ steps distance between them\n",
    "\n",
    "**T5 Training Parameters for MiniPile**\n",
    "- AdamW, matrix-wise LR scaling by its root mean square (RMS), no weight decay\n",
    "- base learning rate $0.02$\n",
    "- cosine schedule with final of $1 \\times 10^{-5}$\n",
    "- gradient-clipping $1.0$\n",
    "- batch size $288$\n",
    "- $10\\text{k}$ warmup steps, $65536$ total training steps\n",
    "- weight averaging of the $k = 5$ latest checkpoints and $1k$ steps distance between them (akin to BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> There doesn't exist a clear description of a setup for training Pythia models on MiniPile.\n",
    "\n",
    "**We actually have two options to pin down best possible training arguments at this point:**\n",
    "- Find a most appropriate training configuration for the Pythia models to apply for MiniPile, e.g. from approaches trained solely on MiniPile following the decoder-only paradigm and available online, or\n",
    "- Apply the same training configuration as for the deduplicated Pile to MiniPile, but scaled.\n",
    "\n",
    "There exists a [GPT NeoX 122M MiniPile](https://huggingface.co/euclaise/gpt-neox-122m-minipile-digits) model that can be reverse-engineered for investigating the first option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training arguments from the minipile-trained decode reference model GPT-NeoX-122M:\n",
    "# https://huggingface.co/euclaise/gpt-neox-122m-minipile-digits\n",
    "\n",
    "# Newer versions fail for missing attributes, 4.30.0 is documented to have been used\n",
    "if str(transformers.__version__) == \"4.30.0\":\n",
    "    training_args = torch.load(base_path / 'training_args_gptNEO122m.bin', weights_only=False)\n",
    "    output_file = 'train_args_gptNEO122m_minipile.txt'\n",
    "    try: \n",
    "        with open(output_file, 'w') as f:\n",
    "            f.write(\"TrainingArguments attributes:\\n\")\n",
    "            for attr in dir(training_args):\n",
    "                if hasattr(training_args, attr) and not attr.startswith('_'):\n",
    "                    value = getattr(training_args, attr)\n",
    "                    f.write(f\"- {attr}: {value}\\n\")\n",
    "    except NameError as _:\n",
    "        pass # Fully ignore NameError, appears every time\n",
    "else:\n",
    "    print('Skipped for version mismatch.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file attained above is part of the repository and can be found in the base directory as `train_args_gptNEO122m_minipile.json`.\n",
    "\n",
    "The GPTNeoX model card seems to be a bit misleading, as it is stated that this model was trained exclusively on MiniPile.<br>\n",
    "The tiny learning rate $5 \\times 10^{-6}$ with no weight decay implies a fine-tuning approach, though.\n",
    "\n",
    "Because of this, and because of wanting to create most comparable training settings for different the different (i.e. Pile-trained, MiniPile-trained) Pythia models, we will opt for the second option and apply a scaled training configuration for the deduplicated Pile to MiniPile.\n",
    "\n",
    "Core parameters are however not directly transferable: `train-iters` and therefore also `lr-decay-iters`.<br>\n",
    "For Pile deduplicated this was $143,000$, but we have to scale this to the MiniPile dataset size, as the number of tokens processed by the model is crucial for the training process and could lead to overfitting if not adjusted properly.\n",
    "\n",
    "In other words, overshooting distorts knowledge retention qualities, while undershooting may equally lead to insufficient representation of the dataset's capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that both datasets purely consist of text, we can actually use the memory size as a proxy for the number of tokens in the dataset.<br>\n",
    "Based on that, we can estimate the scaling effect we have to apply to the `train-iters` parameter. This leaves us with a good starting point for the training configuration and the step count for training MiniPile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Byte-based scale factor: 139.609153x\n",
      "MiniPile (scaled) Train-Iters/LR-Decay-Iters: 1024.288 ~ 1024\n"
     ]
    }
   ],
   "source": [
    "# I use the byte sizes as proxy for the number of tokens, as both datasets will get tokenized with the same tokenizer\n",
    "minipile_train_bytes = 5906108510 # see https://huggingface.co/datasets/JeanKaddour/minipile/blob/main/README.md\n",
    "pile_train_bytes = 824546807506   # see https://huggingface.co/datasets/EleutherAI/the_pile_deduplicated/blob/main/dataset_infos.json\n",
    "pile_effective_epochs = 1.5       # this many epochs are actually trained in the original model (calculation isn't affected, training params below are)\n",
    "\n",
    "scale_factor = (pile_train_bytes * pile_effective_epochs) / (minipile_train_bytes * pile_effective_epochs)\n",
    "print(f\"Byte-based scale factor: {scale_factor:10.6f}x\")\n",
    "print(f\"MiniPile (scaled) Train-Iters/LR-Decay-Iters: {143000 / scale_factor:.3f} ~ {round(143000 / scale_factor)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $1024$ training steps may seem awkwardly small.<br>\n",
    "But, to reiterate, we strictly scaled it down according to dataset size difference.\n",
    "\n",
    "Additionally, thankfully, the training arguments state that the training step count for the deduplicated Pile was equal to 1.5 epochs on the dataset.<br>\n",
    "By testing on our training code base, we find that we hit this target with the scaled-down training step count for MiniPile.<br>\n",
    "So, the above memory-proxy step count approximation wasn't necessary, as the training code setup tells us the step count for 1.5 epochs automatically, but it's a good sanity check.\n",
    "\n",
    "By all means, we thoroughly neuter overall exposure to data when switching to MiniPile. This scale-correct limiting and overall lower exposure is exactly what we need here to operate relative to the original, large-scale Pythia training to compare knowledge retention and generalization capabilities achievable on `The Pile Deduplicated` vs. the 'distilled' `MiniPile` under size-appropriate, similar conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now lay out the complete parameters:<br>\n",
    "With the three approaches and their descriptions retrieved, we can take a more educated guess at the training params for Pythia $160\\text{M}$ on MiniPile:\n",
    "\n",
    "- Adam optimizer (GPT NeoX and T5-Base MiniPile suggest the 'generally more stable' AdamW, but Pythia uses Adam so we keep it most similar)\n",
    "    - $\\beta_1 = 0.9$, $\\beta_2 = 0.95$, (Pythia)\n",
    "    - $\\epsilon = 1 \\times 10^{-8}$ (GPT NeoX and Pythia)\n",
    "    - learning rate $6 \\times 10^{-4}$ (Pythia)\n",
    "    - lr-schedule $\\text{cosine annealing}$ (Pythia)\n",
    "    - lr-warmup $0.01$ of total steps (Pythia)\n",
    "    - lr-min $0.1 \\times \\text{lr}$ (Pythia)\n",
    "    - weight-decay $1 \\times 10^{-2}$ (Pythia)\n",
    "- gradient-clipping $1.0$ (Pythia)\n",
    "- batch size $1024$ (Pythia, grad accum needed, expect need for multi-GPU)\n",
    "- sequence length $2048$ (Pythia)\n",
    "- **train-iters: $1024$ (MiniPile-specific)**\n",
    "- **lr-decay-iters: same as train-iters (MiniPile-specific)**\n",
    "- (won't do mixed precision for sake of most similar training conditions to Pile-trained Pythia)\n",
    "- (won't do weight averaging)\n",
    "- **Same GPT-NeoX-20B tokenizer as for Pythia-Pile**\n",
    "\n",
    "We can start training Pythia $160\\text{M}$ on MiniPile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Train Pythia $160\\text{M}$ Untrained on MiniPile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by loading the Pythia-accompanying tokenizer and the initialized, untrained Pythia 160M model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the untrained Pythia 160M tokenizer and model\n",
    "# https://stackoverflow.com/questions/64001128/load-a-pre-trained-model-from-disk-with-huggingface-transformers\n",
    "# Tokenizer is in fact a GPTNeoXTokenizer, only has a fast version available\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_path / \"pythia160m_dedup_untrained\", use_fast=True, local_files_only=True)\n",
    "empty_model = AutoModelForCausalLM.from_pretrained(base_path / \"pythia160m_dedup_untrained\", local_files_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Pythia paper](https://arxiv.org/abs/2304.01373) utilizes a standard configuration where individual examples consist of up to $2048$ tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer doesn't have a pad token, use EOS as a substitute\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "def tokenize(example): \n",
    "    # seq_len = max_length = 2048 (as upper boundary, so not strict size -> no padding needed)\n",
    "    return tokenizer(example[\"text\"], \n",
    "                     truncation=True, \n",
    "                     max_length=2048,\n",
    "                     return_special_tokens_mask=True)\n",
    "\n",
    "if os.path.exists(base_path / \"minipile_train_tokenized\"):\n",
    "    minipile_train_tokenized = load_dataset(\"arrow\", data_files=str(base_path / \"minipile_train_tokenized/*.arrow\"), split=\"train\")\n",
    "    minipile_val_tokenized = load_dataset(\"arrow\", data_files=str(base_path / \"minipile_val_tokenized/*.arrow\"), split=\"train\")\n",
    "else:\n",
    "    minipile_train_tokenized = minipile_train.map(tokenize, batched=True, remove_columns=minipile_train.column_names) # retain only new fields from tokenization\n",
    "    minipile_val_tokenized = minipile_val.map(tokenize, batched=True, remove_columns=minipile_val.column_names)\n",
    "    minipile_train_tokenized.save_to_disk(base_path / \"minipile_train_tokenized\")\n",
    "    minipile_val_tokenized.save_to_disk(base_path / \"minipile_val_tokenized\")\n",
    "\n",
    "# Dynamic padding during training (mlm -> mask language model -> we're doing causal here)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As described above, we settled for these parameters to most closely mimic the paper's guidelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = str(base_path / \"pythia160m_minipile_trained\")\n",
    "log_dir = str(base_path / \"160m_minipile_logs\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# https://huggingface.co/docs/transformers/v4.46.0/en/main_classes/trainer#transformers.TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1.5,            # Since train_iters gets set, use num_train_epochs=1.5 like for The Pile\n",
    "    per_device_train_batch_size=4,   # Gives an effective batch size of 1024 after grad accum\n",
    "    per_device_eval_batch_size=4,    # Same as training batch size\n",
    "    gradient_accumulation_steps=256, # Achieve a batch size of 1024, this should be (total_batch // batch_size // device_count)\n",
    "    learning_rate=6e-4,              # Default Pythia 160M\n",
    "    weight_decay=0.01,               # Default Pythia 160M\n",
    "    max_steps=1024,                  # Adjusted for MiniPile (https://discuss.huggingface.co/t/how-does-max-steps-affect-the-number-of-samples-the-model-sees/69681)\n",
    "    lr_scheduler_type=\"cosine\",      # As per Pythia 160M paper\n",
    "    warmup_steps=int(0.01 * 1024),   # 1% of total steps for warmup\n",
    "    logging_dir=log_dir,\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,     # Frequency for evaluation during training\n",
    "    save_steps=1024,    # Save at the end of training\n",
    "    save_total_limit=1, # Only keep the most recent checkpoint\n",
    "    fp16=True,         # Not using mixed precision for comparable conditions\n",
    "    report_to=None,     # Noting this for later iterations, maybe set this as \"wandb\", \"tensorboard\" or smth\n",
    "    ddp_find_unused_parameters=False, # see https://discuss.pytorch.org/t/how-to-change-ddp-parameter-find-unused-parameters-true-to-false-during-training/130763\n",
    "    max_grad_norm=1.0,  # As per Pythia 160M paper\n",
    ")\n",
    "\n",
    "# Ensure training across multiple GPUs if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "empty_model = empty_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of completeness, the code below shows an earlier attempt of reproducing the learning rate scheduling of the Pythia paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(empty_model.parameters(), lr=training_args.learning_rate, betas=(0.9, 0.95), eps=1e-8, weight_decay=0.01)\n",
    "\n",
    "# Train Pythia 160M Untrained on MiniPile\n",
    "# https://huggingface.co/docs/transformers/v4.46.0/en/main_classes/trainer\n",
    "trainer = Trainer(model=empty_model,\n",
    "                  args=training_args,\n",
    "                  train_dataset=minipile_train_tokenized,\n",
    "                  eval_dataset=minipile_val_tokenized,\n",
    "                  data_collator=data_collator,\n",
    "                  optimizers=(optimizer, None))\n",
    "\n",
    "scheduler = get_scheduler(name=training_args.lr_scheduler_type,\n",
    "                          optimizer=optimizer,\n",
    "                          num_warmup_steps=training_args.warmup_steps,\n",
    "                          num_training_steps=training_args.max_steps)\n",
    "\n",
    "num_batches = len(trainer.get_train_dataloader())  # Number of batches\n",
    "total_training_steps = num_batches * training_args.gradient_accumulation_steps * int(training_args.num_train_epochs)\n",
    "\n",
    "# Training loop with manual minimum learning rate enforcement\n",
    "for epoch in range(int(training_args.num_train_epochs)):\n",
    "    with tqdm(total=total_training_steps, desc=f\"Training Epoch {epoch + 1}/{int(training_args.num_train_epochs)}\") as pbar:\n",
    "        for _, batch in enumerate(trainer.get_train_dataloader()):\n",
    "            trainer.training_step(trainer.model, batch)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            for param_group in optimizer.param_groups:\n",
    "                # Manually ... ensure lr doesn't go below min_lr (Pythia wants this)\n",
    "                param_group['lr'] = max(param_group['lr'], 0.1 * training_args.learning_rate)\n",
    "            pbar.update(1)\n",
    "\n",
    "trainer.save_model(str(base_path / \"pythia160m_minipile_trained\")) # Save the model weights\n",
    "tokenizer.save_pretrained(str(base_path / \"pythia160m_minipile_trained\")) # Save the tokenizer (don't know if needed, better save than sorry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the parameters fit and technically everything is in place, the hard-wired for loop for learning rate checking and boosting to the lower bound is functional, but questionable.\n",
    "\n",
    "I came across [this code example](https://github.com/kazuto1011/deeplab-pytorch/blob/master/libs/utils/lr_scheduler.py) and from there found [this other code example](https://github.com/tugstugi/pytorch-saltnet/blob/master/utils/lr_scheduler.py) and decided to wrap the `transformers` library's `get_linear_schedule_with_warmup` scheduler inside a custom pytorch Scheduler instead.\n",
    "\n",
    "This may have been overkill, it didn't resolve the need for a for-loop, but it externalized the learning rate scheduling and made it more readable, modular and adaptable for me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "class CosineSchedulerWithMinLR(_LRScheduler):\n",
    "    # Basically wrapping the get_cosing_schedule_with_warmup in a lower-bound setting\n",
    "    # Allows for Cosine Scheduling with a min_lr enforced\n",
    "    def __init__(self, optimizer, num_warmup_steps, num_training_steps, min_lr=6e-5):\n",
    "        self.min_lr = min_lr\n",
    "        self.base_scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer=optimizer,\n",
    "            num_warmup_steps=num_warmup_steps,\n",
    "            num_training_steps=num_training_steps\n",
    "        )\n",
    "        super().__init__(optimizer)\n",
    "        \n",
    "    def get_lr(self):\n",
    "        lrs = self.base_scheduler.get_lr()\n",
    "        return [max(lr, self.min_lr) for lr in lrs]\n",
    "        \n",
    "    def step(self):\n",
    "        self.base_scheduler.step()\n",
    "        super().step() # I stumbled over this. several times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training of Pythia $160\\text{M}$ on MiniPile was done using the `02_train_160M_minipile.py` script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Evaluate Pythia $160\\text{M}$ MiniPile vs. Pythia $160\\text{M}$ Pile-Trained on Benchmarks\n",
    "\n",
    "We go on and evaluate the zero-shot performance of the models on the following benchmarks:\n",
    "\n",
    "### AI2 Reasoning Challenge Set (ARC-Challenge)\n",
    "[Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge (Clark, et al. 2018)](https://arxiv.org/abs/1803.05457#)\n",
    "- grade-school science questions, require the model to apply reasoning efforts beyond pure information retrieval\n",
    "- testing specifically for reaction to exposure to new concepts, effectively checking cross-domain generalization\n",
    "- requires broader domain knowledge, reasoning capabilities and combinatory skills for responses\n",
    "- intended to help evaluate assessing whether a MiniPile-trained model retains *the flexibility and cross-domain generalization capabilities* of a Pile-trained model\n",
    "\n",
    "### AI2 Reasoning Easy Set (ARC-Easy)\n",
    "[Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge (Clark, et al. 2018)](https://arxiv.org/abs/1803.05457#)\n",
    "- structurally much alike the ARC-Challenge, but... easier\n",
    "- supposed to be a sort of baseline for simpler question-answering tasks\n",
    "- additional measure for evaluating the cross-task generalization capabilities\n",
    "\n",
    "### Massive Multitask Language Understanding (MMLU)\n",
    "[Measuring Massive Multitask Language Understanding (Hendrycks, et al. 2021)](https://arxiv.org/abs/2009.03300)\n",
    "- comprehensive, inputs/examples span $57$ different tasks\n",
    "- specifically testing across *all* of these subjects\n",
    "- intended to help evaluate assessing whether a MiniPile-trained model retains *specifically the width of knowledge* of The Pile-trained model\n",
    "\n",
    "### HellaSwag\n",
    "[HellaSwag: Can a Machine *Really* Finish Your Sentence (Zellers, et al. 2019)](https://aclanthology.org/P19-1472.pdf)\n",
    "- presenting a model with a situation, asking it to choose a most plausible continuation from four options\n",
    "- tests for context understanding, conversational capabilities and generalization\n",
    "- I've seen it being used in [Karpathy's tutorial series](https://youtu.be/l8pRSuU81PU?si=JtMDjEt8t674mCQp&t=12504), \n",
    "- find it interesting to check how much of a conversation model we have here or how much it could get confused from the rather open-ended questions with smaller datasets as basis\n",
    "\n",
    "### WinoGrande\n",
    "[WinoGrande: An Adversarial Winograd Schema Challenge at Scale (Sakaguchi, et al. 2019)](https://arxiv.org/abs/1907.10641)\n",
    "- testing the ability to determine pronouns based on 'common sense reasoning'\n",
    "- checks whether beyond dataset contents, the model can just as deeply perceive/learn language understanding and reasoning capabilities\n",
    "- helps investigate whether reducing MiniPile to most relevant data with \"topically reducing k-Means\" still retains an overall language understanding\n",
    "\n",
    "### Language Model Benchmark for Autoregressive Data Analysis (Lambada)\n",
    "[The LAMBADA dataset: Word prediction requiring a broad discourse context (Paperno, et al. 2016)](https://arxiv.org/abs/1606.06031)\n",
    "- evaluates a model's ability to predict a last word of some text, where generating the right word requires long-range context understanding\n",
    "- tests for context understanding, conversational capabilities and generalization\n",
    "- helps with calibrating the benchmark pipeline, because numbers were reported for Pythia $160M$ on this benchmark\n",
    "- this in turn gives more credibility and a better look into the MiniPile-trained Pythia's performance overall\n",
    "- maybe reducing dataset size can disturb long range context processing, this helps us evaluate that\n",
    "- using the OpenAI version for cross-referencability with the Pythia paper\n",
    "\n",
    "### Benchmark of Linguistic Minimal Pairs (BLiMP)\n",
    "[BLiMP: The Benchmark of Linguistic Minimal Pairs for English (Warstadt, et al. 2020)](https://arxiv.org/abs/1912.00582)\n",
    "- evaluates grammatical knowledge across $67$ distinct linguistic phenomena/subtasks in English\n",
    "- minimal pairs of sentences differing in grammaticality used to test fine-grained linguistic competence\n",
    "- helps indicate if a model is capable of distinguishing grammatical and ungrammatical sentences $\\rightarrow$ What degree of language understanding is retained?\n",
    "- useful for checking whether dataset reduction impacts capability to capture even subtle grammatical distinctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lm_eval import utils, simple_evaluate\n",
    "from lm_eval.models.huggingface import HFLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluation - Pythia 160M Trained on Pile\n",
    "\n",
    "# Showcase uses Pythia: https://colab.research.google.com/github/EleutherAI/lm-evaluation-harness/blob/main/examples/lm-eval-overview.ipynb\n",
    "# Only genuine doc seems to be (a mess): https://github.com/EleutherAI/lm-evaluation-harness/tree/main/docs\n",
    "\n",
    "# Load model and tokenizer\n",
    "pythia_pile = AutoModelForCausalLM.from_pretrained(base_path / \"pythia160m_dedup_pile\", local_files_only=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_path / \"pythia160m_dedup_untrained\", use_fast=True, local_files_only=True)\n",
    "pythia_pile = pythia_pile.to(device)\n",
    "\n",
    "# From the HuggingFace Data Views (allenai/ai2_arc, cais/mmlu, allenai/winogrande, Rowan/hellaswag):\n",
    "# MMLU: {'answer': 1, 'choices': ['0', '4', '2', '6'], 'question': 'Find the degree for the given field extension Q(sqrt(2), sqrt(3), sqrt(18)) over Q.', 'subject': 'abstract_algebra'}\n",
    "# ARC-C/E: {'id': 'Mercury_7175875', 'question': 'An astronomer observes that a planet rotates faster after a meteorite impact. Which is the most likely effect of this increase in rotation?', 'choices': {'text': ['Planetary density will decrease.', 'Planetary years will become longer.', 'Planetary days will become shorter.', 'Planetary gravity will become stronger.'], 'label': ['A', 'B', 'C', 'D']}, 'answerKey': 'C'}\n",
    "# Winogrande: {'sentence': \"Ian volunteered to eat Dennis's menudo after already having a bowl because _ despised eating intestine.\", 'option1': 'Ian', 'option2': 'Dennis', 'answer': '2'} (answer content not contained in test set)\n",
    "# HellaSwag: {'ind': 14, 'activity_label': 'Wakeboarding', 'ctx_a': 'A man is being pulled on a water ski as he floats in the water casually.', 'ctx_b': 'he', 'ctx': 'A man is being pulled on a water ski as he floats in the water casually. he', 'endings': ['mounts the water ski and tears through the water at fast speeds.', 'goes over several speeds, trying to stay upright.', 'struggles a little bit as he talks about it.', 'is seated in a boat with three other people.'], 'source_id': 'activitynet~v_-5KAycAQlC4', 'split': 'test', 'split_type': 'indomain', 'label': ''}\n",
    "# Lambada (OAI): {'text': 'In my palm is a clear stone, and inside it is a small ivory statuette. A guardian angel.\\n\\n\"Figured if you\\'re going to be out at night getting hit by cars, you might as well have some backup.\"\\n\\nI look at him, feeling stunned. Like this is some sort of sign. But as I stare at Harlin, his mouth curved in a confident grin, I don\\'t care about signs'}\n",
    "# BLiMP: {'sentence_good': 'Who should Derek hug after shocking Richard?', 'sentence_bad': 'Who should Derek hug Richard after shocking?', 'field': 'syntax', 'linguistics_term': 'island_effects', 'UID': 'adjunct_island', 'simple_LM_method': true, 'one_prefix_method': false, 'two_prefix_method': false, 'lexically_identical': true, 'pair_id': 0}\n",
    " \n",
    "batch_size_hflm = 1\n",
    "\n",
    "# Found that in here of all things: https://github.com/pytorch/ao/blob/e2301e9dba91fa962d673fdc3b3f0002856a3ba7/torchao/_models/_eval.py#L17-L22\n",
    "# https://github.com/EleutherAI/lm-evaluation-harness/blob/main/lm_eval/models/huggingface.py\n",
    "pythia_pile_hflm = HFLM(pretrained=pythia_pile,\n",
    "                        tokenizer=tokenizer,\n",
    "                        batch_size=batch_size_hflm)\n",
    "\n",
    "# Thankfully both MMLU and ARC (and way more) are available in the lm_eval.tasks module\n",
    "# https://github.com/EleutherAI/lm-evaluation-harness/blob/main/lm_eval/tasks/arc\n",
    "# https://github.com/EleutherAI/lm-evaluation-harness/tree/main/lm_eval/tasks/mmlu\n",
    "# Initially evaluator.evaluate looked promising, but I don't understand it and this works\n",
    "# Found simple_evaluate in https://github.com/EleutherAI/lm-evaluation-harness/blob/main/docs/interface.md\n",
    "# Winogrande is in fact 'winogrande_xl', see https://github.com/EleutherAI/lm-evaluation-harness/blob/main/lm_eval/tasks/winogrande/default.yaml\n",
    "results = simple_evaluate(model=pythia_pile_hflm,\n",
    "                          tasks=[\"arc_challenge\", \"mmlu\", \"winogrande\", \"hellaswag\", \"lambada\", \"blimp\"], # I have no idea how to inject pre-downloaded datasets here, I gave up on that\n",
    "                          num_fewshot=0,  # Pythia paper stated they used zero-shot\n",
    "                          batch_size=batch_size_hflm,\n",
    "                          device=\"cuda\",\n",
    "                          limit=None)\n",
    "\n",
    "# Save for reference (for proof of table below)\n",
    "with open('02_eval_160M_pretrained.txt', 'w') as f:\n",
    "    f.write(str(results))\n",
    "\n",
    "# Manually saved this to 02_eval_160M_pretrained_table.txt\n",
    "# This was nicely documented. Not. https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/eleuther_eval.py\n",
    "print(utils.make_table(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pro of this is that the processing is standardized\n",
    "- This works with Pythia models because the doc uses Pythia as use case example\n",
    "    - However, with this implementation I have no idea what's actually going on under the hood\n",
    "- How do we know this benchmarking pipeline actually benchmarks correctly?\n",
    "    - Utilize the reported Pythia Benchmarks from the paper and compare\n",
    "\n",
    "![](./img/pythia_paper_dedup_benchmarks.png)\n",
    "\n",
    "I initially reported to benchmark only on ARC-Challenge and MMLU. But, with the current setup in place, I wanted to use more benchmarks, specifically some of the ones mentioned in the Pythia paper to cross-reference whether the evaluation pipeline itself is correct. Also, if that were to be the case, we would build a more informationally speaking, rich benchmarking report. Additionally, big 1.4B models will report ARC-Easy results, too.\n",
    "\n",
    "The overlapping confidence intervals suggest that the performances on WinoGrande and ARC-Challenge are consistent with the reported results.<br>\n",
    "This is narrowly not the case for Lambada (OpenAI).<br>\n",
    "Note that the benchmarking approach used here also reports larger `stderrs`.<br>\n",
    "I suppose the differences come from different versions of the LM-Eval harness (I further assume EleutherAI used their own LM Eval harness to test their models) and thus processing differences. While plausible, this is by no way a definitive statement.\n",
    "\n",
    "Given a notable divergence of performance results only occuring for Lambada, I interpret the deviation as a result of the different LM-Eval harness versions used for benchmarking. We can go ahead and evaluate Pythia $160\\text{M}$ MiniPile with the same setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTNeoXForCausalLM were not initialized from the model checkpoint at /mnt/data/pythia160m_minipile_trained and are newly initialized: ['embed_out.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2024-11-18:18:11:33,482 WARNING  [huggingface.py:95] `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "2024-11-18:18:11:33,483 INFO     [huggingface.py:481] Using model type 'default'\n",
      "2024-11-18:18:11:33,497 WARNING  [huggingface.py:275] Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "2024-11-18:18:11:33,500 INFO     [evaluator.py:164] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2024-11-18:18:11:33,500 INFO     [evaluator.py:217] Using pre-initialized model\n",
      "2024-11-18:18:11:38,295 INFO     [__init__.py:459] The tag 'arc_ca' is already registered as a group, this tag will not be registered. This may affect tasks you want to call.\n",
      "2024-11-18:18:11:38,314 INFO     [__init__.py:459] The tag 'arc_ca' is already registered as a group, this tag will not be registered. This may affect tasks you want to call.\n",
      "2024-11-18:18:14:25,213 WARNING  [task.py:325] [Task: lambada_openai] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
      "2024-11-18:18:14:25,214 WARNING  [task.py:325] [Task: lambada_openai] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
      "2024-11-18:18:14:29,423 WARNING  [task.py:799] [Task: blimp_wh_vs_that_with_gap_long_distance] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:14:29,424 WARNING  [task.py:811] [Task: blimp_wh_vs_that_with_gap_long_distance] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:14:32,249 WARNING  [task.py:799] [Task: blimp_wh_vs_that_with_gap] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:14:32,250 WARNING  [task.py:811] [Task: blimp_wh_vs_that_with_gap] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:14:34,924 WARNING  [task.py:799] [Task: blimp_wh_vs_that_no_gap_long_distance] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:14:34,925 WARNING  [task.py:811] [Task: blimp_wh_vs_that_no_gap_long_distance] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:14:37,401 WARNING  [task.py:799] [Task: blimp_wh_vs_that_no_gap] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:14:37,402 WARNING  [task.py:811] [Task: blimp_wh_vs_that_no_gap] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:14:39,840 WARNING  [task.py:799] [Task: blimp_wh_questions_subject_gap_long_distance] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:14:39,841 WARNING  [task.py:811] [Task: blimp_wh_questions_subject_gap_long_distance] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:14:42,311 WARNING  [task.py:799] [Task: blimp_wh_questions_subject_gap] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:14:42,312 WARNING  [task.py:811] [Task: blimp_wh_questions_subject_gap] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:14:44,709 WARNING  [task.py:799] [Task: blimp_wh_questions_object_gap] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:14:44,710 WARNING  [task.py:811] [Task: blimp_wh_questions_object_gap] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:14:48,730 WARNING  [task.py:799] [Task: blimp_wh_island] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:14:48,731 WARNING  [task.py:811] [Task: blimp_wh_island] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:14:51,072 WARNING  [task.py:799] [Task: blimp_transitive] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:14:51,073 WARNING  [task.py:811] [Task: blimp_transitive] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:14:53,443 WARNING  [task.py:799] [Task: blimp_tough_vs_raising_2] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:14:53,444 WARNING  [task.py:811] [Task: blimp_tough_vs_raising_2] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:14:55,885 WARNING  [task.py:799] [Task: blimp_tough_vs_raising_1] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:14:55,886 WARNING  [task.py:811] [Task: blimp_tough_vs_raising_1] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:14:58,276 WARNING  [task.py:799] [Task: blimp_superlative_quantifiers_2] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:14:58,277 WARNING  [task.py:811] [Task: blimp_superlative_quantifiers_2] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:15:00,786 WARNING  [task.py:799] [Task: blimp_superlative_quantifiers_1] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:15:00,787 WARNING  [task.py:811] [Task: blimp_superlative_quantifiers_1] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:15:03,208 WARNING  [task.py:799] [Task: blimp_sentential_subject_island] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:15:03,209 WARNING  [task.py:811] [Task: blimp_sentential_subject_island] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:15:05,626 WARNING  [task.py:799] [Task: blimp_sentential_negation_npi_scope] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:15:05,628 WARNING  [task.py:811] [Task: blimp_sentential_negation_npi_scope] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:15:07,979 WARNING  [task.py:799] [Task: blimp_sentential_negation_npi_licensor_present] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:15:07,980 WARNING  [task.py:811] [Task: blimp_sentential_negation_npi_licensor_present] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:15:10,321 WARNING  [task.py:799] [Task: blimp_regular_plural_subject_verb_agreement_2] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:15:10,322 WARNING  [task.py:811] [Task: blimp_regular_plural_subject_verb_agreement_2] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:15:12,704 WARNING  [task.py:799] [Task: blimp_regular_plural_subject_verb_agreement_1] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:15:12,705 WARNING  [task.py:811] [Task: blimp_regular_plural_subject_verb_agreement_1] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:15:15,109 WARNING  [task.py:799] [Task: blimp_principle_A_reconstruction] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:15:15,110 WARNING  [task.py:811] [Task: blimp_principle_A_reconstruction] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:15:17,674 WARNING  [task.py:799] [Task: blimp_principle_A_domain_3] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:15:17,675 WARNING  [task.py:811] [Task: blimp_principle_A_domain_3] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:15:20,412 WARNING  [task.py:799] [Task: blimp_principle_A_domain_2] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:15:20,413 WARNING  [task.py:811] [Task: blimp_principle_A_domain_2] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:15:22,774 WARNING  [task.py:799] [Task: blimp_principle_A_domain_1] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:15:22,775 WARNING  [task.py:811] [Task: blimp_principle_A_domain_1] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:15:25,442 WARNING  [task.py:799] [Task: blimp_principle_A_case_2] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:15:25,443 WARNING  [task.py:811] [Task: blimp_principle_A_case_2] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:15:28,083 WARNING  [task.py:799] [Task: blimp_principle_A_case_1] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:15:28,084 WARNING  [task.py:811] [Task: blimp_principle_A_case_1] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:15:30,536 WARNING  [task.py:799] [Task: blimp_principle_A_c_command] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:15:30,537 WARNING  [task.py:811] [Task: blimp_principle_A_c_command] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:15:32,975 WARNING  [task.py:799] [Task: blimp_passive_2] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:15:32,976 WARNING  [task.py:811] [Task: blimp_passive_2] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:15:35,324 WARNING  [task.py:799] [Task: blimp_passive_1] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:15:35,325 WARNING  [task.py:811] [Task: blimp_passive_1] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:15:37,726 WARNING  [task.py:799] [Task: blimp_only_npi_scope] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:15:37,726 WARNING  [task.py:811] [Task: blimp_only_npi_scope] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:15:40,933 WARNING  [task.py:799] [Task: blimp_only_npi_licensor_present] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:15:40,934 WARNING  [task.py:811] [Task: blimp_only_npi_licensor_present] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:15:43,292 WARNING  [task.py:799] [Task: blimp_npi_present_2] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:15:43,293 WARNING  [task.py:811] [Task: blimp_npi_present_2] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:15:45,797 WARNING  [task.py:799] [Task: blimp_npi_present_1] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:15:45,798 WARNING  [task.py:811] [Task: blimp_npi_present_1] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:15:48,193 WARNING  [task.py:799] [Task: blimp_matrix_question_npi_licensor_present] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:15:48,194 WARNING  [task.py:811] [Task: blimp_matrix_question_npi_licensor_present] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:15:50,598 WARNING  [task.py:799] [Task: blimp_left_branch_island_simple_question] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:15:50,599 WARNING  [task.py:811] [Task: blimp_left_branch_island_simple_question] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:15:52,973 WARNING  [task.py:799] [Task: blimp_left_branch_island_echo_question] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:15:52,974 WARNING  [task.py:811] [Task: blimp_left_branch_island_echo_question] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:15:55,367 WARNING  [task.py:799] [Task: blimp_irregular_plural_subject_verb_agreement_2] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:15:55,369 WARNING  [task.py:811] [Task: blimp_irregular_plural_subject_verb_agreement_2] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:15:57,847 WARNING  [task.py:799] [Task: blimp_irregular_plural_subject_verb_agreement_1] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:15:57,848 WARNING  [task.py:811] [Task: blimp_irregular_plural_subject_verb_agreement_1] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:16:00,248 WARNING  [task.py:799] [Task: blimp_irregular_past_participle_verbs] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:16:00,249 WARNING  [task.py:811] [Task: blimp_irregular_past_participle_verbs] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:16:02,626 WARNING  [task.py:799] [Task: blimp_irregular_past_participle_adjectives] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:16:02,627 WARNING  [task.py:811] [Task: blimp_irregular_past_participle_adjectives] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:16:04,968 WARNING  [task.py:799] [Task: blimp_intransitive] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:16:04,969 WARNING  [task.py:811] [Task: blimp_intransitive] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:16:07,458 WARNING  [task.py:799] [Task: blimp_inchoative] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:16:07,459 WARNING  [task.py:811] [Task: blimp_inchoative] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:16:09,918 WARNING  [task.py:799] [Task: blimp_expletive_it_object_raising] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:16:09,919 WARNING  [task.py:811] [Task: blimp_expletive_it_object_raising] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:16:12,321 WARNING  [task.py:799] [Task: blimp_existential_there_subject_raising] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:16:12,322 WARNING  [task.py:811] [Task: blimp_existential_there_subject_raising] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:16:15,925 WARNING  [task.py:799] [Task: blimp_existential_there_quantifiers_2] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:16:15,926 WARNING  [task.py:811] [Task: blimp_existential_there_quantifiers_2] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:16:18,308 WARNING  [task.py:799] [Task: blimp_existential_there_quantifiers_1] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:16:18,309 WARNING  [task.py:811] [Task: blimp_existential_there_quantifiers_1] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:16:20,750 WARNING  [task.py:799] [Task: blimp_existential_there_object_raising] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:16:20,751 WARNING  [task.py:811] [Task: blimp_existential_there_object_raising] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:16:23,170 WARNING  [task.py:799] [Task: blimp_ellipsis_n_bar_2] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:16:23,171 WARNING  [task.py:811] [Task: blimp_ellipsis_n_bar_2] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:16:25,566 WARNING  [task.py:799] [Task: blimp_ellipsis_n_bar_1] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:16:25,567 WARNING  [task.py:811] [Task: blimp_ellipsis_n_bar_1] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:16:28,131 WARNING  [task.py:799] [Task: blimp_drop_argument] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:16:28,132 WARNING  [task.py:811] [Task: blimp_drop_argument] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:16:30,867 WARNING  [task.py:799] [Task: blimp_distractor_agreement_relative_clause] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:16:30,868 WARNING  [task.py:811] [Task: blimp_distractor_agreement_relative_clause] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:16:34,245 WARNING  [task.py:799] [Task: blimp_distractor_agreement_relational_noun] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:16:34,246 WARNING  [task.py:811] [Task: blimp_distractor_agreement_relational_noun] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:16:36,730 WARNING  [task.py:799] [Task: blimp_determiner_noun_agreement_with_adjective_1] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:16:36,731 WARNING  [task.py:811] [Task: blimp_determiner_noun_agreement_with_adjective_1] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:16:39,353 WARNING  [task.py:799] [Task: blimp_determiner_noun_agreement_with_adj_irregular_2] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:16:39,354 WARNING  [task.py:811] [Task: blimp_determiner_noun_agreement_with_adj_irregular_2] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:16:41,776 WARNING  [task.py:799] [Task: blimp_determiner_noun_agreement_with_adj_irregular_1] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:16:41,777 WARNING  [task.py:811] [Task: blimp_determiner_noun_agreement_with_adj_irregular_1] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:16:44,164 WARNING  [task.py:799] [Task: blimp_determiner_noun_agreement_with_adj_2] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:16:44,164 WARNING  [task.py:811] [Task: blimp_determiner_noun_agreement_with_adj_2] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:16:46,775 WARNING  [task.py:799] [Task: blimp_determiner_noun_agreement_irregular_2] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:16:46,776 WARNING  [task.py:811] [Task: blimp_determiner_noun_agreement_irregular_2] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:16:49,237 WARNING  [task.py:799] [Task: blimp_determiner_noun_agreement_irregular_1] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:16:49,238 WARNING  [task.py:811] [Task: blimp_determiner_noun_agreement_irregular_1] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:16:51,635 WARNING  [task.py:799] [Task: blimp_determiner_noun_agreement_2] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:16:51,636 WARNING  [task.py:811] [Task: blimp_determiner_noun_agreement_2] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:16:54,322 WARNING  [task.py:799] [Task: blimp_determiner_noun_agreement_1] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:16:54,323 WARNING  [task.py:811] [Task: blimp_determiner_noun_agreement_1] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:16:56,936 WARNING  [task.py:799] [Task: blimp_coordinate_structure_constraint_object_extraction] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:16:56,938 WARNING  [task.py:811] [Task: blimp_coordinate_structure_constraint_object_extraction] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:16:59,367 WARNING  [task.py:799] [Task: blimp_coordinate_structure_constraint_complex_left_branch] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:16:59,368 WARNING  [task.py:811] [Task: blimp_coordinate_structure_constraint_complex_left_branch] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:17:01,777 WARNING  [task.py:799] [Task: blimp_complex_NP_island] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:17:01,778 WARNING  [task.py:811] [Task: blimp_complex_NP_island] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:17:04,140 WARNING  [task.py:799] [Task: blimp_causative] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:17:04,141 WARNING  [task.py:811] [Task: blimp_causative] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:17:06,493 WARNING  [task.py:799] [Task: blimp_animate_subject_trans] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:17:06,494 WARNING  [task.py:811] [Task: blimp_animate_subject_trans] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:17:08,929 WARNING  [task.py:799] [Task: blimp_animate_subject_passive] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:17:08,930 WARNING  [task.py:811] [Task: blimp_animate_subject_passive] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:17:11,298 WARNING  [task.py:799] [Task: blimp_anaphor_number_agreement] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:17:11,299 WARNING  [task.py:811] [Task: blimp_anaphor_number_agreement] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:17:13,710 WARNING  [task.py:799] [Task: blimp_anaphor_gender_agreement] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:17:13,711 WARNING  [task.py:811] [Task: blimp_anaphor_gender_agreement] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:17:16,214 WARNING  [task.py:799] [Task: blimp_adjunct_island] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-11-18:18:17:16,215 WARNING  [task.py:811] [Task: blimp_adjunct_island] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-11-18:18:17:18,578 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_adjunct_island in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,579 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_anaphor_gender_agreement in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,580 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_anaphor_number_agreement in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,581 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_animate_subject_passive in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,583 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_animate_subject_trans in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,583 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_causative in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,584 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_complex_NP_island in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,585 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_coordinate_structure_constraint_complex_left_branch in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,585 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_coordinate_structure_constraint_object_extraction in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,586 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_determiner_noun_agreement_1 in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,587 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_determiner_noun_agreement_2 in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,588 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_determiner_noun_agreement_irregular_1 in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,589 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_determiner_noun_agreement_irregular_2 in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,589 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_determiner_noun_agreement_with_adj_2 in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,590 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_determiner_noun_agreement_with_adj_irregular_1 in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,591 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_determiner_noun_agreement_with_adj_irregular_2 in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,592 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_determiner_noun_agreement_with_adjective_1 in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,595 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_distractor_agreement_relational_noun in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,595 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_distractor_agreement_relative_clause in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,596 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_drop_argument in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,597 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_ellipsis_n_bar_1 in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,597 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_ellipsis_n_bar_2 in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,598 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_existential_there_object_raising in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,598 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_existential_there_quantifiers_1 in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,600 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_existential_there_quantifiers_2 in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,600 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_existential_there_subject_raising in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,601 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_expletive_it_object_raising in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,601 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_inchoative in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,602 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_intransitive in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,602 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_irregular_past_participle_adjectives in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,603 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_irregular_past_participle_verbs in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,604 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_irregular_plural_subject_verb_agreement_1 in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,604 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_irregular_plural_subject_verb_agreement_2 in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,605 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_left_branch_island_echo_question in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,605 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_left_branch_island_simple_question in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,606 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_matrix_question_npi_licensor_present in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,606 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_npi_present_1 in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,607 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_npi_present_2 in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,607 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_only_npi_licensor_present in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,608 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_only_npi_scope in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,609 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_passive_1 in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,609 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_passive_2 in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,610 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_principle_A_c_command in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,611 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_principle_A_case_1 in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,612 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_principle_A_case_2 in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,613 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_principle_A_domain_1 in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,614 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_principle_A_domain_2 in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,614 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_principle_A_domain_3 in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,615 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_principle_A_reconstruction in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,616 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_regular_plural_subject_verb_agreement_1 in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,617 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_regular_plural_subject_verb_agreement_2 in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,617 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_sentential_negation_npi_licensor_present in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,618 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_sentential_negation_npi_scope in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,619 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_sentential_subject_island in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,620 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_superlative_quantifiers_1 in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,621 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_superlative_quantifiers_2 in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,621 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_tough_vs_raising_1 in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,622 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_tough_vs_raising_2 in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,623 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_transitive in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,624 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_wh_island in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,625 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_wh_questions_object_gap in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,626 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_wh_questions_subject_gap in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,627 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_wh_questions_subject_gap_long_distance in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,627 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_wh_vs_that_no_gap in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,628 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_wh_vs_that_no_gap_long_distance in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,629 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_wh_vs_that_with_gap in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,631 INFO     [evaluator.py:266] num_fewshot has been set to 0 for blimp_wh_vs_that_with_gap_long_distance in its config. Manual configuration will be ignored.\n",
      "2024-11-18:18:17:18,631 WARNING  [evaluator.py:270] Overwriting default num_fewshot of lambada_standard from None to 0\n",
      "2024-11-18:18:17:18,632 WARNING  [evaluator.py:270] Overwriting default num_fewshot of lambada_openai from None to 0\n",
      "2024-11-18:18:17:18,633 WARNING  [evaluator.py:270] Overwriting default num_fewshot of hellaswag from None to 0\n",
      "2024-11-18:18:17:18,634 WARNING  [evaluator.py:270] Overwriting default num_fewshot of winogrande from None to 0\n",
      "2024-11-18:18:17:18,634 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_machine_learning from None to 0\n",
      "2024-11-18:18:17:18,635 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_astronomy from None to 0\n",
      "2024-11-18:18:17:18,636 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_college_mathematics from None to 0\n",
      "2024-11-18:18:17:18,637 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_physics from None to 0\n",
      "2024-11-18:18:17:18,638 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_biology from None to 0\n",
      "2024-11-18:18:17:18,639 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_mathematics from None to 0\n",
      "2024-11-18:18:17:18,639 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_college_computer_science from None to 0\n",
      "2024-11-18:18:17:18,640 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_elementary_mathematics from None to 0\n",
      "2024-11-18:18:17:18,641 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_chemistry from None to 0\n",
      "2024-11-18:18:17:18,642 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_conceptual_physics from None to 0\n",
      "2024-11-18:18:17:18,643 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_computer_security from None to 0\n",
      "2024-11-18:18:17:18,643 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_electrical_engineering from None to 0\n",
      "2024-11-18:18:17:18,644 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_anatomy from None to 0\n",
      "2024-11-18:18:17:18,645 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_college_chemistry from None to 0\n",
      "2024-11-18:18:17:18,646 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_college_physics from None to 0\n",
      "2024-11-18:18:17:18,647 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_abstract_algebra from None to 0\n",
      "2024-11-18:18:17:18,648 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_computer_science from None to 0\n",
      "2024-11-18:18:17:18,649 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_college_biology from None to 0\n",
      "2024-11-18:18:17:18,649 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_statistics from None to 0\n",
      "2024-11-18:18:17:18,650 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_global_facts from None to 0\n",
      "2024-11-18:18:17:18,651 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_marketing from None to 0\n",
      "2024-11-18:18:17:18,652 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_nutrition from None to 0\n",
      "2024-11-18:18:17:18,653 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_college_medicine from None to 0\n",
      "2024-11-18:18:17:18,654 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_business_ethics from None to 0\n",
      "2024-11-18:18:17:18,655 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_management from None to 0\n",
      "2024-11-18:18:17:18,656 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_virology from None to 0\n",
      "2024-11-18:18:17:18,657 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_human_aging from None to 0\n",
      "2024-11-18:18:17:18,658 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_clinical_knowledge from None to 0\n",
      "2024-11-18:18:17:18,659 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_medical_genetics from None to 0\n",
      "2024-11-18:18:17:18,660 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_professional_medicine from None to 0\n",
      "2024-11-18:18:17:18,660 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_professional_accounting from None to 0\n",
      "2024-11-18:18:17:18,661 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_miscellaneous from None to 0\n",
      "2024-11-18:18:17:18,663 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_econometrics from None to 0\n",
      "2024-11-18:18:17:18,664 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_professional_psychology from None to 0\n",
      "2024-11-18:18:17:18,665 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_psychology from None to 0\n",
      "2024-11-18:18:17:18,666 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_government_and_politics from None to 0\n",
      "2024-11-18:18:17:18,666 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_sociology from None to 0\n",
      "2024-11-18:18:17:18,667 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_human_sexuality from None to 0\n",
      "2024-11-18:18:17:18,668 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_geography from None to 0\n",
      "2024-11-18:18:17:18,669 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_macroeconomics from None to 0\n",
      "2024-11-18:18:17:18,670 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_microeconomics from None to 0\n",
      "2024-11-18:18:17:18,671 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_public_relations from None to 0\n",
      "2024-11-18:18:17:18,671 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_security_studies from None to 0\n",
      "2024-11-18:18:17:18,672 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_us_foreign_policy from None to 0\n",
      "2024-11-18:18:17:18,673 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_international_law from None to 0\n",
      "2024-11-18:18:17:18,674 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_european_history from None to 0\n",
      "2024-11-18:18:17:18,675 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_philosophy from None to 0\n",
      "2024-11-18:18:17:18,676 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_moral_scenarios from None to 0\n",
      "2024-11-18:18:17:18,676 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_jurisprudence from None to 0\n",
      "2024-11-18:18:17:18,677 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_formal_logic from None to 0\n",
      "2024-11-18:18:17:18,678 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_world_religions from None to 0\n",
      "2024-11-18:18:17:18,679 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_prehistory from None to 0\n",
      "2024-11-18:18:17:18,680 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_us_history from None to 0\n",
      "2024-11-18:18:17:18,681 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_moral_disputes from None to 0\n",
      "2024-11-18:18:17:18,681 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_logical_fallacies from None to 0\n",
      "2024-11-18:18:17:18,682 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_world_history from None to 0\n",
      "2024-11-18:18:17:18,683 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_professional_law from None to 0\n",
      "2024-11-18:18:17:18,684 WARNING  [evaluator.py:270] Overwriting default num_fewshot of arc_challenge from None to 0\n",
      "2024-11-18:18:17:18,696 INFO     [task.py:415] Building contexts for blimp_adjunct_island on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 877.03it/s]\n",
      "2024-11-18:18:17:20,060 INFO     [task.py:415] Building contexts for blimp_anaphor_gender_agreement on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 879.60it/s]\n",
      "2024-11-18:18:17:21,371 INFO     [task.py:415] Building contexts for blimp_anaphor_number_agreement on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 848.21it/s]\n",
      "2024-11-18:18:17:22,729 INFO     [task.py:415] Building contexts for blimp_animate_subject_passive on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 889.49it/s]\n",
      "2024-11-18:18:17:24,029 INFO     [task.py:415] Building contexts for blimp_animate_subject_trans on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 891.85it/s]\n",
      "2024-11-18:18:17:25,324 INFO     [task.py:415] Building contexts for blimp_causative on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 516.42it/s]\n",
      "2024-11-18:18:17:27,433 INFO     [task.py:415] Building contexts for blimp_complex_NP_island on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 879.85it/s]\n",
      "2024-11-18:18:17:28,743 INFO     [task.py:415] Building contexts for blimp_coordinate_structure_constraint_complex_left_branch on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 878.38it/s]\n",
      "2024-11-18:18:17:30,057 INFO     [task.py:415] Building contexts for blimp_coordinate_structure_constraint_object_extraction on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 882.01it/s]\n",
      "2024-11-18:18:17:31,367 INFO     [task.py:415] Building contexts for blimp_determiner_noun_agreement_1 on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 891.11it/s]\n",
      "2024-11-18:18:17:32,662 INFO     [task.py:415] Building contexts for blimp_determiner_noun_agreement_2 on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 890.14it/s]\n",
      "2024-11-18:18:17:33,957 INFO     [task.py:415] Building contexts for blimp_determiner_noun_agreement_irregular_1 on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 890.96it/s]\n",
      "2024-11-18:18:17:35,254 INFO     [task.py:415] Building contexts for blimp_determiner_noun_agreement_irregular_2 on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 879.93it/s]\n",
      "2024-11-18:18:17:36,564 INFO     [task.py:415] Building contexts for blimp_determiner_noun_agreement_with_adj_2 on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 887.79it/s]\n",
      "2024-11-18:18:17:37,866 INFO     [task.py:415] Building contexts for blimp_determiner_noun_agreement_with_adj_irregular_1 on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 889.72it/s]\n",
      "2024-11-18:18:17:39,180 INFO     [task.py:415] Building contexts for blimp_determiner_noun_agreement_with_adj_irregular_2 on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 888.61it/s]\n",
      "2024-11-18:18:17:40,478 INFO     [task.py:415] Building contexts for blimp_determiner_noun_agreement_with_adjective_1 on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 888.11it/s]\n",
      "2024-11-18:18:17:41,779 INFO     [task.py:415] Building contexts for blimp_distractor_agreement_relational_noun on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 886.92it/s]\n",
      "2024-11-18:18:17:43,080 INFO     [task.py:415] Building contexts for blimp_distractor_agreement_relative_clause on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 871.80it/s]\n",
      "2024-11-18:18:17:44,406 INFO     [task.py:415] Building contexts for blimp_drop_argument on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 868.93it/s]\n",
      "2024-11-18:18:17:45,736 INFO     [task.py:415] Building contexts for blimp_ellipsis_n_bar_1 on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 866.16it/s]\n",
      "2024-11-18:18:17:47,065 INFO     [task.py:415] Building contexts for blimp_ellipsis_n_bar_2 on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 870.26it/s]\n",
      "2024-11-18:18:17:48,395 INFO     [task.py:415] Building contexts for blimp_existential_there_object_raising on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 867.91it/s]\n",
      "2024-11-18:18:17:49,722 INFO     [task.py:415] Building contexts for blimp_existential_there_quantifiers_1 on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 889.36it/s]\n",
      "2024-11-18:18:17:51,018 INFO     [task.py:415] Building contexts for blimp_existential_there_quantifiers_2 on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 880.85it/s]\n",
      "2024-11-18:18:17:52,328 INFO     [task.py:415] Building contexts for blimp_existential_there_subject_raising on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 870.57it/s]\n",
      "2024-11-18:18:17:53,662 INFO     [task.py:415] Building contexts for blimp_expletive_it_object_raising on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 876.97it/s]\n",
      "2024-11-18:18:17:54,976 INFO     [task.py:415] Building contexts for blimp_inchoative on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 866.95it/s]\n",
      "2024-11-18:18:17:56,304 INFO     [task.py:415] Building contexts for blimp_intransitive on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 869.92it/s]\n",
      "2024-11-18:18:17:57,630 INFO     [task.py:415] Building contexts for blimp_irregular_past_participle_adjectives on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 863.03it/s]\n",
      "2024-11-18:18:17:58,966 INFO     [task.py:415] Building contexts for blimp_irregular_past_participle_verbs on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 874.97it/s]\n",
      "2024-11-18:18:18:00,284 INFO     [task.py:415] Building contexts for blimp_irregular_plural_subject_verb_agreement_1 on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 881.64it/s]\n",
      "2024-11-18:18:18:01,597 INFO     [task.py:415] Building contexts for blimp_irregular_plural_subject_verb_agreement_2 on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 872.09it/s]\n",
      "2024-11-18:18:18:02,918 INFO     [task.py:415] Building contexts for blimp_left_branch_island_echo_question on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 884.94it/s]\n",
      "2024-11-18:18:18:04,222 INFO     [task.py:415] Building contexts for blimp_left_branch_island_simple_question on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 875.82it/s]\n",
      "2024-11-18:18:18:05,566 INFO     [task.py:415] Building contexts for blimp_matrix_question_npi_licensor_present on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 870.38it/s]\n",
      "2024-11-18:18:18:06,895 INFO     [task.py:415] Building contexts for blimp_npi_present_1 on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 864.90it/s]\n",
      "2024-11-18:18:18:08,227 INFO     [task.py:415] Building contexts for blimp_npi_present_2 on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 791.62it/s]\n",
      "2024-11-18:18:18:09,680 INFO     [task.py:415] Building contexts for blimp_only_npi_licensor_present on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 872.47it/s]\n",
      "2024-11-18:18:18:11,007 INFO     [task.py:415] Building contexts for blimp_only_npi_scope on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 816.11it/s]\n",
      "2024-11-18:18:18:12,419 INFO     [task.py:415] Building contexts for blimp_passive_1 on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 904.49it/s]\n",
      "2024-11-18:18:18:13,700 INFO     [task.py:415] Building contexts for blimp_passive_2 on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 885.98it/s]\n",
      "2024-11-18:18:18:14,999 INFO     [task.py:415] Building contexts for blimp_principle_A_c_command on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 881.45it/s]\n",
      "2024-11-18:18:18:16,307 INFO     [task.py:415] Building contexts for blimp_principle_A_case_1 on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 880.72it/s]\n",
      "2024-11-18:18:18:17,620 INFO     [task.py:415] Building contexts for blimp_principle_A_case_2 on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 882.54it/s]\n",
      "2024-11-18:18:18:18,927 INFO     [task.py:415] Building contexts for blimp_principle_A_domain_1 on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 875.86it/s]\n",
      "2024-11-18:18:18:20,243 INFO     [task.py:415] Building contexts for blimp_principle_A_domain_2 on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 864.19it/s]\n",
      "2024-11-18:18:18:21,576 INFO     [task.py:415] Building contexts for blimp_principle_A_domain_3 on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 878.61it/s]\n",
      "2024-11-18:18:18:22,891 INFO     [task.py:415] Building contexts for blimp_principle_A_reconstruction on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 881.37it/s]\n",
      "2024-11-18:18:18:24,202 INFO     [task.py:415] Building contexts for blimp_regular_plural_subject_verb_agreement_1 on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 870.75it/s]\n",
      "2024-11-18:18:18:25,525 INFO     [task.py:415] Building contexts for blimp_regular_plural_subject_verb_agreement_2 on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 894.39it/s]\n",
      "2024-11-18:18:18:26,817 INFO     [task.py:415] Building contexts for blimp_sentential_negation_npi_licensor_present on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 876.90it/s]\n",
      "2024-11-18:18:18:28,130 INFO     [task.py:415] Building contexts for blimp_sentential_negation_npi_scope on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 883.24it/s]\n",
      "2024-11-18:18:18:29,436 INFO     [task.py:415] Building contexts for blimp_sentential_subject_island on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 877.98it/s]\n",
      "2024-11-18:18:18:30,747 INFO     [task.py:415] Building contexts for blimp_superlative_quantifiers_1 on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 894.54it/s]\n",
      "2024-11-18:18:18:32,039 INFO     [task.py:415] Building contexts for blimp_superlative_quantifiers_2 on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 878.18it/s]\n",
      "2024-11-18:18:18:33,350 INFO     [task.py:415] Building contexts for blimp_tough_vs_raising_1 on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 905.27it/s]\n",
      "2024-11-18:18:18:34,631 INFO     [task.py:415] Building contexts for blimp_tough_vs_raising_2 on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 885.59it/s]\n",
      "2024-11-18:18:18:35,931 INFO     [task.py:415] Building contexts for blimp_transitive on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 905.71it/s]\n",
      "2024-11-18:18:18:37,206 INFO     [task.py:415] Building contexts for blimp_wh_island on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 903.93it/s]\n",
      "2024-11-18:18:18:38,486 INFO     [task.py:415] Building contexts for blimp_wh_questions_object_gap on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 900.32it/s]\n",
      "2024-11-18:18:18:39,768 INFO     [task.py:415] Building contexts for blimp_wh_questions_subject_gap on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 869.45it/s]\n",
      "2024-11-18:18:18:41,100 INFO     [task.py:415] Building contexts for blimp_wh_questions_subject_gap_long_distance on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 868.96it/s]\n",
      "2024-11-18:18:18:42,436 INFO     [task.py:415] Building contexts for blimp_wh_vs_that_no_gap on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 895.51it/s]\n",
      "2024-11-18:18:18:43,723 INFO     [task.py:415] Building contexts for blimp_wh_vs_that_no_gap_long_distance on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 904.82it/s]\n",
      "2024-11-18:18:18:44,999 INFO     [task.py:415] Building contexts for blimp_wh_vs_that_with_gap on rank 0...\n",
      "100%|| 1000/1000 [00:01<00:00, 908.94it/s]\n",
      "2024-11-18:18:18:46,269 INFO     [task.py:415] Building contexts for blimp_wh_vs_that_with_gap_long_distance on rank 0...\n",
      "100%|| 1000/1000 [00:02<00:00, 480.92it/s]\n",
      "2024-11-18:18:18:48,521 INFO     [task.py:415] Building contexts for lambada_standard on rank 0...\n",
      "100%|| 5153/5153 [00:13<00:00, 387.88it/s]\n",
      "2024-11-18:18:19:02,090 INFO     [task.py:415] Building contexts for lambada_openai on rank 0...\n",
      "100%|| 5153/5153 [00:13<00:00, 388.59it/s]\n",
      "2024-11-18:18:19:15,574 INFO     [task.py:415] Building contexts for hellaswag on rank 0...\n",
      "100%|| 10042/10042 [00:05<00:00, 1783.26it/s]\n",
      "2024-11-18:18:19:23,663 INFO     [task.py:415] Building contexts for winogrande on rank 0...\n",
      "100%|| 1267/1267 [00:00<00:00, 50048.81it/s]\n",
      "2024-11-18:18:19:23,800 INFO     [task.py:415] Building contexts for mmlu_machine_learning on rank 0...\n",
      "100%|| 112/112 [00:00<00:00, 430.93it/s]\n",
      "2024-11-18:18:19:24,075 INFO     [task.py:415] Building contexts for mmlu_astronomy on rank 0...\n",
      "100%|| 152/152 [00:00<00:00, 433.87it/s]\n",
      "2024-11-18:18:19:24,445 INFO     [task.py:415] Building contexts for mmlu_college_mathematics on rank 0...\n",
      "100%|| 100/100 [00:00<00:00, 433.40it/s]\n",
      "2024-11-18:18:19:24,689 INFO     [task.py:415] Building contexts for mmlu_high_school_physics on rank 0...\n",
      "100%|| 151/151 [00:00<00:00, 402.90it/s]\n",
      "2024-11-18:18:19:25,086 INFO     [task.py:415] Building contexts for mmlu_high_school_biology on rank 0...\n",
      "100%|| 310/310 [00:00<00:00, 410.92it/s]\n",
      "2024-11-18:18:19:25,878 INFO     [task.py:415] Building contexts for mmlu_high_school_mathematics on rank 0...\n",
      "100%|| 270/270 [00:00<00:00, 433.62it/s]\n",
      "2024-11-18:18:19:26,532 INFO     [task.py:415] Building contexts for mmlu_college_computer_science on rank 0...\n",
      "100%|| 100/100 [00:00<00:00, 422.30it/s]\n",
      "2024-11-18:18:19:26,783 INFO     [task.py:415] Building contexts for mmlu_elementary_mathematics on rank 0...\n",
      "100%|| 378/378 [00:00<00:00, 432.83it/s]\n",
      "2024-11-18:18:19:27,700 INFO     [task.py:415] Building contexts for mmlu_high_school_chemistry on rank 0...\n",
      "100%|| 203/203 [00:00<00:00, 430.39it/s]\n",
      "2024-11-18:18:19:28,196 INFO     [task.py:415] Building contexts for mmlu_conceptual_physics on rank 0...\n",
      "100%|| 235/235 [00:00<00:00, 428.30it/s]\n",
      "2024-11-18:18:19:28,772 INFO     [task.py:415] Building contexts for mmlu_computer_security on rank 0...\n",
      "100%|| 100/100 [00:00<00:00, 429.35it/s]\n",
      "2024-11-18:18:19:29,018 INFO     [task.py:415] Building contexts for mmlu_electrical_engineering on rank 0...\n",
      "100%|| 145/145 [00:00<00:00, 431.22it/s]\n",
      "2024-11-18:18:19:29,373 INFO     [task.py:415] Building contexts for mmlu_anatomy on rank 0...\n",
      "100%|| 135/135 [00:00<00:00, 428.74it/s]\n",
      "2024-11-18:18:19:29,706 INFO     [task.py:415] Building contexts for mmlu_college_chemistry on rank 0...\n",
      "100%|| 100/100 [00:00<00:00, 428.93it/s]\n",
      "2024-11-18:18:19:29,954 INFO     [task.py:415] Building contexts for mmlu_college_physics on rank 0...\n",
      "100%|| 102/102 [00:00<00:00, 425.81it/s]\n",
      "2024-11-18:18:19:30,208 INFO     [task.py:415] Building contexts for mmlu_abstract_algebra on rank 0...\n",
      "100%|| 100/100 [00:00<00:00, 425.18it/s]\n",
      "2024-11-18:18:19:30,457 INFO     [task.py:415] Building contexts for mmlu_high_school_computer_science on rank 0...\n",
      "100%|| 100/100 [00:00<00:00, 430.10it/s]\n",
      "2024-11-18:18:19:30,703 INFO     [task.py:415] Building contexts for mmlu_college_biology on rank 0...\n",
      "100%|| 144/144 [00:00<00:00, 430.44it/s]\n",
      "2024-11-18:18:19:31,055 INFO     [task.py:415] Building contexts for mmlu_high_school_statistics on rank 0...\n",
      "100%|| 216/216 [00:00<00:00, 431.53it/s]\n",
      "2024-11-18:18:19:31,582 INFO     [task.py:415] Building contexts for mmlu_global_facts on rank 0...\n",
      "100%|| 100/100 [00:00<00:00, 431.14it/s]\n",
      "2024-11-18:18:19:31,827 INFO     [task.py:415] Building contexts for mmlu_marketing on rank 0...\n",
      "100%|| 234/234 [00:00<00:00, 421.84it/s]\n",
      "2024-11-18:18:19:32,409 INFO     [task.py:415] Building contexts for mmlu_nutrition on rank 0...\n",
      "100%|| 306/306 [00:00<00:00, 426.08it/s]\n",
      "2024-11-18:18:19:33,162 INFO     [task.py:415] Building contexts for mmlu_college_medicine on rank 0...\n",
      "100%|| 173/173 [00:00<00:00, 423.88it/s]\n",
      "2024-11-18:18:19:33,592 INFO     [task.py:415] Building contexts for mmlu_business_ethics on rank 0...\n",
      "100%|| 100/100 [00:00<00:00, 436.49it/s]\n",
      "2024-11-18:18:19:33,835 INFO     [task.py:415] Building contexts for mmlu_management on rank 0...\n",
      "100%|| 103/103 [00:00<00:00, 425.78it/s]\n",
      "2024-11-18:18:19:34,091 INFO     [task.py:415] Building contexts for mmlu_virology on rank 0...\n",
      "100%|| 166/166 [00:00<00:00, 432.14it/s]\n",
      "2024-11-18:18:19:34,495 INFO     [task.py:415] Building contexts for mmlu_human_aging on rank 0...\n",
      "100%|| 223/223 [00:00<00:00, 428.62it/s]\n",
      "2024-11-18:18:19:35,042 INFO     [task.py:415] Building contexts for mmlu_clinical_knowledge on rank 0...\n",
      "100%|| 265/265 [00:00<00:00, 431.08it/s]\n",
      "2024-11-18:18:19:35,687 INFO     [task.py:415] Building contexts for mmlu_medical_genetics on rank 0...\n",
      "100%|| 100/100 [00:00<00:00, 402.58it/s]\n",
      "2024-11-18:18:19:35,949 INFO     [task.py:415] Building contexts for mmlu_professional_medicine on rank 0...\n",
      "100%|| 272/272 [00:00<00:00, 418.96it/s]\n",
      "2024-11-18:18:19:36,630 INFO     [task.py:415] Building contexts for mmlu_professional_accounting on rank 0...\n",
      "100%|| 282/282 [00:00<00:00, 416.60it/s]\n",
      "2024-11-18:18:19:37,342 INFO     [task.py:415] Building contexts for mmlu_miscellaneous on rank 0...\n",
      "100%|| 783/783 [00:01<00:00, 436.98it/s]\n",
      "2024-11-18:18:19:39,216 INFO     [task.py:415] Building contexts for mmlu_econometrics on rank 0...\n",
      "100%|| 114/114 [00:00<00:00, 437.67it/s]\n",
      "2024-11-18:18:19:39,492 INFO     [task.py:415] Building contexts for mmlu_professional_psychology on rank 0...\n",
      "100%|| 612/612 [00:01<00:00, 438.36it/s]\n",
      "2024-11-18:18:19:40,952 INFO     [task.py:415] Building contexts for mmlu_high_school_psychology on rank 0...\n",
      "100%|| 545/545 [00:01<00:00, 434.45it/s]\n",
      "2024-11-18:18:19:42,264 INFO     [task.py:415] Building contexts for mmlu_high_school_government_and_politics on rank 0...\n",
      "100%|| 193/193 [00:00<00:00, 438.50it/s]\n",
      "2024-11-18:18:19:42,728 INFO     [task.py:415] Building contexts for mmlu_sociology on rank 0...\n",
      "100%|| 201/201 [00:00<00:00, 439.38it/s]\n",
      "2024-11-18:18:19:43,209 INFO     [task.py:415] Building contexts for mmlu_human_sexuality on rank 0...\n",
      "100%|| 131/131 [00:00<00:00, 439.97it/s]\n",
      "2024-11-18:18:19:43,523 INFO     [task.py:415] Building contexts for mmlu_high_school_geography on rank 0...\n",
      "100%|| 198/198 [00:00<00:00, 440.56it/s]\n",
      "2024-11-18:18:19:43,996 INFO     [task.py:415] Building contexts for mmlu_high_school_macroeconomics on rank 0...\n",
      "100%|| 390/390 [00:00<00:00, 435.01it/s]\n",
      "2024-11-18:18:19:44,935 INFO     [task.py:415] Building contexts for mmlu_high_school_microeconomics on rank 0...\n",
      "100%|| 238/238 [00:00<00:00, 433.93it/s]\n",
      "2024-11-18:18:19:45,510 INFO     [task.py:415] Building contexts for mmlu_public_relations on rank 0...\n",
      "100%|| 110/110 [00:00<00:00, 423.81it/s]\n",
      "2024-11-18:18:19:45,785 INFO     [task.py:415] Building contexts for mmlu_security_studies on rank 0...\n",
      "100%|| 245/245 [00:00<00:00, 431.99it/s]\n",
      "2024-11-18:18:19:46,381 INFO     [task.py:415] Building contexts for mmlu_us_foreign_policy on rank 0...\n",
      "100%|| 100/100 [00:00<00:00, 431.65it/s]\n",
      "2024-11-18:18:19:46,627 INFO     [task.py:415] Building contexts for mmlu_international_law on rank 0...\n",
      "100%|| 121/121 [00:00<00:00, 432.60it/s]\n",
      "2024-11-18:18:19:46,922 INFO     [task.py:415] Building contexts for mmlu_high_school_european_history on rank 0...\n",
      "100%|| 165/165 [00:00<00:00, 429.27it/s]\n",
      "2024-11-18:18:19:47,331 INFO     [task.py:415] Building contexts for mmlu_philosophy on rank 0...\n",
      "100%|| 311/311 [00:00<00:00, 426.49it/s]\n",
      "2024-11-18:18:19:48,096 INFO     [task.py:415] Building contexts for mmlu_moral_scenarios on rank 0...\n",
      "100%|| 895/895 [00:02<00:00, 422.82it/s]\n",
      "2024-11-18:18:19:50,308 INFO     [task.py:415] Building contexts for mmlu_jurisprudence on rank 0...\n",
      "100%|| 108/108 [00:00<00:00, 382.24it/s]\n",
      "2024-11-18:18:19:50,610 INFO     [task.py:415] Building contexts for mmlu_formal_logic on rank 0...\n",
      "100%|| 126/126 [00:00<00:00, 377.24it/s]\n",
      "2024-11-18:18:19:50,966 INFO     [task.py:415] Building contexts for mmlu_world_religions on rank 0...\n",
      "100%|| 171/171 [00:00<00:00, 388.36it/s]\n",
      "2024-11-18:18:19:51,432 INFO     [task.py:415] Building contexts for mmlu_prehistory on rank 0...\n",
      "100%|| 324/324 [00:00<00:00, 371.28it/s]\n",
      "2024-11-18:18:19:52,353 INFO     [task.py:415] Building contexts for mmlu_high_school_us_history on rank 0...\n",
      "100%|| 204/204 [00:00<00:00, 379.37it/s]\n",
      "2024-11-18:18:19:52,925 INFO     [task.py:415] Building contexts for mmlu_moral_disputes on rank 0...\n",
      "100%|| 346/346 [00:00<00:00, 409.84it/s]\n",
      "2024-11-18:18:19:53,819 INFO     [task.py:415] Building contexts for mmlu_logical_fallacies on rank 0...\n",
      "100%|| 163/163 [00:00<00:00, 431.59it/s]\n",
      "2024-11-18:18:19:54,217 INFO     [task.py:415] Building contexts for mmlu_high_school_world_history on rank 0...\n",
      "100%|| 237/237 [00:00<00:00, 433.47it/s]\n",
      "2024-11-18:18:19:54,794 INFO     [task.py:415] Building contexts for mmlu_professional_law on rank 0...\n",
      "100%|| 1534/1534 [00:03<00:00, 428.25it/s]\n",
      "2024-11-18:18:19:58,544 INFO     [task.py:415] Building contexts for arc_challenge on rank 0...\n",
      "100%|| 1172/1172 [00:01<00:00, 739.66it/s]\n",
      "2024-11-18:18:20:00,291 INFO     [evaluator.py:489] Running loglikelihood requests\n",
      "Running loglikelihood requests: 100%|| 247863/247863 [1:17:36<00:00, 53.23it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bootstrapping for stddev: perplexity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 100/100 [00:07<00:00, 13.94it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bootstrapping for stddev: perplexity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 100/100 [00:07<00:00, 14.06it/s]\n",
      "2024-11-18:19:42:04,264 WARNING  [huggingface.py:1353] Failed to get model SHA for GPTNeoXForCausalLM(\n",
      "  (gpt_neox): GPTNeoXModel(\n",
      "    (embed_in): Embedding(50304, 768)\n",
      "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x GPTNeoXLayer(\n",
      "        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (attention): GPTNeoXSdpaAttention(\n",
      "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "          (query_key_value): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (mlp): GPTNeoXMLP(\n",
      "          (dense_h_to_4h): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (dense_4h_to_h): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): GELUActivation()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  )\n",
      "  (embed_out): Linear(in_features=768, out_features=50304, bias=False)\n",
      ") at revision main. Error: Repo id must be a string, not <class 'transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXForCausalLM'>: 'GPTNeoXForCausalLM(\n",
      "  (gpt_neox): GPTNeoXModel(\n",
      "    (embed_in): Embedding(50304, 768)\n",
      "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x GPTNeoXLayer(\n",
      "        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (attention): GPTNeoXSdpaAttention(\n",
      "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "          (query_key_value): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (mlp): GPTNeoXMLP(\n",
      "          (dense_h_to_4h): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (dense_4h_to_h): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): GELUActivation()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  )\n",
      "  (embed_out): Linear(in_features=768, out_features=50304, bias=False)\n",
      ")'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|                           Tasks                            |Version|Filter|n-shot|  Metric  |   |    Value    |   |   Stderr   |\n",
      "|------------------------------------------------------------|------:|------|-----:|----------|---|------------:|---|-----------:|\n",
      "|arc_challenge                                               |      1|none  |     0|acc       |  |       0.2125|  |      0.0120|\n",
      "|                                                            |       |none  |     0|acc_norm  |  |       0.2679|  |      0.0129|\n",
      "|blimp                                                       |      2|none  |      |acc       |  |       0.5194|  |      0.0018|\n",
      "| - blimp_adjunct_island                                     |      1|none  |     0|acc       |  |       0.6290|  |      0.0153|\n",
      "| - blimp_anaphor_gender_agreement                           |      1|none  |     0|acc       |  |       0.7590|  |      0.0135|\n",
      "| - blimp_anaphor_number_agreement                           |      1|none  |     0|acc       |  |       0.5730|  |      0.0156|\n",
      "| - blimp_animate_subject_passive                            |      1|none  |     0|acc       |  |       0.6220|  |      0.0153|\n",
      "| - blimp_animate_subject_trans                              |      1|none  |     0|acc       |  |       0.7790|  |      0.0131|\n",
      "| - blimp_causative                                          |      1|none  |     0|acc       |  |       0.4080|  |      0.0155|\n",
      "| - blimp_complex_NP_island                                  |      1|none  |     0|acc       |  |       0.4910|  |      0.0158|\n",
      "| - blimp_coordinate_structure_constraint_complex_left_branch|      1|none  |     0|acc       |  |       0.6770|  |      0.0148|\n",
      "| - blimp_coordinate_structure_constraint_object_extraction  |      1|none  |     0|acc       |  |       0.3380|  |      0.0150|\n",
      "| - blimp_determiner_noun_agreement_1                        |      1|none  |     0|acc       |  |       0.5100|  |      0.0158|\n",
      "| - blimp_determiner_noun_agreement_2                        |      1|none  |     0|acc       |  |       0.5120|  |      0.0158|\n",
      "| - blimp_determiner_noun_agreement_irregular_1              |      1|none  |     0|acc       |  |       0.4870|  |      0.0158|\n",
      "| - blimp_determiner_noun_agreement_irregular_2              |      1|none  |     0|acc       |  |       0.5090|  |      0.0158|\n",
      "| - blimp_determiner_noun_agreement_with_adj_2               |      1|none  |     0|acc       |  |       0.5060|  |      0.0158|\n",
      "| - blimp_determiner_noun_agreement_with_adj_irregular_1     |      1|none  |     0|acc       |  |       0.4490|  |      0.0157|\n",
      "| - blimp_determiner_noun_agreement_with_adj_irregular_2     |      1|none  |     0|acc       |  |       0.5120|  |      0.0158|\n",
      "| - blimp_determiner_noun_agreement_with_adjective_1         |      1|none  |     0|acc       |  |       0.4800|  |      0.0158|\n",
      "| - blimp_distractor_agreement_relational_noun               |      1|none  |     0|acc       |  |       0.5190|  |      0.0158|\n",
      "| - blimp_distractor_agreement_relative_clause               |      1|none  |     0|acc       |  |       0.5520|  |      0.0157|\n",
      "| - blimp_drop_argument                                      |      1|none  |     0|acc       |  |       0.6990|  |      0.0145|\n",
      "| - blimp_ellipsis_n_bar_1                                   |      1|none  |     0|acc       |  |       0.4710|  |      0.0158|\n",
      "| - blimp_ellipsis_n_bar_2                                   |      1|none  |     0|acc       |  |       0.3220|  |      0.0148|\n",
      "| - blimp_existential_there_object_raising                   |      1|none  |     0|acc       |  |       0.7090|  |      0.0144|\n",
      "| - blimp_existential_there_quantifiers_1                    |      1|none  |     0|acc       |  |       0.3270|  |      0.0148|\n",
      "| - blimp_existential_there_quantifiers_2                    |      1|none  |     0|acc       |  |       0.6870|  |      0.0147|\n",
      "| - blimp_existential_there_subject_raising                  |      1|none  |     0|acc       |  |       0.4200|  |      0.0156|\n",
      "| - blimp_expletive_it_object_raising                        |      1|none  |     0|acc       |  |       0.6060|  |      0.0155|\n",
      "| - blimp_inchoative                                         |      1|none  |     0|acc       |  |       0.4190|  |      0.0156|\n",
      "| - blimp_intransitive                                       |      1|none  |     0|acc       |  |       0.6020|  |      0.0155|\n",
      "| - blimp_irregular_past_participle_adjectives               |      1|none  |     0|acc       |  |       0.7440|  |      0.0138|\n",
      "| - blimp_irregular_past_participle_verbs                    |      1|none  |     0|acc       |  |       0.4470|  |      0.0157|\n",
      "| - blimp_irregular_plural_subject_verb_agreement_1          |      1|none  |     0|acc       |  |       0.4930|  |      0.0158|\n",
      "| - blimp_irregular_plural_subject_verb_agreement_2          |      1|none  |     0|acc       |  |       0.5420|  |      0.0158|\n",
      "| - blimp_left_branch_island_echo_question                   |      1|none  |     0|acc       |  |       0.5630|  |      0.0157|\n",
      "| - blimp_left_branch_island_simple_question                 |      1|none  |     0|acc       |  |       0.5160|  |      0.0158|\n",
      "| - blimp_matrix_question_npi_licensor_present               |      1|none  |     0|acc       |  |       0.6320|  |      0.0153|\n",
      "| - blimp_npi_present_1                                      |      1|none  |     0|acc       |  |       0.6440|  |      0.0151|\n",
      "| - blimp_npi_present_2                                      |      1|none  |     0|acc       |  |       0.5790|  |      0.0156|\n",
      "| - blimp_only_npi_licensor_present                          |      1|none  |     0|acc       |  |       0.5420|  |      0.0158|\n",
      "| - blimp_only_npi_scope                                     |      1|none  |     0|acc       |  |       0.3320|  |      0.0149|\n",
      "| - blimp_passive_1                                          |      1|none  |     0|acc       |  |       0.7070|  |      0.0144|\n",
      "| - blimp_passive_2                                          |      1|none  |     0|acc       |  |       0.5820|  |      0.0156|\n",
      "| - blimp_principle_A_c_command                              |      1|none  |     0|acc       |  |       0.3080|  |      0.0146|\n",
      "| - blimp_principle_A_case_1                                 |      1|none  |     0|acc       |  |       0.5540|  |      0.0157|\n",
      "| - blimp_principle_A_case_2                                 |      1|none  |     0|acc       |  |       0.4770|  |      0.0158|\n",
      "| - blimp_principle_A_domain_1                               |      1|none  |     0|acc       |  |       0.4730|  |      0.0158|\n",
      "| - blimp_principle_A_domain_2                               |      1|none  |     0|acc       |  |       0.4620|  |      0.0158|\n",
      "| - blimp_principle_A_domain_3                               |      1|none  |     0|acc       |  |       0.5200|  |      0.0158|\n",
      "| - blimp_principle_A_reconstruction                         |      1|none  |     0|acc       |  |       0.3920|  |      0.0154|\n",
      "| - blimp_regular_plural_subject_verb_agreement_1            |      1|none  |     0|acc       |  |       0.4400|  |      0.0157|\n",
      "| - blimp_regular_plural_subject_verb_agreement_2            |      1|none  |     0|acc       |  |       0.5130|  |      0.0158|\n",
      "| - blimp_sentential_negation_npi_licensor_present           |      1|none  |     0|acc       |  |       0.5270|  |      0.0158|\n",
      "| - blimp_sentential_negation_npi_scope                      |      1|none  |     0|acc       |  |       0.2560|  |      0.0138|\n",
      "| - blimp_sentential_subject_island                          |      1|none  |     0|acc       |  |       0.4310|  |      0.0157|\n",
      "| - blimp_superlative_quantifiers_1                          |      1|none  |     0|acc       |  |       0.5080|  |      0.0158|\n",
      "| - blimp_superlative_quantifiers_2                          |      1|none  |     0|acc       |  |       0.9110|  |      0.0090|\n",
      "| - blimp_tough_vs_raising_1                                 |      1|none  |     0|acc       |  |       0.2720|  |      0.0141|\n",
      "| - blimp_tough_vs_raising_2                                 |      1|none  |     0|acc       |  |       0.7090|  |      0.0144|\n",
      "| - blimp_transitive                                         |      1|none  |     0|acc       |  |       0.5390|  |      0.0158|\n",
      "| - blimp_wh_island                                          |      1|none  |     0|acc       |  |       0.8840|  |      0.0101|\n",
      "| - blimp_wh_questions_object_gap                            |      1|none  |     0|acc       |  |       0.0680|  |      0.0080|\n",
      "| - blimp_wh_questions_subject_gap                           |      1|none  |     0|acc       |  |       0.2390|  |      0.0135|\n",
      "| - blimp_wh_questions_subject_gap_long_distance             |      1|none  |     0|acc       |  |       0.4310|  |      0.0157|\n",
      "| - blimp_wh_vs_that_no_gap                                  |      1|none  |     0|acc       |  |       0.4310|  |      0.0157|\n",
      "| - blimp_wh_vs_that_no_gap_long_distance                    |      1|none  |     0|acc       |  |       0.4740|  |      0.0158|\n",
      "| - blimp_wh_vs_that_with_gap                                |      1|none  |     0|acc       |  |       0.5410|  |      0.0158|\n",
      "| - blimp_wh_vs_that_with_gap_long_distance                  |      1|none  |     0|acc       |  |       0.5420|  |      0.0158|\n",
      "|hellaswag                                                   |      1|none  |     0|acc       |  |       0.2582|  |      0.0044|\n",
      "|                                                            |       |none  |     0|acc_norm  |  |       0.2616|  |      0.0044|\n",
      "|lambada_openai                                              |      1|none  |     0|acc       |  |       0.0000|  |      0.0000|\n",
      "|                                                            |       |none  |     0|perplexity|  | 3033175.2693|  | 288926.5827|\n",
      "|lambada_standard                                            |      1|none  |     0|acc       |  |       0.0000|  |      0.0000|\n",
      "|                                                            |       |none  |     0|perplexity|  |27067951.3461|  |2710040.1909|\n",
      "|mmlu                                                        |      2|none  |      |acc       |  |       0.2465|  |      0.0036|\n",
      "| - humanities                                               |      2|none  |      |acc       |  |       0.2451|  |      0.0063|\n",
      "|  - formal_logic                                            |      1|none  |     0|acc       |  |       0.1984|  |      0.0357|\n",
      "|  - high_school_european_history                            |      1|none  |     0|acc       |  |       0.2424|  |      0.0335|\n",
      "|  - high_school_us_history                                  |      1|none  |     0|acc       |  |       0.2353|  |      0.0298|\n",
      "|  - high_school_world_history                               |      1|none  |     0|acc       |  |       0.2616|  |      0.0286|\n",
      "|  - international_law                                       |      1|none  |     0|acc       |  |       0.2479|  |      0.0394|\n",
      "|  - jurisprudence                                           |      1|none  |     0|acc       |  |       0.2963|  |      0.0441|\n",
      "|  - logical_fallacies                                       |      1|none  |     0|acc       |  |       0.2454|  |      0.0338|\n",
      "|  - moral_disputes                                          |      1|none  |     0|acc       |  |       0.2457|  |      0.0232|\n",
      "|  - moral_scenarios                                         |      1|none  |     0|acc       |  |       0.2425|  |      0.0143|\n",
      "|  - philosophy                                              |      1|none  |     0|acc       |  |       0.2733|  |      0.0253|\n",
      "|  - prehistory                                              |      1|none  |     0|acc       |  |       0.2654|  |      0.0246|\n",
      "|  - professional_law                                        |      1|none  |     0|acc       |  |       0.2392|  |      0.0109|\n",
      "|  - world_religions                                         |      1|none  |     0|acc       |  |       0.2105|  |      0.0313|\n",
      "| - other                                                    |      2|none  |      |acc       |  |       0.2687|  |      0.0079|\n",
      "|  - business_ethics                                         |      1|none  |     0|acc       |  |       0.2600|  |      0.0441|\n",
      "|  - clinical_knowledge                                      |      1|none  |     0|acc       |  |       0.2679|  |      0.0273|\n",
      "|  - college_medicine                                        |      1|none  |     0|acc       |  |       0.2081|  |      0.0310|\n",
      "|  - global_facts                                            |      1|none  |     0|acc       |  |       0.3100|  |      0.0465|\n",
      "|  - human_aging                                             |      1|none  |     0|acc       |  |       0.3767|  |      0.0325|\n",
      "|  - management                                              |      1|none  |     0|acc       |  |       0.2524|  |      0.0430|\n",
      "|  - marketing                                               |      1|none  |     0|acc       |  |       0.2564|  |      0.0286|\n",
      "|  - medical_genetics                                        |      1|none  |     0|acc       |  |       0.2600|  |      0.0441|\n",
      "|  - miscellaneous                                           |      1|none  |     0|acc       |  |       0.2874|  |      0.0162|\n",
      "|  - nutrition                                               |      1|none  |     0|acc       |  |       0.2288|  |      0.0241|\n",
      "|  - professional_accounting                                 |      1|none  |     0|acc       |  |       0.2553|  |      0.0260|\n",
      "|  - professional_medicine                                   |      1|none  |     0|acc       |  |       0.2022|  |      0.0244|\n",
      "|  - virology                                                |      1|none  |     0|acc       |  |       0.3193|  |      0.0363|\n",
      "| - social sciences                                          |      2|none  |      |acc       |  |       0.2343|  |      0.0076|\n",
      "|  - econometrics                                            |      1|none  |     0|acc       |  |       0.2807|  |      0.0423|\n",
      "|  - high_school_geography                                   |      1|none  |     0|acc       |  |       0.2172|  |      0.0294|\n",
      "|  - high_school_government_and_politics                     |      1|none  |     0|acc       |  |       0.2073|  |      0.0293|\n",
      "|  - high_school_macroeconomics                              |      1|none  |     0|acc       |  |       0.2205|  |      0.0210|\n",
      "|  - high_school_microeconomics                              |      1|none  |     0|acc       |  |       0.2311|  |      0.0274|\n",
      "|  - high_school_psychology                                  |      1|none  |     0|acc       |  |       0.2367|  |      0.0182|\n",
      "|  - human_sexuality                                         |      1|none  |     0|acc       |  |       0.2290|  |      0.0369|\n",
      "|  - professional_psychology                                 |      1|none  |     0|acc       |  |       0.2565|  |      0.0177|\n",
      "|  - public_relations                                        |      1|none  |     0|acc       |  |       0.3455|  |      0.0455|\n",
      "|  - security_studies                                        |      1|none  |     0|acc       |  |       0.1714|  |      0.0241|\n",
      "|  - sociology                                               |      1|none  |     0|acc       |  |       0.2388|  |      0.0301|\n",
      "|  - us_foreign_policy                                       |      1|none  |     0|acc       |  |       0.2100|  |      0.0409|\n",
      "| - stem                                                     |      2|none  |      |acc       |  |       0.2388|  |      0.0076|\n",
      "|  - abstract_algebra                                        |      1|none  |     0|acc       |  |       0.2600|  |      0.0441|\n",
      "|  - anatomy                                                 |      1|none  |     0|acc       |  |       0.2519|  |      0.0375|\n",
      "|  - astronomy                                               |      1|none  |     0|acc       |  |       0.1842|  |      0.0315|\n",
      "|  - college_biology                                         |      1|none  |     0|acc       |  |       0.2222|  |      0.0348|\n",
      "|  - college_chemistry                                       |      1|none  |     0|acc       |  |       0.2100|  |      0.0409|\n",
      "|  - college_computer_science                                |      1|none  |     0|acc       |  |       0.1500|  |      0.0359|\n",
      "|  - college_mathematics                                     |      1|none  |     0|acc       |  |       0.2300|  |      0.0423|\n",
      "|  - college_physics                                         |      1|none  |     0|acc       |  |       0.1961|  |      0.0395|\n",
      "|  - computer_security                                       |      1|none  |     0|acc       |  |       0.2400|  |      0.0429|\n",
      "|  - conceptual_physics                                      |      1|none  |     0|acc       |  |       0.3234|  |      0.0306|\n",
      "|  - electrical_engineering                                  |      1|none  |     0|acc       |  |       0.2207|  |      0.0346|\n",
      "|  - elementary_mathematics                                  |      1|none  |     0|acc       |  |       0.2566|  |      0.0225|\n",
      "|  - high_school_biology                                     |      1|none  |     0|acc       |  |       0.2548|  |      0.0248|\n",
      "|  - high_school_chemistry                                   |      1|none  |     0|acc       |  |       0.2709|  |      0.0313|\n",
      "|  - high_school_computer_science                            |      1|none  |     0|acc       |  |       0.2300|  |      0.0423|\n",
      "|  - high_school_mathematics                                 |      1|none  |     0|acc       |  |       0.2630|  |      0.0268|\n",
      "|  - high_school_physics                                     |      1|none  |     0|acc       |  |       0.1987|  |      0.0326|\n",
      "|  - high_school_statistics                                  |      1|none  |     0|acc       |  |       0.1620|  |      0.0251|\n",
      "|  - machine_learning                                        |      1|none  |     0|acc       |  |       0.2857|  |      0.0429|\n",
      "|winogrande                                                  |      1|none  |     0|acc       |  |       0.4933|  |      0.0141|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Evaluation - Pythia 160M Trained on MiniPile\n",
    "\n",
    "pythia_minipile = AutoModelForCausalLM.from_pretrained(base_path / \"pythia160m_minipile_trained\", local_files_only=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_path / \"pythia160m_dedup_untrained\", use_fast=True, local_files_only=True) # Use exact same tokenizer\n",
    "pythia_minipile = pythia_minipile.to(device)\n",
    " \n",
    "batch_size_hflm = 1\n",
    "\n",
    "pythia_minipile_hflm = HFLM(pretrained=pythia_minipile,\n",
    "                        tokenizer=tokenizer,\n",
    "                        batch_size=batch_size_hflm)\n",
    "\n",
    "results = simple_evaluate(model=pythia_minipile_hflm,\n",
    "                          tasks=[\"arc_challenge\", \"mmlu\", \"winogrande\", \"hellaswag\", \"lambada\", \"blimp\"],\n",
    "                          num_fewshot=0,\n",
    "                          batch_size=batch_size_hflm,\n",
    "                          device=\"cuda\",\n",
    "                          limit=None)\n",
    "\n",
    "with open('02_eval_160M_minipile.txt', 'w') as f:\n",
    "    f.write(str(results))\n",
    "\n",
    "print(utils.make_table(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This above layed out benchmarking pipeline was implemented in the `03_bench_160M.py` and `03_bench_1.4B.py` scripts for later use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's the catch?\n",
    "\n",
    "I calculated the \"Percentage Difference of Means\" and \"95% Confidence Interval\" in the `MiniPile_Pile_Benchmark_Comparisons.ods` spreadsheet.<br>\n",
    "Crucially, we see the following results for the MiniPile-trained Pythia $160M$ model, compared against the Pile-trained Pythia $160M$ model:\n",
    "\n",
    "| Benchmark        | Measure      |   | 160M Pile Deduplicated | 160M MiniPile               | Percentage Difference of Means | 95% Confidence Interval        | Interpretation                            |\n",
    "| ---------------- | ------------ | - | ---------------------- | --------------------------- | ------------------------------ | ------------------------------ | ----------------------------------------- |\n",
    "| ARC-Challenge    | acc          |  | 0.1997  0.0117        | **0.2125  0.0120**         | 6.4096                         | (0.0456; -0.0200)              | Difference not significant                |\n",
    "| MMLU             | acc          |  | 0.2299  0.0035        | **0.2699  0.0037**         | 17.3989                        | (0.0500; 0.0300)               | **MiniPile-trained better**               |\n",
    "| HellaSwag        | acc          |  | **0.2903  0.0045**    | 0.2560  0.0044             | -11.8154                       | (-0.0220; -0.0466)             | Pile Deduplicated-trained better          |\n",
    "| WinoGrande       | acc          |  | **0.4964  0.0141**    | 0.4720  0.0140             | -4.9154                        | (0.0145; -0.0633)              | Difference not significant                |\n",
    "| Lambada (OpenAI) | acc          |  | **0.3689  0.0067**    | 0.0000  0.0000             | -100.00                        | (-0.3558; -0.3820)             | Pile Deduplicated-trained severely better |\n",
    "| Lambada (OpenAI) | perplexity   |  | **31.2589  1.1594**   | 3033175.2693  288926.5827  | 9703297.3342                   | (3599440.1125; 2466847.9083)   | Pile Deduplicated-trained severely better |\n",
    "| Lambada (Std)    | acc          |  | **0.2335  0.0059**    | 0.0000  0.0000             | -100.00                        | (-0.2219; -0.2451)             | Pile Deduplicated-trained severely better |\n",
    "| Lambada (Std)    | perplexity   |  | **172.7619  7.7265**  | 27067951.3460  2710040.191 | 15667678.2231                  | (32379457.3584; 21756099.8100) | Pile Deduplicated-trained severely better |\n",
    "| BLiMP            | acc          |  | **0.7294  0.0015**    | 0.5194  0.0018             | -28.7908                       | (-0,2054; -0,2146)             | Pile Deduplicated-trained better          |\n",
    "\n",
    "- Training pipeline replication seems successful, as the MiniPile-trained model displays competitive performance on multiple benchmarks.\n",
    "    - Performance significantly better on MMLU\n",
    "    - Comparable performance on ARC-Challenge, WinoGrande\n",
    "    - Inferior performance on HellaSwag\n",
    "    - Severely inferior performance on Lambada (OpenAI)\n",
    "\n",
    "I interpret the results for the Lambada benchmark as follows:\n",
    "- MiniPile seems to lack distinctly crucial linguistic understanding that was certainly emitted from The Pile Deduplicated\n",
    "- It seems general linguistic understanding on MiniPile, given the extremely high perplexity, is severely lacking\n",
    "\n",
    "Therefore, while MiniPile can enable competitive performance on some tasks with three orders of magnitude less data, it seems to break down in capturing more general aspects of language necessary for those benchmarks involving next-token prediction in rather context-rich settings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minipile",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
