{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking Pythia $160\\text{M}$ pre-trained on The Pile vs. Pythia $160\\text{M}$ trained on MiniPile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objectives:\n",
    "- [x] Prepare (Download) two models - **Pythia $160M$ Untrained** and **Pythia $160M$ fully Pile-trained**\n",
    "- [x] Load MiniPile Dataset from disk\n",
    "- [x] Train **Pythia $160M$ Untrained** on MiniPile (according to the MiniPile paper) *and save the model* (`pythia160m_minipile_trained`)\n",
    "- [x] Evaluate the performance of **Pythia $160M$ Pile-trained** on MMLU, ARC, WinoGrande, HellaSwag, Lambada benchmarks\n",
    "- [x] Evaluate the performance of **Pythia $160M$ Untrained** on MMLU, ARC, WinoGrande, HellaSwag, Lambada benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /mnt/storage/miniconda3/envs/minipile/lib/python3.12/site-packages/huggingface_hub-0.26.2-py3.8.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting lm-eval\n",
      "  Downloading lm_eval-0.4.5-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in /mnt/storage/miniconda3/envs/minipile/lib/python3.12/site-packages (from lm-eval) (1.1.1)\n",
      "Requirement already satisfied: evaluate in /mnt/storage/miniconda3/envs/minipile/lib/python3.12/site-packages (from lm-eval) (0.4.1)\n",
      "Requirement already satisfied: datasets>=2.16.0 in /mnt/storage/miniconda3/envs/minipile/lib/python3.12/site-packages (from lm-eval) (3.1.0)\n",
      "Collecting jsonlines (from lm-eval)\n",
      "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: numexpr in /mnt/storage/miniconda3/envs/minipile/lib/python3.12/site-packages (from lm-eval) (2.10.1)\n",
      "Collecting peft>=0.2.0 (from lm-eval)\n",
      "  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pybind11>=2.6.2 (from lm-eval)\n",
      "  Downloading pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting pytablewriter (from lm-eval)\n",
      "  Downloading pytablewriter-1.2.0-py3-none-any.whl.metadata (37 kB)\n",
      "Collecting rouge-score>=0.0.4 (from lm-eval)\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting sacrebleu>=1.5.0 (from lm-eval)\n",
      "  Downloading sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.24.1 in /mnt/storage/miniconda3/envs/minipile/lib/python3.12/site-packages (from lm-eval) (1.5.2)\n",
      "Collecting sqlitedict (from lm-eval)\n",
      "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.8 in /mnt/storage/miniconda3/envs/minipile/lib/python3.12/site-packages (from lm-eval) (2.5.1)\n",
      "Collecting tqdm-multiprocess (from lm-eval)\n",
      "  Downloading tqdm_multiprocess-0.0.11-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: transformers>=4.1 in /mnt/storage/miniconda3/envs/minipile/lib/python3.12/site-packages (from lm-eval) (4.46.2)\n",
      "Collecting zstandard (from lm-eval)\n",
      "  Downloading zstandard-0.23.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: dill in /mnt/storage/miniconda3/envs/minipile/lib/python3.12/site-packages (from lm-eval) (0.3.8)\n",
      "Collecting word2number (from lm-eval)\n",
      "  Downloading word2number-1.1.zip (9.7 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting more-itertools (from lm-eval)\n",
      "  Downloading more_itertools-10.5.0-py3-none-any.whl.metadata (36 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /mnt/storage/miniconda3/envs/minipile/lib/python3.12/site-packages (from accelerate>=0.26.0->lm-eval) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /mnt/storage/miniconda3/envs/minipile/lib/python3.12/site-packages (from accelerate>=0.26.0->lm-eval) (24.2)\n",
      "Requirement already satisfied: psutil in /home/marcus/.local/lib/python3.12/site-packages (from accelerate>=0.26.0->lm-eval) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in /home/marcus/.local/lib/python3.12/site-packages (from accelerate>=0.26.0->lm-eval) (6.0.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /mnt/storage/miniconda3/envs/minipile/lib/python3.12/site-packages/huggingface_hub-0.26.2-py3.8.egg (from accelerate>=0.26.0->lm-eval) (0.26.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /mnt/storage/miniconda3/envs/minipile/lib/python3.12/site-packages (from accelerate>=0.26.0->lm-eval) (0.4.5)\n",
      "Requirement already satisfied: filelock in /mnt/storage/miniconda3/envs/minipile/lib/python3.12/site-packages (from datasets>=2.16.0->lm-eval) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /mnt/storage/miniconda3/envs/minipile/lib/python3.12/site-packages (from datasets>=2.16.0->lm-eval) (16.1.0)\n",
      "Requirement already satisfied: pandas in /mnt/storage/miniconda3/envs/minipile/lib/python3.12/site-packages (from datasets>=2.16.0->lm-eval) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /mnt/storage/miniconda3/envs/minipile/lib/python3.12/site-packages (from datasets>=2.16.0->lm-eval) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /mnt/storage/miniconda3/envs/minipile/lib/python3.12/site-packages (from datasets>=2.16.0->lm-eval) (4.67.0)\n",
      "Requirement already satisfied: xxhash in /mnt/storage/miniconda3/envs/minipile/lib/python3.12/site-packages (from datasets>=2.16.0->lm-eval) (2.0.2)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /mnt/storage/miniconda3/envs/minipile/lib/python3.12/site-packages (from datasets>=2.16.0->lm-eval) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /mnt/storage/miniconda3/envs/minipile/lib/python3.12/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.16.0->lm-eval) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /mnt/storage/miniconda3/envs/minipile/lib/python3.12/site-packages (from datasets>=2.16.0->lm-eval) (3.10.5)\n",
      "Requirement already satisfied: responses<0.19 in /mnt/storage/miniconda3/envs/minipile/lib/python3.12/site-packages (from evaluate->lm-eval) (0.18.0)\n",
      "Collecting absl-py (from rouge-score>=0.0.4->lm-eval)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting nltk (from rouge-score>=0.0.4->lm-eval)\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/marcus/.local/lib/python3.12/site-packages (from rouge-score>=0.0.4->lm-eval) (1.16.0)\n",
      "Collecting portalocker (from sacrebleu>=1.5.0->lm-eval)\n",
      "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: regex in /mnt/storage/miniconda3/envs/minipile/lib/python3.12/site-packages (from sacrebleu>=1.5.0->lm-eval) (2024.9.11)\n",
      "Collecting tabulate>=0.8.9 (from sacrebleu>=1.5.0->lm-eval)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: colorama in /mnt/storage/miniconda3/envs/minipile/lib/python3.12/site-packages (from sacrebleu>=1.5.0->lm-eval) (0.4.6)\n",
      "Collecting lxml (from sacrebleu>=1.5.0->lm-eval)\n",
      "  Downloading lxml-5.3.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /mnt/storage/miniconda3/envs/minipile/lib/python3.12/site-packages (from scikit-learn>=0.24.1->lm-eval) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /mnt/storage/miniconda3/envs/minipile/lib/python3.12/site-packages (from scikit-learn>=0.24.1->lm-eval) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /mnt/storage/miniconda3/envs/minipile/lib/python3.12/site-packages (from scikit-learn>=0.24.1->lm-eval) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /mnt/storage/miniconda3/envs/minipile/lib/python3.12/site-packages (from torch>=1.8->lm-eval) (4.11.0)\n",
      "Requirement already satisfied: setuptools in /mnt/storage/miniconda3/envs/minipile/lib/python3.12/site-packages (from torch>=1.8->lm-eval) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /mnt/storage/miniconda3/envs/minipile/lib/python3.12/site-packages (from torch>=1.8->lm-eval) (1.13.1)\n",
      "Requirement already satisfied: networkx in /mnt/storage/miniconda3/envs/minipile/lib/python3.12/site-packages (from torch>=1.8->lm-eval) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /mnt/storage/miniconda3/envs/minipile/lib/python3.12/site-packages (from torch>=1.8->lm-eval) (3.1.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /mnt/storage/miniconda3/envs/minipile/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.8->lm-eval) (1.3.0)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /mnt/storage/miniconda3/envs/minipile/lib/python3.12/site-packages (from transformers>=4.1->lm-eval) (0.20.1)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /home/marcus/.local/lib/python3.12/site-packages (from jsonlines->lm-eval) (23.2.0)\n",
      "Collecting DataProperty<2,>=1.0.1 (from pytablewriter->lm-eval)\n",
      "  Downloading DataProperty-1.0.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting mbstrdecoder<2,>=1.0.0 (from pytablewriter->lm-eval)\n",
      "  Downloading mbstrdecoder-1.1.3-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting pathvalidate<4,>=2.3.0 (from pytablewriter->lm-eval)\n",
      "  Downloading pathvalidate-3.2.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting tabledata<2,>=1.3.1 (from pytablewriter->lm-eval)\n",
      "  Downloading tabledata-1.3.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting tcolorpy<1,>=0.0.5 (from pytablewriter->lm-eval)\n",
      "  Downloading tcolorpy-0.1.6-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting typepy<2,>=1.3.2 (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm-eval)\n",
      "  Downloading typepy-1.3.2-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /mnt/storage/miniconda3/envs/minipile/lib/python3.12/site-packages (from aiohttp->datasets>=2.16.0->lm-eval) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /mnt/storage/miniconda3/envs/minipile/lib/python3.12/site-packages (from aiohttp->datasets>=2.16.0->lm-eval) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /mnt/storage/miniconda3/envs/minipile/lib/python3.12/site-packages (from aiohttp->datasets>=2.16.0->lm-eval) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /mnt/storage/miniconda3/envs/minipile/lib/python3.12/site-packages (from aiohttp->datasets>=2.16.0->lm-eval) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /mnt/storage/miniconda3/envs/minipile/lib/python3.12/site-packages (from aiohttp->datasets>=2.16.0->lm-eval) (1.11.0)\n",
      "Requirement already satisfied: chardet<6,>=3.0.4 in /mnt/storage/miniconda3/envs/minipile/lib/python3.12/site-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter->lm-eval) (3.0.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/storage/miniconda3/envs/minipile/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.16.0->lm-eval) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mnt/storage/miniconda3/envs/minipile/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.16.0->lm-eval) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/storage/miniconda3/envs/minipile/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.16.0->lm-eval) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/storage/miniconda3/envs/minipile/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.16.0->lm-eval) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.0 in /home/marcus/.local/lib/python3.12/site-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm-eval) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2018.9 in /mnt/storage/miniconda3/envs/minipile/lib/python3.12/site-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm-eval) (2024.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/marcus/.local/lib/python3.12/site-packages (from jinja2->torch>=1.8->lm-eval) (2.1.5)\n",
      "Requirement already satisfied: click in /mnt/storage/miniconda3/envs/minipile/lib/python3.12/site-packages (from nltk->rouge-score>=0.0.4->lm-eval) (8.1.7)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /mnt/storage/miniconda3/envs/minipile/lib/python3.12/site-packages (from pandas->datasets>=2.16.0->lm-eval) (2024.2)\n",
      "Downloading lm_eval-0.4.5-py3-none-any.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.13.2-py3-none-any.whl (320 kB)\n",
      "Downloading pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
      "Downloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n",
      "Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
      "Downloading more_itertools-10.5.0-py3-none-any.whl (60 kB)\n",
      "Downloading pytablewriter-1.2.0-py3-none-any.whl (111 kB)\n",
      "Downloading tqdm_multiprocess-0.0.11-py3-none-any.whl (9.8 kB)\n",
      "Downloading zstandard-0.23.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading DataProperty-1.0.1-py3-none-any.whl (27 kB)\n",
      "Downloading mbstrdecoder-1.1.3-py3-none-any.whl (7.8 kB)\n",
      "Downloading pathvalidate-3.2.1-py3-none-any.whl (23 kB)\n",
      "Downloading tabledata-1.3.3-py3-none-any.whl (11 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading tcolorpy-0.1.6-py3-none-any.whl (8.1 kB)\n",
      "Downloading typepy-1.3.2-py3-none-any.whl (31 kB)\n",
      "Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Downloading lxml-5.3.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
      "Building wheels for collected packages: rouge-score, sqlitedict, word2number\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=a5da87315b882d021f7bb35aecb3c17a69216eee951fb6913a6baa2d754118ad\n",
      "  Stored in directory: /home/marcus/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
      "  Building wheel for sqlitedict (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16864 sha256=ecb2330fe80841f474cd336aa4e2a0a9484de2331355de58aa0321cd1babb82a\n",
      "  Stored in directory: /home/marcus/.cache/pip/wheels/7a/6f/21/fc016aef45ffcabe27129a2252f061387cbf278d2086225a64\n",
      "  Building wheel for word2number (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5568 sha256=7866e170cb532c4db898175a15957e07196fb671b4b41f77dff868c6064a639e\n",
      "  Stored in directory: /home/marcus/.cache/pip/wheels/5b/79/fb/d25928e599c7e11fe4e00d32048cd74933f34a74c633d2aea6\n",
      "Successfully built rouge-score sqlitedict word2number\n",
      "Installing collected packages: word2number, sqlitedict, zstandard, tqdm-multiprocess, tcolorpy, tabulate, pybind11, portalocker, pathvalidate, nltk, more-itertools, mbstrdecoder, lxml, jsonlines, absl-py, typepy, sacrebleu, rouge-score, DataProperty, tabledata, peft, pytablewriter, lm-eval\n",
      "Successfully installed DataProperty-1.0.1 absl-py-2.1.0 jsonlines-4.0.0 lm-eval-0.4.5 lxml-5.3.0 mbstrdecoder-1.1.3 more-itertools-10.5.0 nltk-3.9.1 pathvalidate-3.2.1 peft-0.13.2 portalocker-2.10.1 pybind11-2.13.6 pytablewriter-1.2.0 rouge-score-0.1.2 sacrebleu-2.4.3 sqlitedict-2.1.0 tabledata-1.3.3 tabulate-0.9.0 tcolorpy-0.1.6 tqdm-multiprocess-0.0.11 typepy-1.3.2 word2number-1.1 zstandard-0.23.0\n"
     ]
    }
   ],
   "source": [
    "#! pip install transformers datasets torch accelerate evaluate wandb\n",
    "! pip install lm-eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from torch.optim import Adam\n",
    "from datasets import load_dataset\n",
    "from lm_eval import tasks, evaluator, utils\n",
    "from huggingface_hub import snapshot_download\n",
    "from transformers import AutoModelForSequenceClassification, pipeline, EvalPrediction\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, get_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/mnt/data\"\n",
    "base_path = Path(base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Download Pythia $160\\text{M}$ Untrained and Pythia $160\\text{M}$ Pile-Trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_model(down_dir: str, target_folder: str, cache_folder: str, repo_id: str, branch: str = \"main\") -> None:\n",
    "    down_dir = Path(down_dir)\n",
    "    target_dir = down_dir / target_folder\n",
    "    cache_dir = down_dir / cache_folder\n",
    "\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"Downloading {repo_id}/{branch}...\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            snapshot_download(\n",
    "                repo_id,\n",
    "                repo_type=\"model\",\n",
    "                revision=branch,\n",
    "                cache_dir=str(cache_dir),\n",
    "                local_dir=str(target_dir)\n",
    "            )\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Download attempt failed: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading EleutherAI/pythia-160m-deduped/step0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f046fd453a5842fbbed3ffc4bcf1df7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "download_model(down_dir=base_dir, target_folder=\"pythia160m_dedup_untrained\", \n",
    "               cache_folder=\"pythia160m_dedup_untrained_Cache\",\n",
    "               repo_id=\"EleutherAI/pythia-160m-deduped\", branch=\"step0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading EleutherAI/pythia-160m-deduped/main...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b21c91ff82f34bf2b4d235fd07c6580e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://huggingface.co/EleutherAI/pythia-160m/blob/main/README.md states:\n",
    "# \"[...] final step 143000 corresponds exactly to the model checkpoint on the main branch of each model.\"\n",
    "download_model(down_dir=base_dir, target_folder=\"pythia160m_dedup_pile\", \n",
    "               cache_folder=\"pythia160m_dedup_pile_Cache\",\n",
    "               repo_id=\"EleutherAI/pythia-160m-deduped\", branch=\"main\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Load MiniPile Dataset from Disk\n",
    "\n",
    "We expect the MiniPile dataset to already have been downloaded to disk at an earlier point.<br>\n",
    "The logic for this can be found in the `01_get_piles` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading minipile train + val splits from the local directory \n",
    "# https://stackoverflow.com/questions/77020278/how-to-load-a-huggingface-dataset-from-local-path\n",
    "# https://github.com/MK2112/mobileYOLOv3/blob/main/mobileyolov3-cocotext.ipynb\n",
    "# Split is named exactly like with the original dataset https://huggingface.co/datasets/JeanKaddour/minipile\n",
    "minipile_train = load_dataset(\"parquet\",\n",
    "                              data_files={\n",
    "                                  \"train\": str(base_path / \"MiniPile\" / \"data\" / \"train-*.parquet\"),\n",
    "                                  \"validation\": str(base_path / \"MiniPile\" / \"data\" / \"validation-*.parquet\"),\n",
    "                                  \"test\": str(base_path / \"MiniPile\" / \"data\" / \"test-*.parquet\")\n",
    "                              },\n",
    "                              cache_dir=str(base_path / \"MiniPile_Cache\"),\n",
    "                              split=\"train\")\n",
    "\n",
    "minipile_val = load_dataset(\"parquet\",\n",
    "                            data_files={\n",
    "                                \"train\": str(base_path / \"MiniPile\" / \"data\" / \"train-*.parquet\"),\n",
    "                                \"validation\": str(base_path / \"MiniPile\" / \"data\" / \"validation-*.parquet\"),\n",
    "                                \"test\": str(base_path / \"MiniPile\" / \"data\" / \"test-*.parquet\")\n",
    "                            },\n",
    "                            cache_dir=str(base_path / \"MiniPile_Cache\"),\n",
    "                            split=\"validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Hyperparameters for Pythia $160\\text{M}$ Untrained on MiniPile\n",
    "\n",
    "**Training Parameters for $160M$ for The Pile Deduplicated (*not* MiniPile)**<br>\n",
    "See [Pythia Paper](https://arxiv.org/abs/2304.01373) (p. 22) and [Pythia GitHub](https://github.com/EleutherAI/pythia/blob/main/models/160M/pythia-160m-deduped.yml):\n",
    "\n",
    "![](./img/pythia_train_params.png)\n",
    "\n",
    "- Each model gets exposed to $299,892,736,000 \\approx 300B$ tokens through training ($\\approx 1.5$ epochs on The Pile)\n",
    "- Batch size of $1024$ samples\n",
    "- Sequence length of $2048$\n",
    "- Adam optimizer with $\\beta_1 = 0.9$, $\\beta_2 = 0.95$, $\\epsilon = 1 \\times 10^{-8}$\n",
    "- Learning rates vary by model size:\n",
    "    - $70M$ model:  $10.0 \\times 10^{-4}$\n",
    "    - $160M$ model: $6.0 \\times 10^{-4}$\n",
    "    - $410M$ model: $3.0 \\times 10^{-4}$\n",
    "    - $1.0B$ model: $3.0 \\times 10^{-4}$\n",
    "    - $1.4B$ model: $2.0 \\times 10^{-4}$\n",
    "    - $2.8B$ model: $1.6 \\times 10^{-4}$\n",
    "    - $6.9B$ model: $1.2 \\times 10^{-4}$\n",
    "    - $12B$ model:  $1.2 \\times 10^{-4}$\n",
    "- train-iters $143000$\n",
    "- lr-decay-iters $143000$\n",
    "- lr-decay-style $\\text{cosine}$\n",
    "- lr-warmup $0.01$\n",
    "- weight-decay $0.01$\n",
    "- gradient-clipping $1.0$\n",
    "- lr-min $0.1 \\times \\text{optimizer.params.lr}$ (which isn't in the paper)\n",
    "- synchronize-each-layer $\\text{True}$ (i.e. gradients across all GPUs after each layer synced)\n",
    "- LR Scheduling: Decays to a minimum of $0.1\\times$ the maximum learning rate for all models\n",
    "- (Tokenizer is loaded as the same as for GPT-NeoX-20B)\n",
    "\n",
    "**Training Parameters for $160M$ for MiniPile**<br>\n",
    "See [MiniPile paper](https://arxiv.org/abs/2304.08442)\n",
    "- $1M/500/10k$ training/validation/test examples\n",
    "    - Vocab size: $32309614$\n",
    "    - Median document length: $294$\n",
    "    - Longest document length: $929633$\n",
    "\n",
    "**BERT Training Parameters for MiniPile**\n",
    "- Adam, $\\beta_1 = 0.9$, $\\beta_2 = 0.98$, $\\epsilon = 1 \\times 10^{-12}$\n",
    "- weight-decay $0.001$\n",
    "- One cycle policy with peak learning rate of $1 \\times 10^{-3}$\n",
    "- gradient-clipping $0.5$\n",
    "- Progressive batch size from $128$ to $4096$ with a linear increase over the course of training up to $300k$ steps, no warmup\n",
    "- $800k$ total training steps\n",
    "- weight averaging of the $k = 5$ latest checkpoints and $1k$ steps distance between them\n",
    "\n",
    "**T5 Training Parameters for MiniPile**\n",
    "- AdamW, matrix-wise LR scaling by its root mean square (RMS), no weight decay\n",
    "- base learning rate $0.02$\n",
    "- cosine schedule with final of $1 \\times 10^{-5}$\n",
    "- gradient-clipping $1.0$\n",
    "- batch size $288$\n",
    "- $10k$ warmup steps, $65536$ total training steps\n",
    "- weight averaging of the $k = 5$ latest checkpoints and $1k$ steps distance between them (akin to BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These training parameters are a good start, but they can only be interpreted as at most guiding, because they were applied for decoder-only and encoder-decoder models, yet not for pure decoder-only models like Pythia. Thus, if possible, one should look for approaches trained solely on MiniPile following the decoder-only paradigm for a more accurate guide to our own approach with Pythia. \n",
    "\n",
    "Luckily there exists a [GPT NeoX 122M MiniPile](https://huggingface.co/euclaise/gpt-neox-122m-minipile-digits) model that can be reverse-engineered for our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped for version mismatch.\n"
     ]
    }
   ],
   "source": [
    "# Load the training arguments from the minipile-trained decode reference model GPT-NeoX-122M:\n",
    "# https://huggingface.co/euclaise/gpt-neox-122m-minipile-digits\n",
    "\n",
    "# Newer versions fail for missing attributes, 4.30.0 is documented to have been used\n",
    "if str(transformers.__version__) == \"4.30.0\":\n",
    "    training_args = torch.load(base_path / 'training_args_gptNEO122m.bin', weights_only=False)\n",
    "    output_file = 'train_args_gptNEO122m_minipile.txt'\n",
    "    try: \n",
    "        with open(output_file, 'w') as f:\n",
    "            f.write(\"TrainingArguments attributes:\\n\")\n",
    "            for attr in dir(training_args):\n",
    "                if hasattr(training_args, attr) and not attr.startswith('_'):\n",
    "                    value = getattr(training_args, attr)\n",
    "                    f.write(f\"- {attr}: {value}\\n\")\n",
    "    except NameError as _:\n",
    "        pass # Fully ignore NameError, appears every time\n",
    "else:\n",
    "    print('Skipped for version mismatch.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GPTNeoX model card is a bit misleading, as it is stated that this model was trained exclusively on MiniPile. The tiny learning rate $5 \\times 10^{-6}$ with no weight decay implies a fine-tuning approach.\n",
    "\n",
    "I did this mostly to get a feeling for much the encoder-based model params deviate from the decoder-based model params.<br>\n",
    "I interpret the results as not too far off, e.g. we use the exact same learning rate and optimizer.<br>\n",
    "\n",
    "This implies that the training params on The Pile for Pythia $160M$ are a good starting point and we can scale these to accommodate the MiniPile dataset size and expect appropriate training effects.\n",
    "\n",
    "Core parameters are however not directly transferable: `train-iters` and therefore also `lr-decay-iters`.<br>\n",
    "For Pile deduplicated this was $143000$, but we have to scale this to the MiniPile dataset size, as the number of tokens processed by the model is crucial for the training process and could lead to overfitting and not accurately reflecting dataset knowledge retention capabilities if not adjusted properly.\n",
    "\n",
    "In other words, overshooting distorts dataset knowledge, while undershooting leads to underfitting and insufficient representation of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Byte-based scale factor: 139.609153x\n",
      "MiniPile (scaled) Train-Iters/LR-Decay-Iters: 1024.288 ~ 1024\n"
     ]
    }
   ],
   "source": [
    "# I use the byte sizes as proxy for the number of tokens, as both datasets will get tokenized with the same tokenizer\n",
    "minipile_train_bytes = 5906108510 # see https://huggingface.co/datasets/JeanKaddour/minipile/blob/main/README.md\n",
    "pile_train_bytes = 824546807506   # see https://huggingface.co/datasets/EleutherAI/the_pile_deduplicated/blob/main/dataset_infos.json\n",
    "pile_effective_epochs = 1.5       # this many epochs are actually trained in the original model (calculation isn't affected, training params below are)\n",
    "\n",
    "scale_factor = (pile_train_bytes * pile_effective_epochs) / (minipile_train_bytes * pile_effective_epochs)\n",
    "print(f\"Byte-based scale factor: {scale_factor:10.6f}x\")\n",
    "print(f\"MiniPile (scaled) Train-Iters/LR-Decay-Iters: {143000 / scale_factor:.3f} ~ {round(143000 / scale_factor)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point the $1024$ for training iterations may seem awkwardly small.<br>\n",
    "But, to reiterate, we strictly scaled it down iterations according to dataset size difference.\n",
    "\n",
    "While this may seem horrible in most other cases, as we thoroughly neuter exposure to data, this scale-correct limiting and overall lower exposure is exactly what we need here to operate relative to the original Pythia training. After all, the goal is to compare knowledge retention and generalization capabilities achievable on `The Pile Deduplicated` vs. the 'distilled' `MiniPile` under size-appropriate, similar conditions. Therefore, scaling the `train-iters` and therefore also `lr-decay-iters` using byte sizes as a proxy is actually appropriate here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now lay out the complete parameters:<br>\n",
    "With the three approach descriptions retrieved, we can take a more educated guess at the training params for Pythia $160M$ on MiniPile:\n",
    "\n",
    "- Adam optimizer (GPT NeoX and T5-Base MiniPile suggest the 'generally more stable' AdamW, but Pythia uses Adam so we keep it most similar)\n",
    "    - $\\beta_1 = 0.9$, $\\beta_2 = 0.95$, (Pythia)\n",
    "    - $\\epsilon = 1 \\times 10^{-8}$ (GPT NeoX and Pythia)\n",
    "    - learning rate $6 \\times 10^{-4}$ (Pythia)\n",
    "    - lr-schedule $\\text{cosine annealing}$ (Pythia)\n",
    "    - lr-warmup $0.01$ of total steps (Pythia)\n",
    "    - lr-min $0.1 \\times \\text{lr}$ (Pythia)\n",
    "    - weight-decay $1 \\times 10^{-2}$ (Pythia)\n",
    "- gradient-clipping $1.0$ (Pythia)\n",
    "- batch size $1024$ (Pythia, probably grad accum needed, expect multi-GPU)\n",
    "- sequence length $2048$ (Pythia)\n",
    "- **train-iters: $1024$ (MiniPile-specific)**\n",
    "- **lr-decay-iters: same as train-iters (MiniPile-specific)**\n",
    "- (won't do mixed precision for sake of most similar training conditions to Pile-trained Pythia)\n",
    "- (won't do weight averaging)\n",
    "- **Same GPT-NeoX-20B tokenizer as for Pythia-Pile**\n",
    "\n",
    "We can start training Pythia $160\\text{M}$ on MiniPile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Train Pythia $160\\text{M}$ Untrained on MiniPile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the untrained Pythia 160M tokenizer and model\n",
    "# https://stackoverflow.com/questions/64001128/load-a-pre-trained-model-from-disk-with-huggingface-transformers\n",
    "# Tokenizer is in fact a GPTNeoXTokenizer, only has a fast version available\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_path / \"pythia160m_dedup_untrained\", use_fast=True, local_files_only=True)\n",
    "empty_model = AutoModelForCausalLM.from_pretrained(base_path / \"pythia160m_dedup_untrained\", local_files_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pythia paper states a standard configuration where individual examples consist of up to $2048$ tokens.<br>\n",
    "This explains why the tokenizer doesn't contain a padding token, as the model is trained on variable-length sequences with this upper bound instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer doesn't have a pad token, use EOS as a substitute\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "def tokenize(example): \n",
    "    # seq_len = max_length = 2048 (as upper boundary, so not strict size -> no padding needed)\n",
    "    return tokenizer(example[\"text\"], \n",
    "                     truncation=True, \n",
    "                     max_length=2048,\n",
    "                     return_special_tokens_mask=True)\n",
    "\n",
    "if os.path.exists(base_path / \"minipile_train_tokenized\"):\n",
    "    minipile_train_tokenized = load_dataset(\"arrow\", data_files=str(base_path / \"minipile_train_tokenized/*.arrow\"), split=\"train\")\n",
    "    minipile_val_tokenized = load_dataset(\"arrow\", data_files=str(base_path / \"minipile_val_tokenized/*.arrow\"), split=\"train\")\n",
    "else:\n",
    "    minipile_train_tokenized = minipile_train.map(tokenize, batched=True, remove_columns=minipile_train.column_names) # retain only new fields from tokenization\n",
    "    minipile_val_tokenized = minipile_val.map(tokenize, batched=True, remove_columns=minipile_val.column_names)\n",
    "    minipile_train_tokenized.save_to_disk(base_path / \"minipile_train_tokenized\")\n",
    "    minipile_val_tokenized.save_to_disk(base_path / \"minipile_val_tokenized\")\n",
    "\n",
    "# Dynamic padding during training (mlm -> mask language model -> we're doing causal here)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs: 1\n",
      "\n",
      "GPU 0:\tNVIDIA GeForce RTX 3060\n",
      "\tTotal memory:\t\t11.76 GiB\n",
      "\tAllocated memory:\t 0.00 GiB\n",
      "\tFree memory:\t\t11.76 GiB\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    device_count = torch.cuda.device_count()\n",
    "    print(f\"Available GPUs: {device_count}\")\n",
    "    for i in range(device_count):\n",
    "        device = torch.device(f'cuda:{i}')\n",
    "        device_properties = torch.cuda.get_device_properties(device)\n",
    "        total_mem = device_properties.total_memory / (1024 ** 3)\n",
    "        allocd_mem = torch.cuda.memory_allocated(device) / (1024 ** 3)\n",
    "        free_mem = total_mem - allocd_mem\n",
    "        print(f\"\\nGPU {i}:\\t{device_properties.name}\")\n",
    "        print(f\"\\tTotal memory:\\t\\t{total_mem:.2f} GiB\")\n",
    "        print(f\"\\tAllocated memory:\\t{allocd_mem:5.2f} GiB\")\n",
    "        print(f\"\\tFree memory:\\t\\t{free_mem:.2f} GiB\")\n",
    "else:\n",
    "    print(\"No CUDA-capable GPUs available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = str(base_path / \"pythia160m_minipile_trained\")\n",
    "log_dir = str(base_path / \"160m_minipile_logs\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# https://huggingface.co/docs/transformers/v4.46.0/en/main_classes/trainer#transformers.TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1.5,            # Since train_iters gets set, use num_train_epochs=1.5 like for The Pile\n",
    "    per_device_train_batch_size=4,   # Gives an effective batch size of 1024 after grad accum\n",
    "    per_device_eval_batch_size=4,    # Same as training batch size\n",
    "    gradient_accumulation_steps=256, # Achieve a batch size of 1024\n",
    "    learning_rate=6e-4,              # Default Pythia 160M\n",
    "    weight_decay=0.01,               # Default Pythia 160M\n",
    "    max_steps=1024,                  # Adjusted for MiniPile (https://discuss.huggingface.co/t/how-does-max-steps-affect-the-number-of-samples-the-model-sees/69681)\n",
    "    lr_scheduler_type=\"cosine\",      # As per Pythia 160M paper\n",
    "    warmup_steps=int(0.01 * 1024),   # 1% of total steps for warmup\n",
    "    logging_dir=log_dir,\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,     # Frequency for evaluation during training\n",
    "    save_steps=1024,    # Save at the end of training\n",
    "    save_total_limit=1, # Only keep the most recent checkpoint\n",
    "    fp16=True,         # Not using mixed precision for comparable conditions\n",
    "    report_to=None,     # Noting this for later iterations, maybe set this as \"wandb\", \"tensorboard\" or smth\n",
    "    ddp_find_unused_parameters=False, # see https://discuss.pytorch.org/t/how-to-change-ddp-parameter-find-unused-parameters-true-to-false-during-training/130763\n",
    "    max_grad_norm=1.0,  # As per Pythia 160M paper\n",
    ")\n",
    "\n",
    "# Ensure training across multiple GPUs if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "empty_model = empty_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(empty_model.parameters(), lr=training_args.learning_rate, betas=(0.9, 0.95), eps=1e-8, weight_decay=0.01)\n",
    "\n",
    "# Train Pythia 160M Untrained on MiniPile\n",
    "# https://huggingface.co/docs/transformers/v4.46.0/en/main_classes/trainer\n",
    "trainer = Trainer(model=empty_model,\n",
    "                  args=training_args,\n",
    "                  train_dataset=minipile_train_tokenized,\n",
    "                  eval_dataset=minipile_val_tokenized,\n",
    "                  data_collator=data_collator,\n",
    "                  optimizers=(optimizer, None))\n",
    "\n",
    "scheduler = get_scheduler(name=training_args.lr_scheduler_type,\n",
    "                          optimizer=optimizer,\n",
    "                          num_warmup_steps=training_args.warmup_steps,\n",
    "                          num_training_steps=training_args.max_steps)\n",
    "\n",
    "num_batches = len(trainer.get_train_dataloader())  # Number of batches\n",
    "total_training_steps = num_batches * training_args.gradient_accumulation_steps * int(training_args.num_train_epochs)\n",
    "\n",
    "# Training loop with manual minimum learning rate enforcement\n",
    "for epoch in range(int(training_args.num_train_epochs)):\n",
    "    with tqdm(total=total_training_steps, desc=f\"Training Epoch {epoch + 1}/{int(training_args.num_train_epochs)}\") as pbar:\n",
    "        for _, batch in enumerate(trainer.get_train_dataloader()):\n",
    "            trainer.training_step(trainer.model, batch)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            for param_group in optimizer.param_groups:\n",
    "                # Manually ... ensure lr doesn't go below min_lr (Pythia wants this)\n",
    "                param_group['lr'] = max(param_group['lr'], 0.1 * training_args.learning_rate)\n",
    "            pbar.update(1)\n",
    "\n",
    "# Why is this a two-step process?!\n",
    "trainer.save_model(str(base_path / \"pythia160m_minipile_trained\")) # This saves the model weights\n",
    "tokenizer.save_pretrained(str(base_path / \"pythia160m_minipile_trained\")) # This saves the tokenizer (don't know if needed, better save than sorry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training of Pythia $160\\text{M}$ on MiniPile was done on `gruenau8` with the `02_train_160M.py` script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Evaluate Pythia $160\\text{M}$ MiniPile vs. Pythia $160\\text{M}$ Pile-Trained on Benchmarks\n",
    "\n",
    "### AI2 Reasoning Challenge (ARC-Challenge)\n",
    "[Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge (Clark, et al. 2018)](https://arxiv.org/abs/1803.05457#)\n",
    "- grade-school science questions, require reasoning efforts beyond just information retrieval\n",
    "- tests specifically for reaction to new concepts through cross-domain generalization\n",
    "- requires broader domain knowledge, reasoning capabilities and combinatory skills for responses\n",
    "- will help evaluate assessing whether a MiniPile-trained model retains *the flexibility and cross-domain generalization capabilities* of The Pile-trained model\n",
    "\n",
    "### Massive Multitask Language Understanding (MMLU)\n",
    "[Measuring Massive Multitask Language Understanding (Hendrycks, et al. 2021)](https://arxiv.org/abs/2009.03300)\n",
    "- comprehensive, inputs/examples span $57$ different tasks\n",
    "- specifically testing across *all* subjects\n",
    "- will help evaluate assessing whether a MiniPile-trained model retains *specifically the width of knowledge* of The Pile-trained model\n",
    "\n",
    "### HellaSwag\n",
    "[HellaSwag: Can a Machine *Really* Finish Your Sentence (Zellers, et al. 2019)](https://aclanthology.org/P19-1472.pdf)\n",
    "- presenting models with a situation and asking them to choose the most plausible continuation\n",
    "- I've seen in Karpathy's series, thus new it was applicable to decoder-only models and thus to Pythia\n",
    "- tests for context understanding, conversational capabilities and generalization\n",
    "\n",
    "### WinoGrande\n",
    "[WinoGrande: An Adversarial Winograd Schema Challenge at Scale (Sakaguchi, et al. 2019)](https://arxiv.org/abs/1907.10641)\n",
    "- testing the ability to determine pronouns based on commonsense reasoning\n",
    "- checks whether beyond dataset contents, the model can just as deeply perceive/learn language understanding and reasoning capabilities\n",
    "- helps us solve the question of whether reducing MiniPile to most relevant data with \"topic-selective k-Means\" still retains an overall language understanding\n",
    "\n",
    "### Language Model Benchmark for Autoregressive Data Analysis (Lambada (OpenAI))\n",
    "[The LAMBADA dataset: Word prediction requiring a broad discourse context (Paperno, et al. 2016)](https://arxiv.org/abs/1606.06031)\n",
    "- evaluates a model's ability to predict a last word of some text, where generating the right word requires long-range context understanding\n",
    "- tests for context understanding, conversational capabilities and generalization\n",
    "- not as directly applicable to our problem set here, but helps calibrate the benchmark pipeline, because numbers were reported for Pythia $160M$ on this benchmark\n",
    "- this in turn gives more credibility and a better look into the MiniPile Pythia's performance overall\n",
    "- maybe reducing dataset size can disturb long range context processing, this helps us evaluate that\n",
    "- using the OpenAI version for cross-referencability with the Pythia paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lm_eval import utils, simple_evaluate\n",
    "from lm_eval.models.huggingface import HFLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-13:13:16:08,313 WARNING  [huggingface.py:95] `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "2024-11-13:13:16:08,314 INFO     [huggingface.py:481] Using model type 'default'\n",
      "2024-11-13:13:16:08,325 WARNING  [huggingface.py:275] Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "2024-11-13:13:16:08,328 INFO     [evaluator.py:164] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2024-11-13:13:16:08,329 INFO     [evaluator.py:217] Using pre-initialized model\n",
      "2024-11-13:13:16:14,701 INFO     [__init__.py:459] The tag 'arc_ca' is already registered as a group, this tag will not be registered. This may affect tasks you want to call.\n",
      "2024-11-13:13:16:14,708 INFO     [__init__.py:459] The tag 'arc_ca' is already registered as a group, this tag will not be registered. This may affect tasks you want to call.\n",
      "Using the latest cached version of the dataset since allenai/ai2_arc couldn't be found on the Hugging Face Hub\n",
      "2024-11-13:13:16:19,690 WARNING  [load.py:1431] Using the latest cached version of the dataset since allenai/ai2_arc couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'ARC-Challenge' at /home/marcus/.cache/huggingface/datasets/allenai___ai2_arc/ARC-Challenge/0.0.0/210d026faf9955653af8916fad021475a3f00453 (last modified on Tue Nov 12 21:57:47 2024).\n",
      "2024-11-13:13:16:19,693 WARNING  [cache.py:94] Found the latest cached dataset configuration 'ARC-Challenge' at /home/marcus/.cache/huggingface/datasets/allenai___ai2_arc/ARC-Challenge/0.0.0/210d026faf9955653af8916fad021475a3f00453 (last modified on Tue Nov 12 21:57:47 2024).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fa0588bbf9947889ed65aa7dae4f1e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/7.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "420c253f64dd435b8aa6d46ed1f6680c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00002.parquet:   0%|          | 0.00/269M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c99a188701e496b8ccbd64159a591fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00001-of-00002.parquet:   0%|          | 0.00/281M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2d17aa13923401fac0ba83dd44c235e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/1.14M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fbf091d94ee436390bc94eb6dbfadf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/1.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb3615b83b00479d90889694bfa86004",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/2662 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "805a73082e7042b281faf0e1b21899c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/5153 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ae1cfb7909c416699c4ca9bc6cf51c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/4869 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7affd17df7ac47179d9fdd95cbad4654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0000.parquet:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5943595f2e034c38b8bda546e824d72f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/5153 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-13:13:19:53,234 WARNING  [task.py:325] [Task: lambada_openai] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
      "2024-11-13:13:19:53,235 WARNING  [task.py:325] [Task: lambada_openai] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
      "2024-11-13:13:19:53,529 WARNING  [evaluator.py:270] Overwriting default num_fewshot of lambada_openai from None to 0\n",
      "2024-11-13:13:19:53,529 WARNING  [evaluator.py:270] Overwriting default num_fewshot of lambada_standard from None to 0\n",
      "2024-11-13:13:19:53,530 WARNING  [evaluator.py:270] Overwriting default num_fewshot of hellaswag from None to 0\n",
      "2024-11-13:13:19:53,531 WARNING  [evaluator.py:270] Overwriting default num_fewshot of winogrande from None to 0\n",
      "2024-11-13:13:19:53,532 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_anatomy from None to 0\n",
      "2024-11-13:13:19:53,533 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_astronomy from None to 0\n",
      "2024-11-13:13:19:53,534 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_college_chemistry from None to 0\n",
      "2024-11-13:13:19:53,534 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_computer_science from None to 0\n",
      "2024-11-13:13:19:53,535 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_electrical_engineering from None to 0\n",
      "2024-11-13:13:19:53,536 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_computer_security from None to 0\n",
      "2024-11-13:13:19:53,537 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_conceptual_physics from None to 0\n",
      "2024-11-13:13:19:53,538 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_college_physics from None to 0\n",
      "2024-11-13:13:19:53,538 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_physics from None to 0\n",
      "2024-11-13:13:19:53,539 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_abstract_algebra from None to 0\n",
      "2024-11-13:13:19:53,540 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_statistics from None to 0\n",
      "2024-11-13:13:19:53,541 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_college_computer_science from None to 0\n",
      "2024-11-13:13:19:53,541 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_college_mathematics from None to 0\n",
      "2024-11-13:13:19:53,542 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_chemistry from None to 0\n",
      "2024-11-13:13:19:53,542 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_college_biology from None to 0\n",
      "2024-11-13:13:19:53,543 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_machine_learning from None to 0\n",
      "2024-11-13:13:19:53,544 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_elementary_mathematics from None to 0\n",
      "2024-11-13:13:19:53,544 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_biology from None to 0\n",
      "2024-11-13:13:19:53,545 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_mathematics from None to 0\n",
      "2024-11-13:13:19:53,545 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_management from None to 0\n",
      "2024-11-13:13:19:53,546 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_global_facts from None to 0\n",
      "2024-11-13:13:19:53,547 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_marketing from None to 0\n",
      "2024-11-13:13:19:53,547 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_nutrition from None to 0\n",
      "2024-11-13:13:19:53,548 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_clinical_knowledge from None to 0\n",
      "2024-11-13:13:19:53,549 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_medical_genetics from None to 0\n",
      "2024-11-13:13:19:53,550 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_virology from None to 0\n",
      "2024-11-13:13:19:53,551 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_miscellaneous from None to 0\n",
      "2024-11-13:13:19:53,551 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_college_medicine from None to 0\n",
      "2024-11-13:13:19:53,552 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_business_ethics from None to 0\n",
      "2024-11-13:13:19:53,553 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_professional_accounting from None to 0\n",
      "2024-11-13:13:19:53,554 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_human_aging from None to 0\n",
      "2024-11-13:13:19:53,555 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_professional_medicine from None to 0\n",
      "2024-11-13:13:19:53,557 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_geography from None to 0\n",
      "2024-11-13:13:19:53,558 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_professional_psychology from None to 0\n",
      "2024-11-13:13:19:53,559 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_public_relations from None to 0\n",
      "2024-11-13:13:19:53,559 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_government_and_politics from None to 0\n",
      "2024-11-13:13:19:53,560 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_security_studies from None to 0\n",
      "2024-11-13:13:19:53,561 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_sociology from None to 0\n",
      "2024-11-13:13:19:53,562 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_psychology from None to 0\n",
      "2024-11-13:13:19:53,563 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_human_sexuality from None to 0\n",
      "2024-11-13:13:19:53,563 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_macroeconomics from None to 0\n",
      "2024-11-13:13:19:53,564 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_us_foreign_policy from None to 0\n",
      "2024-11-13:13:19:53,565 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_econometrics from None to 0\n",
      "2024-11-13:13:19:53,566 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_microeconomics from None to 0\n",
      "2024-11-13:13:19:53,567 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_moral_disputes from None to 0\n",
      "2024-11-13:13:19:53,568 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_international_law from None to 0\n",
      "2024-11-13:13:19:53,568 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_logical_fallacies from None to 0\n",
      "2024-11-13:13:19:53,569 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_world_history from None to 0\n",
      "2024-11-13:13:19:53,570 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_world_religions from None to 0\n",
      "2024-11-13:13:19:53,571 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_moral_scenarios from None to 0\n",
      "2024-11-13:13:19:53,571 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_formal_logic from None to 0\n",
      "2024-11-13:13:19:53,572 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_european_history from None to 0\n",
      "2024-11-13:13:19:53,573 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_professional_law from None to 0\n",
      "2024-11-13:13:19:53,574 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_philosophy from None to 0\n",
      "2024-11-13:13:19:53,575 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_us_history from None to 0\n",
      "2024-11-13:13:19:53,575 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_prehistory from None to 0\n",
      "2024-11-13:13:19:53,576 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_jurisprudence from None to 0\n",
      "2024-11-13:13:19:53,577 WARNING  [evaluator.py:270] Overwriting default num_fewshot of arc_challenge from None to 0\n",
      "2024-11-13:13:19:53,585 INFO     [task.py:415] Building contexts for lambada_openai on rank 0...\n",
      "100%|██████████| 5153/5153 [00:14<00:00, 350.22it/s]\n",
      "2024-11-13:13:20:08,591 INFO     [task.py:415] Building contexts for lambada_standard on rank 0...\n",
      "100%|██████████| 5153/5153 [00:14<00:00, 351.16it/s]\n",
      "2024-11-13:13:20:23,603 INFO     [task.py:415] Building contexts for hellaswag on rank 0...\n",
      "100%|██████████| 10042/10042 [00:06<00:00, 1632.85it/s]\n",
      "2024-11-13:13:20:32,752 INFO     [task.py:415] Building contexts for winogrande on rank 0...\n",
      "100%|██████████| 1267/1267 [00:00<00:00, 43889.85it/s]\n",
      "2024-11-13:13:20:32,917 INFO     [task.py:415] Building contexts for mmlu_anatomy on rank 0...\n",
      "100%|██████████| 135/135 [00:00<00:00, 383.77it/s]\n",
      "2024-11-13:13:20:33,293 INFO     [task.py:415] Building contexts for mmlu_astronomy on rank 0...\n",
      "100%|██████████| 152/152 [00:00<00:00, 381.86it/s]\n",
      "2024-11-13:13:20:33,717 INFO     [task.py:415] Building contexts for mmlu_college_chemistry on rank 0...\n",
      "100%|██████████| 100/100 [00:00<00:00, 391.57it/s]\n",
      "2024-11-13:13:20:33,991 INFO     [task.py:415] Building contexts for mmlu_high_school_computer_science on rank 0...\n",
      "100%|██████████| 100/100 [00:00<00:00, 396.67it/s]\n",
      "2024-11-13:13:20:34,260 INFO     [task.py:415] Building contexts for mmlu_electrical_engineering on rank 0...\n",
      "100%|██████████| 145/145 [00:00<00:00, 356.68it/s]\n",
      "2024-11-13:13:20:34,690 INFO     [task.py:415] Building contexts for mmlu_computer_security on rank 0...\n",
      "100%|██████████| 100/100 [00:01<00:00, 51.91it/s]\n",
      "2024-11-13:13:20:36,636 INFO     [task.py:415] Building contexts for mmlu_conceptual_physics on rank 0...\n",
      "100%|██████████| 235/235 [00:00<00:00, 379.79it/s]\n",
      "2024-11-13:13:20:37,296 INFO     [task.py:415] Building contexts for mmlu_college_physics on rank 0...\n",
      "100%|██████████| 102/102 [00:00<00:00, 374.11it/s]\n",
      "2024-11-13:13:20:37,588 INFO     [task.py:415] Building contexts for mmlu_high_school_physics on rank 0...\n",
      "100%|██████████| 151/151 [00:00<00:00, 384.47it/s]\n",
      "2024-11-13:13:20:38,010 INFO     [task.py:415] Building contexts for mmlu_abstract_algebra on rank 0...\n",
      "100%|██████████| 100/100 [00:00<00:00, 397.45it/s]\n",
      "2024-11-13:13:20:38,278 INFO     [task.py:415] Building contexts for mmlu_high_school_statistics on rank 0...\n",
      "100%|██████████| 216/216 [00:00<00:00, 377.85it/s]\n",
      "2024-11-13:13:20:38,888 INFO     [task.py:415] Building contexts for mmlu_college_computer_science on rank 0...\n",
      "100%|██████████| 100/100 [00:00<00:00, 379.15it/s]\n",
      "2024-11-13:13:20:39,171 INFO     [task.py:415] Building contexts for mmlu_college_mathematics on rank 0...\n",
      "100%|██████████| 100/100 [00:00<00:00, 378.06it/s]\n",
      "2024-11-13:13:20:39,453 INFO     [task.py:415] Building contexts for mmlu_high_school_chemistry on rank 0...\n",
      "100%|██████████| 203/203 [00:00<00:00, 398.13it/s]\n",
      "2024-11-13:13:20:39,993 INFO     [task.py:415] Building contexts for mmlu_college_biology on rank 0...\n",
      "100%|██████████| 144/144 [00:00<00:00, 386.04it/s]\n",
      "2024-11-13:13:20:40,389 INFO     [task.py:415] Building contexts for mmlu_machine_learning on rank 0...\n",
      "100%|██████████| 112/112 [00:00<00:00, 405.31it/s]\n",
      "2024-11-13:13:20:40,684 INFO     [task.py:415] Building contexts for mmlu_elementary_mathematics on rank 0...\n",
      "100%|██████████| 378/378 [00:00<00:00, 380.25it/s]\n",
      "2024-11-13:13:20:41,733 INFO     [task.py:415] Building contexts for mmlu_high_school_biology on rank 0...\n",
      "100%|██████████| 310/310 [00:00<00:00, 390.60it/s]\n",
      "2024-11-13:13:20:42,570 INFO     [task.py:415] Building contexts for mmlu_high_school_mathematics on rank 0...\n",
      "100%|██████████| 270/270 [00:00<00:00, 386.91it/s]\n",
      "2024-11-13:13:20:43,306 INFO     [task.py:415] Building contexts for mmlu_management on rank 0...\n",
      "100%|██████████| 103/103 [00:00<00:00, 382.66it/s]\n",
      "2024-11-13:13:20:43,597 INFO     [task.py:415] Building contexts for mmlu_global_facts on rank 0...\n",
      "100%|██████████| 100/100 [00:00<00:00, 348.94it/s]\n",
      "2024-11-13:13:20:43,903 INFO     [task.py:415] Building contexts for mmlu_marketing on rank 0...\n",
      "100%|██████████| 234/234 [00:00<00:00, 386.08it/s]\n",
      "2024-11-13:13:20:44,544 INFO     [task.py:415] Building contexts for mmlu_nutrition on rank 0...\n",
      "100%|██████████| 306/306 [00:00<00:00, 381.85it/s]\n",
      "2024-11-13:13:20:45,393 INFO     [task.py:415] Building contexts for mmlu_clinical_knowledge on rank 0...\n",
      "100%|██████████| 265/265 [00:00<00:00, 366.23it/s]\n",
      "2024-11-13:13:20:46,170 INFO     [task.py:415] Building contexts for mmlu_medical_genetics on rank 0...\n",
      "100%|██████████| 100/100 [00:00<00:00, 383.19it/s]\n",
      "2024-11-13:13:20:46,450 INFO     [task.py:415] Building contexts for mmlu_virology on rank 0...\n",
      "100%|██████████| 166/166 [00:00<00:00, 390.79it/s]\n",
      "2024-11-13:13:20:46,906 INFO     [task.py:415] Building contexts for mmlu_miscellaneous on rank 0...\n",
      "100%|██████████| 783/783 [00:02<00:00, 382.25it/s]\n",
      "2024-11-13:13:20:49,060 INFO     [task.py:415] Building contexts for mmlu_college_medicine on rank 0...\n",
      "100%|██████████| 173/173 [00:00<00:00, 394.34it/s]\n",
      "2024-11-13:13:20:49,530 INFO     [task.py:415] Building contexts for mmlu_business_ethics on rank 0...\n",
      "100%|██████████| 100/100 [00:00<00:00, 392.86it/s]\n",
      "2024-11-13:13:20:49,803 INFO     [task.py:415] Building contexts for mmlu_professional_accounting on rank 0...\n",
      "100%|██████████| 282/282 [00:00<00:00, 391.70it/s]\n",
      "2024-11-13:13:20:50,565 INFO     [task.py:415] Building contexts for mmlu_human_aging on rank 0...\n",
      "100%|██████████| 223/223 [00:00<00:00, 387.89it/s]\n",
      "2024-11-13:13:20:51,177 INFO     [task.py:415] Building contexts for mmlu_professional_medicine on rank 0...\n",
      "100%|██████████| 272/272 [00:00<00:00, 379.06it/s]\n",
      "2024-11-13:13:20:51,935 INFO     [task.py:415] Building contexts for mmlu_high_school_geography on rank 0...\n",
      "100%|██████████| 198/198 [00:00<00:00, 407.86it/s]\n",
      "2024-11-13:13:20:52,450 INFO     [task.py:415] Building contexts for mmlu_professional_psychology on rank 0...\n",
      "100%|██████████| 612/612 [00:01<00:00, 401.13it/s]\n",
      "2024-11-13:13:20:54,060 INFO     [task.py:415] Building contexts for mmlu_public_relations on rank 0...\n",
      "100%|██████████| 110/110 [00:00<00:00, 404.97it/s]\n",
      "2024-11-13:13:20:54,354 INFO     [task.py:415] Building contexts for mmlu_high_school_government_and_politics on rank 0...\n",
      "100%|██████████| 193/193 [00:00<00:00, 386.57it/s]\n",
      "2024-11-13:13:20:54,881 INFO     [task.py:415] Building contexts for mmlu_security_studies on rank 0...\n",
      "100%|██████████| 245/245 [00:00<00:00, 412.77it/s]\n",
      "2024-11-13:13:20:55,510 INFO     [task.py:415] Building contexts for mmlu_sociology on rank 0...\n",
      "100%|██████████| 201/201 [00:00<00:00, 412.06it/s]\n",
      "2024-11-13:13:20:56,033 INFO     [task.py:415] Building contexts for mmlu_high_school_psychology on rank 0...\n",
      "100%|██████████| 545/545 [00:01<00:00, 411.35it/s]\n",
      "2024-11-13:13:20:57,430 INFO     [task.py:415] Building contexts for mmlu_human_sexuality on rank 0...\n",
      "100%|██████████| 131/131 [00:00<00:00, 411.27it/s]\n",
      "2024-11-13:13:20:57,770 INFO     [task.py:415] Building contexts for mmlu_high_school_macroeconomics on rank 0...\n",
      "100%|██████████| 390/390 [00:00<00:00, 398.65it/s]\n",
      "2024-11-13:13:20:58,806 INFO     [task.py:415] Building contexts for mmlu_us_foreign_policy on rank 0...\n",
      "100%|██████████| 100/100 [00:00<00:00, 396.43it/s]\n",
      "2024-11-13:13:20:59,076 INFO     [task.py:415] Building contexts for mmlu_econometrics on rank 0...\n",
      "100%|██████████| 114/114 [00:00<00:00, 400.27it/s]\n",
      "2024-11-13:13:20:59,380 INFO     [task.py:415] Building contexts for mmlu_high_school_microeconomics on rank 0...\n",
      "100%|██████████| 238/238 [00:00<00:00, 400.11it/s]\n",
      "2024-11-13:13:21:00,011 INFO     [task.py:415] Building contexts for mmlu_moral_disputes on rank 0...\n",
      "100%|██████████| 346/346 [00:00<00:00, 402.84it/s]\n",
      "2024-11-13:13:21:00,918 INFO     [task.py:415] Building contexts for mmlu_international_law on rank 0...\n",
      "100%|██████████| 121/121 [00:00<00:00, 401.36it/s]\n",
      "2024-11-13:13:21:01,239 INFO     [task.py:415] Building contexts for mmlu_logical_fallacies on rank 0...\n",
      "100%|██████████| 163/163 [00:00<00:00, 406.23it/s]\n",
      "2024-11-13:13:21:01,666 INFO     [task.py:415] Building contexts for mmlu_high_school_world_history on rank 0...\n",
      "100%|██████████| 237/237 [00:00<00:00, 405.76it/s]\n",
      "2024-11-13:13:21:02,290 INFO     [task.py:415] Building contexts for mmlu_world_religions on rank 0...\n",
      "100%|██████████| 171/171 [00:00<00:00, 384.60it/s]\n",
      "2024-11-13:13:21:02,761 INFO     [task.py:415] Building contexts for mmlu_moral_scenarios on rank 0...\n",
      "100%|██████████| 895/895 [00:02<00:00, 400.60it/s]\n",
      "2024-11-13:13:21:05,138 INFO     [task.py:415] Building contexts for mmlu_formal_logic on rank 0...\n",
      "100%|██████████| 126/126 [00:00<00:00, 410.64it/s]\n",
      "2024-11-13:13:21:05,467 INFO     [task.py:415] Building contexts for mmlu_high_school_european_history on rank 0...\n",
      "100%|██████████| 165/165 [00:00<00:00, 394.15it/s]\n",
      "2024-11-13:13:21:05,915 INFO     [task.py:415] Building contexts for mmlu_professional_law on rank 0...\n",
      "100%|██████████| 1534/1534 [00:03<00:00, 406.90it/s]\n",
      "2024-11-13:13:21:09,893 INFO     [task.py:415] Building contexts for mmlu_philosophy on rank 0...\n",
      "100%|██████████| 311/311 [00:00<00:00, 414.01it/s]\n",
      "2024-11-13:13:21:10,687 INFO     [task.py:415] Building contexts for mmlu_high_school_us_history on rank 0...\n",
      "100%|██████████| 204/204 [00:00<00:00, 413.82it/s]\n",
      "2024-11-13:13:21:11,216 INFO     [task.py:415] Building contexts for mmlu_prehistory on rank 0...\n",
      "100%|██████████| 324/324 [00:00<00:00, 417.30it/s]\n",
      "2024-11-13:13:21:12,037 INFO     [task.py:415] Building contexts for mmlu_jurisprudence on rank 0...\n",
      "100%|██████████| 108/108 [00:00<00:00, 394.37it/s]\n",
      "2024-11-13:13:21:12,328 INFO     [task.py:415] Building contexts for arc_challenge on rank 0...\n",
      "100%|██████████| 1172/1172 [00:01<00:00, 738.28it/s]\n",
      "2024-11-13:13:21:14,109 INFO     [evaluator.py:489] Running loglikelihood requests\n",
      "Running loglikelihood requests: 100%|██████████| 113863/113863 [28:41<00:00, 66.14it/s] \n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bootstrapping for stddev: perplexity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:07<00:00, 12.74it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bootstrapping for stddev: perplexity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:07<00:00, 12.66it/s]\n",
      "2024-11-13:13:52:43,222 WARNING  [huggingface.py:1353] Failed to get model SHA for GPTNeoXForCausalLM(\n",
      "  (gpt_neox): GPTNeoXModel(\n",
      "    (embed_in): Embedding(50304, 768)\n",
      "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x GPTNeoXLayer(\n",
      "        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (attention): GPTNeoXSdpaAttention(\n",
      "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "          (query_key_value): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (mlp): GPTNeoXMLP(\n",
      "          (dense_h_to_4h): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (dense_4h_to_h): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): GELUActivation()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  )\n",
      "  (embed_out): Linear(in_features=768, out_features=50304, bias=False)\n",
      ") at revision main. Error: Repo id must be a string, not <class 'transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXForCausalLM'>: 'GPTNeoXForCausalLM(\n",
      "  (gpt_neox): GPTNeoXModel(\n",
      "    (embed_in): Embedding(50304, 768)\n",
      "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x GPTNeoXLayer(\n",
      "        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (attention): GPTNeoXSdpaAttention(\n",
      "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "          (query_key_value): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (mlp): GPTNeoXMLP(\n",
      "          (dense_h_to_4h): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (dense_4h_to_h): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): GELUActivation()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  )\n",
      "  (embed_out): Linear(in_features=768, out_features=50304, bias=False)\n",
      ")'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|                 Tasks                 |Version|Filter|n-shot|  Metric  |   | Value  |   |Stderr|\n",
      "|---------------------------------------|------:|------|-----:|----------|---|-------:|---|-----:|\n",
      "|arc_challenge                          |      1|none  |     0|acc       |↑  |  0.1997|±  |0.0117|\n",
      "|                                       |       |none  |     0|acc_norm  |↑  |  0.2398|±  |0.0125|\n",
      "|hellaswag                              |      1|none  |     0|acc       |↑  |  0.2903|±  |0.0045|\n",
      "|                                       |       |none  |     0|acc_norm  |↑  |  0.3136|±  |0.0046|\n",
      "|lambada_openai                         |      1|none  |     0|acc       |↑  |  0.3689|±  |0.0067|\n",
      "|                                       |       |none  |     0|perplexity|↓  | 31.2590|±  |1.1594|\n",
      "|lambada_standard                       |      1|none  |     0|acc       |↑  |  0.2333|±  |0.0059|\n",
      "|                                       |       |none  |     0|perplexity|↓  |172.7634|±  |7.7266|\n",
      "|mmlu                                   |      2|none  |      |acc       |↑  |  0.2299|±  |0.0035|\n",
      "| - humanities                          |      2|none  |      |acc       |↑  |  0.2417|±  |0.0062|\n",
      "|  - formal_logic                       |      1|none  |     0|acc       |↑  |  0.2778|±  |0.0401|\n",
      "|  - high_school_european_history       |      1|none  |     0|acc       |↑  |  0.2242|±  |0.0326|\n",
      "|  - high_school_us_history             |      1|none  |     0|acc       |↑  |  0.2500|±  |0.0304|\n",
      "|  - high_school_world_history          |      1|none  |     0|acc       |↑  |  0.2700|±  |0.0289|\n",
      "|  - international_law                  |      1|none  |     0|acc       |↑  |  0.2397|±  |0.0390|\n",
      "|  - jurisprudence                      |      1|none  |     0|acc       |↑  |  0.2593|±  |0.0424|\n",
      "|  - logical_fallacies                  |      1|none  |     0|acc       |↑  |  0.2209|±  |0.0326|\n",
      "|  - moral_disputes                     |      1|none  |     0|acc       |↑  |  0.2486|±  |0.0233|\n",
      "|  - moral_scenarios                    |      1|none  |     0|acc       |↑  |  0.2380|±  |0.0142|\n",
      "|  - philosophy                         |      1|none  |     0|acc       |↑  |  0.1865|±  |0.0221|\n",
      "|  - prehistory                         |      1|none  |     0|acc       |↑  |  0.2130|±  |0.0228|\n",
      "|  - professional_law                   |      1|none  |     0|acc       |↑  |  0.2458|±  |0.0110|\n",
      "|  - world_religions                    |      1|none  |     0|acc       |↑  |  0.3158|±  |0.0357|\n",
      "| - other                               |      2|none  |      |acc       |↑  |  0.2401|±  |0.0076|\n",
      "|  - business_ethics                    |      1|none  |     0|acc       |↑  |  0.3000|±  |0.0461|\n",
      "|  - clinical_knowledge                 |      1|none  |     0|acc       |↑  |  0.2151|±  |0.0253|\n",
      "|  - college_medicine                   |      1|none  |     0|acc       |↑  |  0.2081|±  |0.0310|\n",
      "|  - global_facts                       |      1|none  |     0|acc       |↑  |  0.1800|±  |0.0386|\n",
      "|  - human_aging                        |      1|none  |     0|acc       |↑  |  0.3094|±  |0.0310|\n",
      "|  - management                         |      1|none  |     0|acc       |↑  |  0.1748|±  |0.0376|\n",
      "|  - marketing                          |      1|none  |     0|acc       |↑  |  0.2906|±  |0.0297|\n",
      "|  - medical_genetics                   |      1|none  |     0|acc       |↑  |  0.3000|±  |0.0461|\n",
      "|  - miscellaneous                      |      1|none  |     0|acc       |↑  |  0.2375|±  |0.0152|\n",
      "|  - nutrition                          |      1|none  |     0|acc       |↑  |  0.2190|±  |0.0237|\n",
      "|  - professional_accounting            |      1|none  |     0|acc       |↑  |  0.2340|±  |0.0253|\n",
      "|  - professional_medicine              |      1|none  |     0|acc       |↑  |  0.1985|±  |0.0242|\n",
      "|  - virology                           |      1|none  |     0|acc       |↑  |  0.2831|±  |0.0351|\n",
      "| - social sciences                     |      2|none  |      |acc       |↑  |  0.2187|±  |0.0074|\n",
      "|  - econometrics                       |      1|none  |     0|acc       |↑  |  0.2368|±  |0.0400|\n",
      "|  - high_school_geography              |      1|none  |     0|acc       |↑  |  0.1869|±  |0.0278|\n",
      "|  - high_school_government_and_politics|      1|none  |     0|acc       |↑  |  0.1969|±  |0.0287|\n",
      "|  - high_school_macroeconomics         |      1|none  |     0|acc       |↑  |  0.2051|±  |0.0205|\n",
      "|  - high_school_microeconomics         |      1|none  |     0|acc       |↑  |  0.2101|±  |0.0265|\n",
      "|  - high_school_psychology             |      1|none  |     0|acc       |↑  |  0.1927|±  |0.0169|\n",
      "|  - human_sexuality                    |      1|none  |     0|acc       |↑  |  0.2595|±  |0.0384|\n",
      "|  - professional_psychology            |      1|none  |     0|acc       |↑  |  0.2516|±  |0.0176|\n",
      "|  - public_relations                   |      1|none  |     0|acc       |↑  |  0.2182|±  |0.0396|\n",
      "|  - security_studies                   |      1|none  |     0|acc       |↑  |  0.1878|±  |0.0250|\n",
      "|  - sociology                          |      1|none  |     0|acc       |↑  |  0.2488|±  |0.0306|\n",
      "|  - us_foreign_policy                  |      1|none  |     0|acc       |↑  |  0.2800|±  |0.0451|\n",
      "| - stem                                |      2|none  |      |acc       |↑  |  0.2131|±  |0.0073|\n",
      "|  - abstract_algebra                   |      1|none  |     0|acc       |↑  |  0.2200|±  |0.0416|\n",
      "|  - anatomy                            |      1|none  |     0|acc       |↑  |  0.1926|±  |0.0341|\n",
      "|  - astronomy                          |      1|none  |     0|acc       |↑  |  0.1776|±  |0.0311|\n",
      "|  - college_biology                    |      1|none  |     0|acc       |↑  |  0.2639|±  |0.0369|\n",
      "|  - college_chemistry                  |      1|none  |     0|acc       |↑  |  0.1900|±  |0.0394|\n",
      "|  - college_computer_science           |      1|none  |     0|acc       |↑  |  0.2600|±  |0.0441|\n",
      "|  - college_mathematics                |      1|none  |     0|acc       |↑  |  0.2100|±  |0.0409|\n",
      "|  - college_physics                    |      1|none  |     0|acc       |↑  |  0.2157|±  |0.0409|\n",
      "|  - computer_security                  |      1|none  |     0|acc       |↑  |  0.2800|±  |0.0451|\n",
      "|  - conceptual_physics                 |      1|none  |     0|acc       |↑  |  0.2596|±  |0.0287|\n",
      "|  - electrical_engineering             |      1|none  |     0|acc       |↑  |  0.2345|±  |0.0353|\n",
      "|  - elementary_mathematics             |      1|none  |     0|acc       |↑  |  0.2090|±  |0.0209|\n",
      "|  - high_school_biology                |      1|none  |     0|acc       |↑  |  0.1742|±  |0.0216|\n",
      "|  - high_school_chemistry              |      1|none  |     0|acc       |↑  |  0.1724|±  |0.0266|\n",
      "|  - high_school_computer_science       |      1|none  |     0|acc       |↑  |  0.2600|±  |0.0441|\n",
      "|  - high_school_mathematics            |      1|none  |     0|acc       |↑  |  0.2074|±  |0.0247|\n",
      "|  - high_school_physics                |      1|none  |     0|acc       |↑  |  0.1987|±  |0.0326|\n",
      "|  - high_school_statistics             |      1|none  |     0|acc       |↑  |  0.1528|±  |0.0245|\n",
      "|  - machine_learning                   |      1|none  |     0|acc       |↑  |  0.3125|±  |0.0440|\n",
      "|winogrande                             |      1|none  |     0|acc       |↑  |  0.4964|±  |0.0141|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Evaluation - Pythia 160M Trained on Pile\n",
    "\n",
    "# Showcase uses Pythia: https://colab.research.google.com/github/EleutherAI/lm-evaluation-harness/blob/main/examples/lm-eval-overview.ipynb\n",
    "# Only genuine doc seems to be (a mess): https://github.com/EleutherAI/lm-evaluation-harness/tree/main/docs\n",
    "\n",
    "# Load model and tokenizer\n",
    "pythia_pile = AutoModelForCausalLM.from_pretrained(base_path / \"pythia160m_dedup_pile\", local_files_only=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_path / \"pythia160m_dedup_untrained\", use_fast=True, local_files_only=True)\n",
    "pythia_pile = pythia_pile.to(device)\n",
    "\n",
    "# From the HuggingFace Data Views (allenai/ai2_arc, cais/mmlu, allenai/winogrande, Rowan/hellaswag):\n",
    "# MMLU: {'answer': 1, 'choices': ['0', '4', '2', '6'], 'question': 'Find the degree for the given field extension Q(sqrt(2), sqrt(3), sqrt(18)) over Q.', 'subject': 'abstract_algebra'}\n",
    "# ARC-C: {'id': 'Mercury_7175875', 'question': 'An astronomer observes that a planet rotates faster after a meteorite impact. Which is the most likely effect of this increase in rotation?', 'choices': {'text': ['Planetary density will decrease.', 'Planetary years will become longer.', 'Planetary days will become shorter.', 'Planetary gravity will become stronger.'], 'label': ['A', 'B', 'C', 'D']}, 'answerKey': 'C'}\n",
    "# Winogrande: {'sentence': \"Ian volunteered to eat Dennis's menudo after already having a bowl because _ despised eating intestine.\", 'option1': 'Ian', 'option2': 'Dennis', 'answer': '2'} (answer content not contained in test set)\n",
    "# HellaSwag: {'ind': 14, 'activity_label': 'Wakeboarding', 'ctx_a': 'A man is being pulled on a water ski as he floats in the water casually.', 'ctx_b': 'he', 'ctx': 'A man is being pulled on a water ski as he floats in the water casually. he', 'endings': ['mounts the water ski and tears through the water at fast speeds.', 'goes over several speeds, trying to stay upright.', 'struggles a little bit as he talks about it.', 'is seated in a boat with three other people.'], 'source_id': 'activitynet~v_-5KAycAQlC4', 'split': 'test', 'split_type': 'indomain', 'label': ''}\n",
    "# Lambada (OAI): {'text': 'In my palm is a clear stone, and inside it is a small ivory statuette. A guardian angel.\\n\\n\"Figured if you\\'re going to be out at night getting hit by cars, you might as well have some backup.\"\\n\\nI look at him, feeling stunned. Like this is some sort of sign. But as I stare at Harlin, his mouth curved in a confident grin, I don\\'t care about signs'}\n",
    " \n",
    "batch_size_hflm = 1\n",
    "\n",
    "# Found that in here of all things: https://github.com/pytorch/ao/blob/e2301e9dba91fa962d673fdc3b3f0002856a3ba7/torchao/_models/_eval.py#L17-L22\n",
    "# https://github.com/EleutherAI/lm-evaluation-harness/blob/main/lm_eval/models/huggingface.py\n",
    "pythia_pile_hflm = HFLM(pretrained=pythia_pile,\n",
    "                        tokenizer=tokenizer,\n",
    "                        batch_size=batch_size_hflm)\n",
    "\n",
    "# Thankfully both MMLU and ARC are available in the lm_eval.tasks module\n",
    "# https://github.com/EleutherAI/lm-evaluation-harness/blob/main/lm_eval/tasks/arc\n",
    "# https://github.com/EleutherAI/lm-evaluation-harness/tree/main/lm_eval/tasks/mmlu\n",
    "# Initially evaluator.evaluate looked promising, but I don't understand it and this works\n",
    "# Found simple_evaluate in https://github.com/EleutherAI/lm-evaluation-harness/blob/main/docs/interface.md\n",
    "# Winogrande is in fact 'winogrande_xl', see https://github.com/EleutherAI/lm-evaluation-harness/blob/main/lm_eval/tasks/winogrande/default.yaml\n",
    "results = simple_evaluate(model=pythia_pile_hflm,\n",
    "                          tasks=[\"arc_challenge\", \"mmlu\", \"winogrande\", \"hellaswag\", \"lambada\"], # I have no idea how to inject pre-downloaded datasets here, I gave up on that\n",
    "                          num_fewshot=0,  # Pythia paper stated they used zero-shot\n",
    "                          batch_size=batch_size_hflm,\n",
    "                          device=\"cuda\",\n",
    "                          limit=None)\n",
    "\n",
    "# Save for reference (for proof of table below)\n",
    "with open('02_eval_160M_pretrained.txt', 'w') as f:\n",
    "    f.write(str(results))\n",
    "\n",
    "# Manually saved this to 02_eval_160M_pretrained_table.txt\n",
    "# This was nicely documented. Not. https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/eleuther_eval.py\n",
    "print(utils.make_table(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pro of this is that the processing is standardized and I know this works with Pythia models because the doc uses Pythia as the de-facto use case example\n",
    "    - However, with this implementation I have no idea what's actually going on under the hood\n",
    "    - Doesn't feel right, I'll read up on this and write more about it here. But the pipeline works\n",
    "- How do we know this benchmarking pipeline actually benchmarks correctly?\n",
    "    - Utilize the reported Pythia Benchmarks from the paper and compare\n",
    "\n",
    "![](./img/pythia_paper_dedup_benchmarks.png)\n",
    "\n",
    "I initially reported to benchmark only on ARC-Challenge and MMLU.<br>\n",
    "After reporting this, I quickly concluded that I wanted to use more benchmarks, specifically some of the ones mentioned in the Pythia paper to cross reference whether the evaluation pipeline itself is correct. Also, if that were to be the case, we would have a more information-rich benchmarking report. Who wouldn't want that?\n",
    "\n",
    "The overlapping confidence intervals suggest that the performances on WinoGrande and ARC-Challenge are consistent with the reported results.<br>\n",
    "This is narrowly not the case for Lambada (OpenAI).<br>\n",
    "Note that the benchmarking approach used here also reports larger `stderrs`.<br>\n",
    "I suppose the differences come from different versions of the LM-Eval harness (I further assume EleutherAI used their own LM Eval harness to test their models, still, speculative) being used and thus processing differences. I can't deem this to be a source of error in any way though.\n",
    "\n",
    "Given a notable divergence of performance results only occuring for Lambada, I interpret the deviation as a result of evaluation harness variance too.\n",
    "\n",
    "We can go ahead and evaluate Pythia $160\\text{M}$ MiniPile with the same setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Marcus2112/pythia160m_minipile/main...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b323d79ca674ca0a3b2df9d43b2b20e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbb7b17566fe4f8ab8c436218b442a05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/3.56M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb38564287f34b349ab78a16a517bf9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/11.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5edeeda92cc46a5bab21748374c5d93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/4.83k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89c57a981db0424b8d237b67ed84feea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe0af71a3bae49d9b639623e0b4a4038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/441 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad408d8b4eb64d1ab19dcab4a098ee5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/819 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "979a7b4aa268482693e3f76548b5748a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/495M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "download_model(down_dir=base_dir, target_folder=\"pythia160m_minipile_trained\", \n",
    "               cache_folder=\"pythia160m_minipile_trained_Cache\",\n",
    "               repo_id=\"Marcus2112/pythia160m_minipile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTNeoXForCausalLM were not initialized from the model checkpoint at /mnt/data/pythia160m_minipile_trained and are newly initialized: ['embed_out.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2024-11-15:12:54:24,505 WARNING  [huggingface.py:95] `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "2024-11-15:12:54:24,506 INFO     [huggingface.py:481] Using model type 'default'\n",
      "2024-11-15:12:54:24,518 WARNING  [huggingface.py:275] Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "2024-11-15:12:54:24,520 INFO     [evaluator.py:164] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2024-11-15:12:54:24,521 INFO     [evaluator.py:217] Using pre-initialized model\n",
      "2024-11-15:12:54:29,191 INFO     [__init__.py:459] The tag 'arc_ca' is already registered as a group, this tag will not be registered. This may affect tasks you want to call.\n",
      "2024-11-15:12:54:29,209 INFO     [__init__.py:459] The tag 'arc_ca' is already registered as a group, this tag will not be registered. This may affect tasks you want to call.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8d8e61223b94e1e880e7e5bda280ff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/9.00k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d2d6f8cc1ff42838fb301300e28f9e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/190k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a33154284d34046a4ee2f4f98893c68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/204k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df8f9ee02db24c98a401f2fb92541c77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/55.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebfaf4be2c5f4ccaab9f61a25a983457",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1119 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a78c5b135d7d4906a88a63fe3e254760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1172 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e77b40409f584f7bb0ca0f76e2e1445c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/299 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c477a476a4734ab7b6a41b6df7d065a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/1.11k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8bb23911b3448ae97caa762f97c1b75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mmlu_no_train.py:   0%|          | 0.00/5.86k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01a43f6e819543e193ba87579ecaed25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data.tar:   0%|          | 0.00/166M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9001da0b7ee84e9d89daa57b9b40e494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfc78d2b671a48a6b692a757cea2e288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70f22cdc56744672a4a8f89dab6aa43e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "617619b00c2946c4a4d47ccb23773ad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d1b7340921e404c848e3760ef70af0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "608cf1f1798c4747bbfbf07e481277ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f581edb015b4630b5ced77a769482fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "153def5274b346aba6abc1f6adf1c49f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f0cb8d0bdc24e22bc1bee77bdc66a88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c20dfb698f594322adc09b5fe1d03ba7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72bd3754dd3d44a6966b5280b50ac4fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5da04f1a76a446b92746e6165613467",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55c48d613a9a4a069eb30e27ee2685af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "062e52e8857247b0a68bdc6f46bad9ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e55dd8d81b054479bf27d3239132265b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73f2e96125c8437dbac94be6493adc24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c042d7d4b3d24458a52fca5e59d9db4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "553ca3a561154b4da13eb65f800554e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f67bc4ba941d4877a852258f88d6957f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "336da4eec3d54c9ebf91c92f538402ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecc85cb7f9504c1e9731b9e22f2edab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abdf815837f24f6d82b500f57027643d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d1c691a2eed47c2b01d0485d0e9bd25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b37e8eeb99674e3eb986479c16b9d93e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2eaf847fa904a64b621ea6b8e5c8eb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "052779f1f66043b091f09e55ca545c58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbb9cd8519884a0b92fae37d71d1e45a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ea8c39f6a9646ff9a710bea63f999fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a7549dfdb4b4fdf8d997f35f946bfaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bf91648248f463fa587951678b605b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70a231c7c8de4e72b1ca1b46210f56eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04c3456cf97544439866d1dbbd30d212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0822f4d5619f401eb5da2150ebd08d0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de82561adf3642beaf30bc78829da501",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9913168460224ea3ae7c446ed66eb3bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "872436fb53a5414fb8a2f7c07ee193ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e12edbb6a2445fcae709181395ba53b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "307d1c8daca24c569d134692daad9170",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78428bfab6a94d6f9795841996e9fe54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a1f23bd25a4417291d36661d7ce6eff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9258a407d2ef46c1a23c3a27f4ca2f8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d5d43503f4c4331a788d68414bdba55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fc0fca59b3d4b58861fbf46bf863615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3c99d95bef045208fd66c97daa96923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3cc02db86ff469e96117f16e2bf616a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a5bbc870e4d418baeaf54522fb755ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "560a41f909fc4afa844840b3f8d460e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e074ef8b821c49829d2f9b0d5bf47115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b39f33a5d21e46d89a1f5e1c59db55bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d57ab572a6b4c85a0aad7ea64ef40c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0489364c66934972b4f34511e1670c3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fd1037a8a764b0583c0cdef978d6924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a78dbae2792e4ff08cc7b97add2a4e87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cb7188fd4bc48609f3c9247275adced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eb50a2bc8ba4625baa3f03d9b82587d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dc0e4edb25241f3aad19f90c944817f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35fbbe001e0b4499b59082f1e3ab1b3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79f49938703a4631a5fa278a130cb7c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d608ef9a6624972ac418add3fb272cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5bccd623fc141c6a9a9acbd2da6968d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b34c9d848b1542b68267fdbbf427d938",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20a74c74c6d1459e9fc1971ac1938800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9339678c3094158a61ce15a3dddfc13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76d802adef0c498694078348bbf92952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "876911389ee84b90a43b9b559285cb71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e1a56bbf5154ccc927f8f0585dc69ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffac2246cca242118b7b503540aa132d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0478b4085a4429dbe44241d40d61460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba2ec3261cdc48d88b6795cb6bf8bb3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eef0891b012479bb7bfb54402153609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f02127b7174e43ed9d2ce2b69e0d3ea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc7cd2602b6f4ec3806023adbbce7c8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2be5c7edaaa641ddb86cf2da3f3cb801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44d5836fa08d4589bb4381be84f61347",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e95f7e640e3242ff81e9a569d6baff1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdb8288ab1944f13a2b1aa2ce5bd3c5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c619696c31b4c3f9e9445aca59da5ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61bfbc86ea7d4051bfaf7c762f45f2cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3646ceb9a1e4adca7ade881c962c5ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b47967791ccd49be999aa78ae5e4b730",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac8be1a1540043058a9fbd049c69b656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5a4cadc411949abaa71dd960abf127d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1050351ede0740068e2aa4487af4e247",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2082e5a85cc846d386e76c0df57a9707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbfd9f0730024c6e901724aaa8f1cd7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a81cbd84570448ab54eb16a7f515216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "713179a4df3b401084396148e1d1bf6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "480e8ff894d342e799211715d72ad7db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c608e63fcdd64e20b9d24291670edb12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21b3e44d75cd4d85a0e278a05c153799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b799dc5579e141959c9dd746f77a5a64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8f925426c8f40f5b84472c152789ec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "641938dde8aa4a8b8b5e13a37609b7e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f45a9d7ecc142929468be35208e6964",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62a34b5e891840909af0cdbfeba69790",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05cceffbbe5b453fa7d226adb91f0f2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e19f9aefcb664c138868640e0da72866",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "870caa080c7e48dc8ee868ac3ed83abd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caeeb0b5fb3844398a1857d47dcc8066",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df604a033e5848ad80f955764e451491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "881775080eaf4f00971634aba29f8c99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ea088d5956348d28e256b440bc59438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49f2e75c0312488aa94cd72bf32c88f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75d66cbde22345aebd912901d51720fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ef7438206c8424cb1a56c5d3a84e3ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "810413fbcb264bf7a424a0156b56148d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c6c63b7c8f34f88ad338663ca2e76dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a056ce23bbe74101972abed596c21d87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "126c8aa8f86a47f0a9d54d5eb8baf122",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02910bf72e7f4ea2af080e1df2b4a4ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9606749c9ee4e7780b5ca9b26844fc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fa41dbd56824b76a9929d913ef3d0c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8af862dae12c4a59bada650ea4bf90aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1f25ceeda0643e8a09c1c33766a42e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "271948aa26ff404f829e2d42db0ce6f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb3ee94a0ba44c548da9afed5bf04285",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e89f7a90b2b44f8e81d02be3af75f795",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc2be2ffe9324278915b5a7f238bb661",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e354bf0ff058487a9e0845fc2ed6c693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "730d972117d34447b72505f8d8c23b35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e072ea63c0342d9a360e1dade4b29e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "620a521c53e1462bb0c1a6a5e6a17b9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22da813c839146b996fdcaddb185599c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20f52f1225dc45bb95450a5c8157838e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "523a000230624ab1bcc99681585bc8b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b709933be5d24b63805983cc053bce53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2a1845fd73e4e5fac7de30852d04f02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd9f37b3d6184616944bbed96f67ec6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5213a85ccdcc4345914811ff48d3ec14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "080740031dbe48d890ba8b02f8512d72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04e0247544aa4807818b9e22a7c4e537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dbace4b01db42c9b1c65ea40a1efb72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3c4aa9991f040b094f490decbc7e8c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cacc813ebaa4ca89077d59cc224418f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "219e463b412c48be90d1aecc99392c69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84f448566e9b4569876f4edf4b192753",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ea97952da9b4a6ab7d565b6601e72f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a06827f87a7c4ac0ae880c25b4e8232b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e924b3e2c8a048db88da31a293672986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e41c6ce98c943ba95b4132a47cf1948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "306a2a17f5904d7c893d57b73d84fcce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e022ea433a6a4d3f9b7d712b782ab06a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b359f22f0464bc3808a0b34f85074c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f2696cab85f4b9d9bd9154f23911b92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea14e07961a6419aa2713bd312cab1d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed0ba768996c4a06a72a7f15129bc373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36c782dcd51d47a7bb61b898c82ffb98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85888861ec7d47228ce28075a6635b7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c321f95f8214614b4ddb382752f31f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "443c2ab7e39d4228a7911d25160a1358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6de3906b6a074655858af5b66694512e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ccee33389ba412f81ff3b8902ed3b42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c3b6958bbc74c049a03eec4c2531a7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7de2d41ea3df43d5a042ab0efbc3efcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b05fe1c9a4542c0906d3e977321bb51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e53bde56ce354312bdc851230d1e6c25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4870dd03dcbf46bebb9ecb331ea1642d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f103bd5cdcf14082b82198c6e2e62a22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "444cfffea7c64789a9ecc7310ac01681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdcdbedf9dc649c7b22f791d6b871665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aadb239cc3ae4521868f206a196e0b51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af318c264a6145c9b1de6e761f1f2833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97c1e63cdbda4e4c94a7b46c91d063ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28542a79ac1845d5aaf2ec8a75b10034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba5d351caaea4f4c8b7d10da9031e146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62865b5cb94244ac8a6fdb8d33c5f178",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d5ffc71fb654c5f9001b7da292ebead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dd2ba779a9c4bf3a6e9aa96c3cd6985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65844236d41a4fd88ce588373caacb03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33af2b8b256b4d4cad02b856d2b8ce6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "214c524268d34ac38ea41ad89452f903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c7a97487e6f416dbc23a5581adfad68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/9.97k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d91893f0edb146688359fccd6f74a7a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "winogrande.py:   0%|          | 0.00/5.65k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ef39529184b43d9b20c816875421d72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/3.40M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4b2850e93bb41298a7c8fd4a3cd9a22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/40398 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8558e71080c48df9da8b4c641ff2283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1767 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dc94195ecc541ca9ec8264bacabf0bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1267 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99393c93c45e4c69a7d53af977a5c850",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/6.84k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "172bb8de949949dfa5b0f7e51b6ffecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "hellaswag.py:   0%|          | 0.00/4.36k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a59e349c794e4d9a818282ac45b1ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dataset_infos.json:   0%|          | 0.00/2.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6980d46758b446b3b36bf05d6fe535c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/47.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7db8ea5baa224b26a2259ea313c02bd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/11.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65d7aa56056544279881fa306c3aaf6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/12.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27704302eab54c28b5617fb49970f9ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/39905 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35b9c78321ff419a86356704fe41a67e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/10003 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef89dd872c2a44c984d1f4ee45d4dd01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/10042 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c259b636d7494fddadaecdb5fbd937e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/39905 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f5a5600eeb541a981b4eb1bbd504aac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10042 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56d27e6a320e41bb9649a8297ad090c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/4.99k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "095335acd18f49378d7c0c0e9d15cc4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "lambada_openai.py:   0%|          | 0.00/4.82k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c22c0e39f61040dfbbab8101e1d22c5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0000.parquet:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0029db7e29eb4fd9a815131b17d9f096",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/5153 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15:12:56:54,038 WARNING  [task.py:325] [Task: lambada_openai] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
      "2024-11-15:12:56:54,039 WARNING  [task.py:325] [Task: lambada_openai] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "746b8ed081234687ad586345b6400e87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/7.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e11741cb679b48f8b4827ce164738f82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00002.parquet:   0%|          | 0.00/269M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd9e04b254af4ed69bc5cc4dc111d246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00001-of-00002.parquet:   0%|          | 0.00/281M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e86e6a4c92b4c79aa80d6440483c5c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/1.14M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c56d7dfe92da47cdb6cfcb848c7bd347",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/1.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "039b90a69dcd4b089fb86082e165dbe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/2662 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a362356b8e54cd7bdb8bcd41e79104e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/5153 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "292db4e146a24eb490e8400a30182a6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/4869 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15:12:57:30,043 WARNING  [evaluator.py:270] Overwriting default num_fewshot of lambada_standard from None to 0\n",
      "2024-11-15:12:57:30,044 WARNING  [evaluator.py:270] Overwriting default num_fewshot of lambada_openai from None to 0\n",
      "2024-11-15:12:57:30,045 WARNING  [evaluator.py:270] Overwriting default num_fewshot of hellaswag from None to 0\n",
      "2024-11-15:12:57:30,046 WARNING  [evaluator.py:270] Overwriting default num_fewshot of winogrande from None to 0\n",
      "2024-11-15:12:57:30,047 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_machine_learning from None to 0\n",
      "2024-11-15:12:57:30,047 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_astronomy from None to 0\n",
      "2024-11-15:12:57:30,048 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_college_mathematics from None to 0\n",
      "2024-11-15:12:57:30,049 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_physics from None to 0\n",
      "2024-11-15:12:57:30,050 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_biology from None to 0\n",
      "2024-11-15:12:57:30,051 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_mathematics from None to 0\n",
      "2024-11-15:12:57:30,051 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_college_computer_science from None to 0\n",
      "2024-11-15:12:57:30,052 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_elementary_mathematics from None to 0\n",
      "2024-11-15:12:57:30,052 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_chemistry from None to 0\n",
      "2024-11-15:12:57:30,053 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_conceptual_physics from None to 0\n",
      "2024-11-15:12:57:30,053 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_computer_security from None to 0\n",
      "2024-11-15:12:57:30,054 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_electrical_engineering from None to 0\n",
      "2024-11-15:12:57:30,055 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_anatomy from None to 0\n",
      "2024-11-15:12:57:30,056 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_college_chemistry from None to 0\n",
      "2024-11-15:12:57:30,057 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_college_physics from None to 0\n",
      "2024-11-15:12:57:30,057 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_abstract_algebra from None to 0\n",
      "2024-11-15:12:57:30,058 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_computer_science from None to 0\n",
      "2024-11-15:12:57:30,059 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_college_biology from None to 0\n",
      "2024-11-15:12:57:30,060 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_statistics from None to 0\n",
      "2024-11-15:12:57:30,061 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_global_facts from None to 0\n",
      "2024-11-15:12:57:30,061 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_marketing from None to 0\n",
      "2024-11-15:12:57:30,062 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_nutrition from None to 0\n",
      "2024-11-15:12:57:30,062 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_college_medicine from None to 0\n",
      "2024-11-15:12:57:30,063 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_business_ethics from None to 0\n",
      "2024-11-15:12:57:30,064 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_management from None to 0\n",
      "2024-11-15:12:57:30,064 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_virology from None to 0\n",
      "2024-11-15:12:57:30,065 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_human_aging from None to 0\n",
      "2024-11-15:12:57:30,065 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_clinical_knowledge from None to 0\n",
      "2024-11-15:12:57:30,066 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_medical_genetics from None to 0\n",
      "2024-11-15:12:57:30,066 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_professional_medicine from None to 0\n",
      "2024-11-15:12:57:30,067 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_professional_accounting from None to 0\n",
      "2024-11-15:12:57:30,068 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_miscellaneous from None to 0\n",
      "2024-11-15:12:57:30,069 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_econometrics from None to 0\n",
      "2024-11-15:12:57:30,070 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_professional_psychology from None to 0\n",
      "2024-11-15:12:57:30,070 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_psychology from None to 0\n",
      "2024-11-15:12:57:30,071 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_government_and_politics from None to 0\n",
      "2024-11-15:12:57:30,072 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_sociology from None to 0\n",
      "2024-11-15:12:57:30,073 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_human_sexuality from None to 0\n",
      "2024-11-15:12:57:30,073 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_geography from None to 0\n",
      "2024-11-15:12:57:30,074 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_macroeconomics from None to 0\n",
      "2024-11-15:12:57:30,075 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_microeconomics from None to 0\n",
      "2024-11-15:12:57:30,076 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_public_relations from None to 0\n",
      "2024-11-15:12:57:30,076 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_security_studies from None to 0\n",
      "2024-11-15:12:57:30,077 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_us_foreign_policy from None to 0\n",
      "2024-11-15:12:57:30,078 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_international_law from None to 0\n",
      "2024-11-15:12:57:30,079 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_european_history from None to 0\n",
      "2024-11-15:12:57:30,080 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_philosophy from None to 0\n",
      "2024-11-15:12:57:30,081 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_moral_scenarios from None to 0\n",
      "2024-11-15:12:57:30,081 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_jurisprudence from None to 0\n",
      "2024-11-15:12:57:30,082 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_formal_logic from None to 0\n",
      "2024-11-15:12:57:30,083 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_world_religions from None to 0\n",
      "2024-11-15:12:57:30,084 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_prehistory from None to 0\n",
      "2024-11-15:12:57:30,084 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_us_history from None to 0\n",
      "2024-11-15:12:57:30,085 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_moral_disputes from None to 0\n",
      "2024-11-15:12:57:30,086 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_logical_fallacies from None to 0\n",
      "2024-11-15:12:57:30,087 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_world_history from None to 0\n",
      "2024-11-15:12:57:30,087 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_professional_law from None to 0\n",
      "2024-11-15:12:57:30,088 WARNING  [evaluator.py:270] Overwriting default num_fewshot of arc_challenge from None to 0\n",
      "2024-11-15:12:57:30,096 INFO     [task.py:415] Building contexts for lambada_standard on rank 0...\n",
      "100%|██████████| 5153/5153 [00:13<00:00, 385.12it/s]\n",
      "2024-11-15:12:57:43,762 INFO     [task.py:415] Building contexts for lambada_openai on rank 0...\n",
      "100%|██████████| 5153/5153 [00:13<00:00, 384.10it/s]\n",
      "2024-11-15:12:57:57,400 INFO     [task.py:415] Building contexts for hellaswag on rank 0...\n",
      "100%|██████████| 10042/10042 [00:06<00:00, 1663.43it/s]\n",
      "2024-11-15:12:58:05,943 INFO     [task.py:415] Building contexts for winogrande on rank 0...\n",
      "100%|██████████| 1267/1267 [00:00<00:00, 54282.86it/s]\n",
      "2024-11-15:12:58:06,078 INFO     [task.py:415] Building contexts for mmlu_machine_learning on rank 0...\n",
      "100%|██████████| 112/112 [00:00<00:00, 436.44it/s]\n",
      "2024-11-15:12:58:06,349 INFO     [task.py:415] Building contexts for mmlu_astronomy on rank 0...\n",
      "100%|██████████| 152/152 [00:00<00:00, 439.28it/s]\n",
      "2024-11-15:12:58:06,715 INFO     [task.py:415] Building contexts for mmlu_college_mathematics on rank 0...\n",
      "100%|██████████| 100/100 [00:00<00:00, 431.69it/s]\n",
      "2024-11-15:12:58:06,961 INFO     [task.py:415] Building contexts for mmlu_high_school_physics on rank 0...\n",
      "100%|██████████| 151/151 [00:00<00:00, 434.09it/s]\n",
      "2024-11-15:12:58:07,329 INFO     [task.py:415] Building contexts for mmlu_high_school_biology on rank 0...\n",
      "100%|██████████| 310/310 [00:00<00:00, 431.01it/s]\n",
      "2024-11-15:12:58:08,084 INFO     [task.py:415] Building contexts for mmlu_high_school_mathematics on rank 0...\n",
      "100%|██████████| 270/270 [00:00<00:00, 430.03it/s]\n",
      "2024-11-15:12:58:08,744 INFO     [task.py:415] Building contexts for mmlu_college_computer_science on rank 0...\n",
      "100%|██████████| 100/100 [00:00<00:00, 389.73it/s]\n",
      "2024-11-15:12:58:09,013 INFO     [task.py:415] Building contexts for mmlu_elementary_mathematics on rank 0...\n",
      "100%|██████████| 378/378 [00:00<00:00, 448.38it/s]\n",
      "2024-11-15:12:58:09,898 INFO     [task.py:415] Building contexts for mmlu_high_school_chemistry on rank 0...\n",
      "100%|██████████| 203/203 [00:00<00:00, 448.72it/s]\n",
      "2024-11-15:12:58:10,374 INFO     [task.py:415] Building contexts for mmlu_conceptual_physics on rank 0...\n",
      "100%|██████████| 235/235 [00:00<00:00, 448.50it/s]\n",
      "2024-11-15:12:58:10,925 INFO     [task.py:415] Building contexts for mmlu_computer_security on rank 0...\n",
      "100%|██████████| 100/100 [00:00<00:00, 445.42it/s]\n",
      "2024-11-15:12:58:11,164 INFO     [task.py:415] Building contexts for mmlu_electrical_engineering on rank 0...\n",
      "100%|██████████| 145/145 [00:00<00:00, 445.02it/s]\n",
      "2024-11-15:12:58:11,508 INFO     [task.py:415] Building contexts for mmlu_anatomy on rank 0...\n",
      "100%|██████████| 135/135 [00:00<00:00, 447.56it/s]\n",
      "2024-11-15:12:58:11,827 INFO     [task.py:415] Building contexts for mmlu_college_chemistry on rank 0...\n",
      "100%|██████████| 100/100 [00:00<00:00, 447.74it/s]\n",
      "2024-11-15:12:58:12,064 INFO     [task.py:415] Building contexts for mmlu_college_physics on rank 0...\n",
      "100%|██████████| 102/102 [00:00<00:00, 448.28it/s]\n",
      "2024-11-15:12:58:12,304 INFO     [task.py:415] Building contexts for mmlu_abstract_algebra on rank 0...\n",
      "100%|██████████| 100/100 [00:00<00:00, 446.13it/s]\n",
      "2024-11-15:12:58:12,542 INFO     [task.py:415] Building contexts for mmlu_high_school_computer_science on rank 0...\n",
      "100%|██████████| 100/100 [00:00<00:00, 394.93it/s]\n",
      "2024-11-15:12:58:12,808 INFO     [task.py:415] Building contexts for mmlu_college_biology on rank 0...\n",
      "100%|██████████| 144/144 [00:00<00:00, 441.30it/s]\n",
      "2024-11-15:12:58:13,153 INFO     [task.py:415] Building contexts for mmlu_high_school_statistics on rank 0...\n",
      "100%|██████████| 216/216 [00:00<00:00, 445.50it/s]\n",
      "2024-11-15:12:58:13,664 INFO     [task.py:415] Building contexts for mmlu_global_facts on rank 0...\n",
      "100%|██████████| 100/100 [00:00<00:00, 444.30it/s]\n",
      "2024-11-15:12:58:13,902 INFO     [task.py:415] Building contexts for mmlu_marketing on rank 0...\n",
      "100%|██████████| 234/234 [00:00<00:00, 442.53it/s]\n",
      "2024-11-15:12:58:14,459 INFO     [task.py:415] Building contexts for mmlu_nutrition on rank 0...\n",
      "100%|██████████| 306/306 [00:00<00:00, 443.07it/s]\n",
      "2024-11-15:12:58:15,185 INFO     [task.py:415] Building contexts for mmlu_college_medicine on rank 0...\n",
      "100%|██████████| 173/173 [00:00<00:00, 441.56it/s]\n",
      "2024-11-15:12:58:15,597 INFO     [task.py:415] Building contexts for mmlu_business_ethics on rank 0...\n",
      "100%|██████████| 100/100 [00:00<00:00, 435.84it/s]\n",
      "2024-11-15:12:58:15,841 INFO     [task.py:415] Building contexts for mmlu_management on rank 0...\n",
      "100%|██████████| 103/103 [00:00<00:00, 435.31it/s]\n",
      "2024-11-15:12:58:16,092 INFO     [task.py:415] Building contexts for mmlu_virology on rank 0...\n",
      "100%|██████████| 166/166 [00:00<00:00, 402.85it/s]\n",
      "2024-11-15:12:58:16,526 INFO     [task.py:415] Building contexts for mmlu_human_aging on rank 0...\n",
      "100%|██████████| 223/223 [00:00<00:00, 440.55it/s]\n",
      "2024-11-15:12:58:17,059 INFO     [task.py:415] Building contexts for mmlu_clinical_knowledge on rank 0...\n",
      "100%|██████████| 265/265 [00:00<00:00, 439.62it/s]\n",
      "2024-11-15:12:58:17,692 INFO     [task.py:415] Building contexts for mmlu_medical_genetics on rank 0...\n",
      "100%|██████████| 100/100 [00:00<00:00, 436.91it/s]\n",
      "2024-11-15:12:58:17,934 INFO     [task.py:415] Building contexts for mmlu_professional_medicine on rank 0...\n",
      "100%|██████████| 272/272 [00:00<00:00, 440.81it/s]\n",
      "2024-11-15:12:58:18,584 INFO     [task.py:415] Building contexts for mmlu_professional_accounting on rank 0...\n",
      "100%|██████████| 282/282 [00:00<00:00, 439.48it/s]\n",
      "2024-11-15:12:58:19,259 INFO     [task.py:415] Building contexts for mmlu_miscellaneous on rank 0...\n",
      "100%|██████████| 783/783 [00:01<00:00, 413.86it/s]\n",
      "2024-11-15:12:58:21,234 INFO     [task.py:415] Building contexts for mmlu_econometrics on rank 0...\n",
      "100%|██████████| 114/114 [00:00<00:00, 401.94it/s]\n",
      "2024-11-15:12:58:21,536 INFO     [task.py:415] Building contexts for mmlu_professional_psychology on rank 0...\n",
      "100%|██████████| 612/612 [00:01<00:00, 414.79it/s]\n",
      "2024-11-15:12:58:23,085 INFO     [task.py:415] Building contexts for mmlu_high_school_psychology on rank 0...\n",
      "100%|██████████| 545/545 [00:01<00:00, 311.95it/s]\n",
      "2024-11-15:12:58:24,900 INFO     [task.py:415] Building contexts for mmlu_high_school_government_and_politics on rank 0...\n",
      "100%|██████████| 193/193 [00:00<00:00, 417.36it/s]\n",
      "2024-11-15:12:58:25,387 INFO     [task.py:415] Building contexts for mmlu_sociology on rank 0...\n",
      "100%|██████████| 201/201 [00:00<00:00, 400.23it/s]\n",
      "2024-11-15:12:58:25,918 INFO     [task.py:415] Building contexts for mmlu_human_sexuality on rank 0...\n",
      "100%|██████████| 131/131 [00:00<00:00, 405.26it/s]\n",
      "2024-11-15:12:58:26,260 INFO     [task.py:415] Building contexts for mmlu_high_school_geography on rank 0...\n",
      "100%|██████████| 198/198 [00:00<00:00, 445.15it/s]\n",
      "2024-11-15:12:58:26,729 INFO     [task.py:415] Building contexts for mmlu_high_school_macroeconomics on rank 0...\n",
      "100%|██████████| 390/390 [00:00<00:00, 415.41it/s]\n",
      "2024-11-15:12:58:27,712 INFO     [task.py:415] Building contexts for mmlu_high_school_microeconomics on rank 0...\n",
      "100%|██████████| 238/238 [00:00<00:00, 428.25it/s]\n",
      "2024-11-15:12:58:28,298 INFO     [task.py:415] Building contexts for mmlu_public_relations on rank 0...\n",
      "100%|██████████| 110/110 [00:00<00:00, 437.58it/s]\n",
      "2024-11-15:12:58:28,565 INFO     [task.py:415] Building contexts for mmlu_security_studies on rank 0...\n",
      "100%|██████████| 245/245 [00:00<00:00, 428.38it/s]\n",
      "2024-11-15:12:58:29,168 INFO     [task.py:415] Building contexts for mmlu_us_foreign_policy on rank 0...\n",
      "100%|██████████| 100/100 [00:00<00:00, 363.04it/s]\n",
      "2024-11-15:12:58:29,457 INFO     [task.py:415] Building contexts for mmlu_international_law on rank 0...\n",
      "100%|██████████| 121/121 [00:00<00:00, 385.07it/s]\n",
      "2024-11-15:12:58:29,790 INFO     [task.py:415] Building contexts for mmlu_high_school_european_history on rank 0...\n",
      "100%|██████████| 165/165 [00:00<00:00, 387.61it/s]\n",
      "2024-11-15:12:58:30,244 INFO     [task.py:415] Building contexts for mmlu_philosophy on rank 0...\n",
      "100%|██████████| 311/311 [00:00<00:00, 394.43it/s]\n",
      "2024-11-15:12:58:31,080 INFO     [task.py:415] Building contexts for mmlu_moral_scenarios on rank 0...\n",
      "100%|██████████| 895/895 [00:02<00:00, 411.84it/s]\n",
      "2024-11-15:12:58:33,376 INFO     [task.py:415] Building contexts for mmlu_jurisprudence on rank 0...\n",
      "100%|██████████| 108/108 [00:00<00:00, 398.88it/s]\n",
      "2024-11-15:12:58:33,664 INFO     [task.py:415] Building contexts for mmlu_formal_logic on rank 0...\n",
      "100%|██████████| 126/126 [00:00<00:00, 415.28it/s]\n",
      "2024-11-15:12:58:33,986 INFO     [task.py:415] Building contexts for mmlu_world_religions on rank 0...\n",
      "100%|██████████| 171/171 [00:00<00:00, 388.24it/s]\n",
      "2024-11-15:12:58:34,450 INFO     [task.py:415] Building contexts for mmlu_prehistory on rank 0...\n",
      "100%|██████████| 324/324 [00:00<00:00, 398.98it/s]\n",
      "2024-11-15:12:58:35,302 INFO     [task.py:415] Building contexts for mmlu_high_school_us_history on rank 0...\n",
      "100%|██████████| 204/204 [00:00<00:00, 420.04it/s]\n",
      "2024-11-15:12:58:35,818 INFO     [task.py:415] Building contexts for mmlu_moral_disputes on rank 0...\n",
      "100%|██████████| 346/346 [00:00<00:00, 416.68it/s]\n",
      "2024-11-15:12:58:36,689 INFO     [task.py:415] Building contexts for mmlu_logical_fallacies on rank 0...\n",
      "100%|██████████| 163/163 [00:00<00:00, 387.78it/s]\n",
      "2024-11-15:12:58:37,143 INFO     [task.py:415] Building contexts for mmlu_high_school_world_history on rank 0...\n",
      "100%|██████████| 237/237 [00:00<00:00, 399.35it/s]\n",
      "2024-11-15:12:58:37,779 INFO     [task.py:415] Building contexts for mmlu_professional_law on rank 0...\n",
      "100%|██████████| 1534/1534 [00:03<00:00, 422.86it/s]\n",
      "2024-11-15:12:58:41,579 INFO     [task.py:415] Building contexts for arc_challenge on rank 0...\n",
      "100%|██████████| 1172/1172 [00:01<00:00, 736.84it/s]\n",
      "2024-11-15:12:58:43,337 INFO     [evaluator.py:489] Running loglikelihood requests\n",
      "Running loglikelihood requests: 100%|██████████| 113863/113863 [28:01<00:00, 67.70it/s] \n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bootstrapping for stddev: perplexity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:06<00:00, 14.40it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bootstrapping for stddev: perplexity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:07<00:00, 14.28it/s]\n",
      "2024-11-15:13:29:40,010 WARNING  [huggingface.py:1353] Failed to get model SHA for GPTNeoXForCausalLM(\n",
      "  (gpt_neox): GPTNeoXModel(\n",
      "    (embed_in): Embedding(50304, 768)\n",
      "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x GPTNeoXLayer(\n",
      "        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (attention): GPTNeoXSdpaAttention(\n",
      "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "          (query_key_value): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (mlp): GPTNeoXMLP(\n",
      "          (dense_h_to_4h): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (dense_4h_to_h): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): GELUActivation()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  )\n",
      "  (embed_out): Linear(in_features=768, out_features=50304, bias=False)\n",
      ") at revision main. Error: Repo id must be a string, not <class 'transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXForCausalLM'>: 'GPTNeoXForCausalLM(\n",
      "  (gpt_neox): GPTNeoXModel(\n",
      "    (embed_in): Embedding(50304, 768)\n",
      "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x GPTNeoXLayer(\n",
      "        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (attention): GPTNeoXSdpaAttention(\n",
      "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "          (query_key_value): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (mlp): GPTNeoXMLP(\n",
      "          (dense_h_to_4h): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (dense_4h_to_h): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): GELUActivation()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  )\n",
      "  (embed_out): Linear(in_features=768, out_features=50304, bias=False)\n",
      ")'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|                 Tasks                 |Version|Filter|n-shot|  Metric  |   |    Value    |   |   Stderr   |\n",
      "|---------------------------------------|------:|------|-----:|----------|---|------------:|---|-----------:|\n",
      "|arc_challenge                          |      1|none  |     0|acc       |↑  |       0.2125|±  |      0.0120|\n",
      "|                                       |       |none  |     0|acc_norm  |↑  |       0.2628|±  |      0.0129|\n",
      "|hellaswag                              |      1|none  |     0|acc       |↑  |       0.2560|±  |      0.0044|\n",
      "|                                       |       |none  |     0|acc_norm  |↑  |       0.2619|±  |      0.0044|\n",
      "|lambada_openai                         |      1|none  |     0|acc       |↑  |       0.0000|±  |      0.0000|\n",
      "|                                       |       |none  |     0|perplexity|↓  | 3138574.4432|±  | 302684.4607|\n",
      "|lambada_standard                       |      1|none  |     0|acc       |↑  |       0.0000|±  |      0.0000|\n",
      "|                                       |       |none  |     0|perplexity|↓  |27646284.9464|±  |2784935.7933|\n",
      "|mmlu                                   |      2|none  |      |acc       |↑  |       0.2699|±  |      0.0037|\n",
      "| - humanities                          |      2|none  |      |acc       |↑  |       0.2444|±  |      0.0063|\n",
      "|  - formal_logic                       |      1|none  |     0|acc       |↑  |       0.3571|±  |      0.0429|\n",
      "|  - high_school_european_history       |      1|none  |     0|acc       |↑  |       0.2606|±  |      0.0343|\n",
      "|  - high_school_us_history             |      1|none  |     0|acc       |↑  |       0.2549|±  |      0.0306|\n",
      "|  - high_school_world_history          |      1|none  |     0|acc       |↑  |       0.2194|±  |      0.0269|\n",
      "|  - international_law                  |      1|none  |     0|acc       |↑  |       0.1405|±  |      0.0317|\n",
      "|  - jurisprudence                      |      1|none  |     0|acc       |↑  |       0.2130|±  |      0.0396|\n",
      "|  - logical_fallacies                  |      1|none  |     0|acc       |↑  |       0.2393|±  |      0.0335|\n",
      "|  - moral_disputes                     |      1|none  |     0|acc       |↑  |       0.2197|±  |      0.0223|\n",
      "|  - moral_scenarios                    |      1|none  |     0|acc       |↑  |       0.2726|±  |      0.0149|\n",
      "|  - philosophy                         |      1|none  |     0|acc       |↑  |       0.2508|±  |      0.0246|\n",
      "|  - prehistory                         |      1|none  |     0|acc       |↑  |       0.2253|±  |      0.0232|\n",
      "|  - professional_law                   |      1|none  |     0|acc       |↑  |       0.2445|±  |      0.0110|\n",
      "|  - world_religions                    |      1|none  |     0|acc       |↑  |       0.1930|±  |      0.0303|\n",
      "| - other                               |      2|none  |      |acc       |↑  |       0.2533|±  |      0.0077|\n",
      "|  - business_ethics                    |      1|none  |     0|acc       |↑  |       0.2200|±  |      0.0416|\n",
      "|  - clinical_knowledge                 |      1|none  |     0|acc       |↑  |       0.2906|±  |      0.0279|\n",
      "|  - college_medicine                   |      1|none  |     0|acc       |↑  |       0.3410|±  |      0.0361|\n",
      "|  - global_facts                       |      1|none  |     0|acc       |↑  |       0.1900|±  |      0.0394|\n",
      "|  - human_aging                        |      1|none  |     0|acc       |↑  |       0.1211|±  |      0.0219|\n",
      "|  - management                         |      1|none  |     0|acc       |↑  |       0.3883|±  |      0.0483|\n",
      "|  - marketing                          |      1|none  |     0|acc       |↑  |       0.1966|±  |      0.0260|\n",
      "|  - medical_genetics                   |      1|none  |     0|acc       |↑  |       0.2500|±  |      0.0435|\n",
      "|  - miscellaneous                      |      1|none  |     0|acc       |↑  |       0.2043|±  |      0.0144|\n",
      "|  - nutrition                          |      1|none  |     0|acc       |↑  |       0.2908|±  |      0.0260|\n",
      "|  - professional_accounting            |      1|none  |     0|acc       |↑  |       0.2411|±  |      0.0255|\n",
      "|  - professional_medicine              |      1|none  |     0|acc       |↑  |       0.4485|±  |      0.0302|\n",
      "|  - virology                           |      1|none  |     0|acc       |↑  |       0.1988|±  |      0.0311|\n",
      "| - social sciences                     |      2|none  |      |acc       |↑  |       0.3107|±  |      0.0083|\n",
      "|  - econometrics                       |      1|none  |     0|acc       |↑  |       0.2368|±  |      0.0400|\n",
      "|  - high_school_geography              |      1|none  |     0|acc       |↑  |       0.3485|±  |      0.0339|\n",
      "|  - high_school_government_and_politics|      1|none  |     0|acc       |↑  |       0.3575|±  |      0.0346|\n",
      "|  - high_school_macroeconomics         |      1|none  |     0|acc       |↑  |       0.3590|±  |      0.0243|\n",
      "|  - high_school_microeconomics         |      1|none  |     0|acc       |↑  |       0.3571|±  |      0.0311|\n",
      "|  - high_school_psychology             |      1|none  |     0|acc       |↑  |       0.3376|±  |      0.0203|\n",
      "|  - human_sexuality                    |      1|none  |     0|acc       |↑  |       0.2824|±  |      0.0395|\n",
      "|  - professional_psychology            |      1|none  |     0|acc       |↑  |       0.2206|±  |      0.0168|\n",
      "|  - public_relations                   |      1|none  |     0|acc       |↑  |       0.2364|±  |      0.0407|\n",
      "|  - security_studies                   |      1|none  |     0|acc       |↑  |       0.4000|±  |      0.0314|\n",
      "|  - sociology                          |      1|none  |     0|acc       |↑  |       0.2985|±  |      0.0324|\n",
      "|  - us_foreign_policy                  |      1|none  |     0|acc       |↑  |       0.2600|±  |      0.0441|\n",
      "| - stem                                |      2|none  |      |acc       |↑  |       0.2845|±  |      0.0079|\n",
      "|  - abstract_algebra                   |      1|none  |     0|acc       |↑  |       0.2100|±  |      0.0409|\n",
      "|  - anatomy                            |      1|none  |     0|acc       |↑  |       0.2222|±  |      0.0359|\n",
      "|  - astronomy                          |      1|none  |     0|acc       |↑  |       0.3355|±  |      0.0384|\n",
      "|  - college_biology                    |      1|none  |     0|acc       |↑  |       0.2708|±  |      0.0372|\n",
      "|  - college_chemistry                  |      1|none  |     0|acc       |↑  |       0.4000|±  |      0.0492|\n",
      "|  - college_computer_science           |      1|none  |     0|acc       |↑  |       0.3300|±  |      0.0473|\n",
      "|  - college_mathematics                |      1|none  |     0|acc       |↑  |       0.3100|±  |      0.0465|\n",
      "|  - college_physics                    |      1|none  |     0|acc       |↑  |       0.3725|±  |      0.0481|\n",
      "|  - computer_security                  |      1|none  |     0|acc       |↑  |       0.1800|±  |      0.0386|\n",
      "|  - conceptual_physics                 |      1|none  |     0|acc       |↑  |       0.2128|±  |      0.0268|\n",
      "|  - electrical_engineering             |      1|none  |     0|acc       |↑  |       0.2483|±  |      0.0360|\n",
      "|  - elementary_mathematics             |      1|none  |     0|acc       |↑  |       0.2646|±  |      0.0227|\n",
      "|  - high_school_biology                |      1|none  |     0|acc       |↑  |       0.3161|±  |      0.0265|\n",
      "|  - high_school_chemistry              |      1|none  |     0|acc       |↑  |       0.2857|±  |      0.0318|\n",
      "|  - high_school_computer_science       |      1|none  |     0|acc       |↑  |       0.1900|±  |      0.0394|\n",
      "|  - high_school_mathematics            |      1|none  |     0|acc       |↑  |       0.2444|±  |      0.0262|\n",
      "|  - high_school_physics                |      1|none  |     0|acc       |↑  |       0.3311|±  |      0.0384|\n",
      "|  - high_school_statistics             |      1|none  |     0|acc       |↑  |       0.4722|±  |      0.0340|\n",
      "|  - machine_learning                   |      1|none  |     0|acc       |↑  |       0.1518|±  |      0.0341|\n",
      "|winogrande                             |      1|none  |     0|acc       |↑  |       0.4720|±  |      0.0140|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Evaluation - Pythia 160M Trained on MiniPile\n",
    "\n",
    "pythia_minipile = AutoModelForCausalLM.from_pretrained(base_path / \"pythia160m_minipile_trained\", local_files_only=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_path / \"pythia160m_dedup_untrained\", use_fast=True, local_files_only=True) # Use exact same tokenizer\n",
    "pythia_minipile = pythia_minipile.to(device)\n",
    " \n",
    "batch_size_hflm = 1\n",
    "\n",
    "pythia_minipile_hflm = HFLM(pretrained=pythia_minipile,\n",
    "                        tokenizer=tokenizer,\n",
    "                        batch_size=batch_size_hflm)\n",
    "\n",
    "results = simple_evaluate(model=pythia_minipile_hflm,\n",
    "                          tasks=[\"arc_challenge\", \"mmlu\", \"winogrande\", \"hellaswag\", \"lambada\"],\n",
    "                          num_fewshot=0,\n",
    "                          batch_size=batch_size_hflm,\n",
    "                          device=\"cuda\",\n",
    "                          limit=None)\n",
    "\n",
    "with open('02_eval_160M_minipile.txt', 'w') as f:\n",
    "    f.write(str(results))\n",
    "\n",
    "print(utils.make_table(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's the catch?\n",
    "\n",
    "I calculated the \"Percentage Difference of Means\" and \"95% Confidence Interval\" in the `MiniPile_Pile_Benchmark_Comparisons.ods` spreadsheet.<br>\n",
    "Crucially, we see the following results for the MiniPile-trained Pythia $160M$ model, compared against the Pile-trained Pythia $160M$ model:\n",
    "\n",
    "| Benchmark        | Measure      | 160M Pile Deduplicated | 160M MiniPile              | Percentage Difference of Means | 95% Confidence Interval      | Interpretation                            |\n",
    "| ---------------- | ------------ | ---------------------- | -------------------------- | ------------------------------ | ---------------------------- | ----------------------------------------- |\n",
    "| ARC-Challenge    | acc ↑        | 0.1997 ± 0.0117        | **0.2125 ± 0.0120**        | 6.4096                         | (0.0456; -0.0200)            | Difference not significant                |\n",
    "| MMLU             | acc ↑        | 0.2299 ± 0.0035        | **0.2699 ± 0.0037**        | 17.3989                        | (0.0500; 0.0300)             | MiniPile-trained better                   |\n",
    "| HellaSwag        | acc ↑        | **0.2903 ± 0.0045**    | 0.2560 ± 0.0044            | -11.8154                       | (-0.0220; -0.0466)           | Pile Deduplicated-trained better          |\n",
    "| WinoGrande       | acc ↑        | **0.4964 ± 0.0141**    | 0.4720 ± 0.0140            | -4.9154                        | (0.0145; -0.0633)            | Difference not significant                |\n",
    "| Lambada (OpenAI) | acc ↑        | **0.3689 ± 0.0067**    | 0.0000 ± 0.0000            | -100.00                        | (-0.3558; -0.3820)           | Pile Deduplicated-trained severely better |\n",
    "| Lambada (OpenAI) | perplexity ↓ | **31.2590 ± 1.1594**   | 3138574.4432 ± 302684.4607 | 10040446.5408                  | (3731804.7272; 2545281.6412) | Pile Deduplicated-trained severely better |\n",
    "\n",
    "- Training pipeline replication seems successful, as the MiniPile-trained model displays competitive performance on multiple benchmarks.\n",
    "    - Performance significantly better on MMLU\n",
    "    - Comparable performance on ARC-Challenge, WinoGrande\n",
    "    - Inferior performance on HellaSwag\n",
    "    - Severely inferior performance on Lambada (OpenAI)\n",
    "\n",
    "I interpret the results for the Lambada benchmark as follows:\n",
    "- MiniPile seems to lack distinctly crucial linguistic understanding that was certainly emitted from The Pile Deduplicated\n",
    "- It seems general linguistic understanding on MiniPile, given the extremely high perplexity, is severely lacking\n",
    "\n",
    "Therefore, while MiniPile can enable competitive performance on some tasks with three orders of magnitude less data, it seems to break down in capturing more general aspects of language necessary for those benchmarks involving next-token prediction in rather context-rich settings.\n",
    "\n",
    "**In other words:** There indeed seems to be no free lunch."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minipile",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
