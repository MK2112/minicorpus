{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking Pythia $160\\text{M}$ pre-trained on The Pile vs. Pythia $160\\text{M}$ trained on MiniPile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objectives:\n",
    "- [x] Prepare (Download) two models - **Pythia $160M$ Untrained** and **Pythia $160M$ fully Pile-trained**\n",
    "- [x] Load MiniPile Dataset from disk\n",
    "- [.] Train **Pythia $160M$ Untrained** on MiniPile (according to the MiniPile paper) *and save the model*\n",
    "- [x] Evaluate the performance of **Pythia $160M$ Pile-trained** on MMLU, ARC, WinoGrande, HellaSwag benchmarks\n",
    "- [.] Evaluate the performance of **Pythia $160M$ Untrained** on MMLU, ARC, WinoGrande, HellaSwag benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers datasets torch accelerate evaluate wandb\n",
    "! pip install lm-eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from torch.optim import Adam\n",
    "from datasets import load_dataset\n",
    "from lm_eval import tasks, evaluator, utils\n",
    "from huggingface_hub import snapshot_download\n",
    "from transformers import AutoModelForSequenceClassification, pipeline, EvalPrediction\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, get_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/mnt/data\"\n",
    "base_path = Path(base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Download Pythia $160\\text{M}$ Untrained and Pythia $160\\text{M}$ Pile-Trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_model(down_dir: str, target_folder: str, cache_folder: str, repo_id: str, branch: str = \"main\") -> None:\n",
    "    down_dir = Path(down_dir)\n",
    "    target_dir = down_dir / target_folder\n",
    "    cache_dir = down_dir / cache_folder\n",
    "\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"Downloading {repo_id}/{branch}...\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            snapshot_download(\n",
    "                repo_id,\n",
    "                repo_type=\"model\",\n",
    "                revision=branch,\n",
    "                cache_dir=str(cache_dir),\n",
    "                local_dir=str(target_dir)\n",
    "            )\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Download attempt failed: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading EleutherAI/pythia-160m-deduped/step0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ac24d6a7eed40d3b0044ad6fd12607a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "download_model(down_dir=base_dir, target_folder=\"pythia160m_dedup_untrained\", \n",
    "               cache_folder=\"pythia160m_dedup_untrained_Cache\",\n",
    "               repo_id=\"EleutherAI/pythia-160m-deduped\", branch=\"step0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading EleutherAI/pythia-160m-deduped/main...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee29f71a929c4363b4c65bd07482d2a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://huggingface.co/EleutherAI/pythia-160m/blob/main/README.md states:\n",
    "# \"[...] final step 143000 corresponds exactly to the model checkpoint on the main branch of each model.\"\n",
    "download_model(down_dir=base_dir, target_folder=\"pythia160m_dedup_pile\", \n",
    "               cache_folder=\"pythia160m_dedup_pile_Cache\",\n",
    "               repo_id=\"EleutherAI/pythia-160m-deduped\", branch=\"main\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Load MiniPile Dataset from Disk\n",
    "\n",
    "We expect the MiniPile dataset to already have been downloaded to disk at an earlier point.<br>\n",
    "The logic for this can be found in the `01_get_piles` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading minipile train + val splits from the local directory \n",
    "# https://stackoverflow.com/questions/77020278/how-to-load-a-huggingface-dataset-from-local-path\n",
    "# https://github.com/MK2112/mobileYOLOv3/blob/main/mobileyolov3-cocotext.ipynb\n",
    "# Split is named exactly like with the original dataset https://huggingface.co/datasets/JeanKaddour/minipile\n",
    "minipile_train = load_dataset(\"parquet\",\n",
    "                              data_files={\n",
    "                                  \"train\": str(base_path / \"MiniPile\" / \"data\" / \"train-*.parquet\"),\n",
    "                                  \"validation\": str(base_path / \"MiniPile\" / \"data\" / \"validation-*.parquet\"),\n",
    "                                  \"test\": str(base_path / \"MiniPile\" / \"data\" / \"test-*.parquet\")\n",
    "                              },\n",
    "                              cache_dir=str(base_path / \"MiniPile_Cache\"),\n",
    "                              split=\"train\")\n",
    "\n",
    "minipile_val = load_dataset(\"parquet\",\n",
    "                            data_files={\n",
    "                                \"train\": str(base_path / \"MiniPile\" / \"data\" / \"train-*.parquet\"),\n",
    "                                \"validation\": str(base_path / \"MiniPile\" / \"data\" / \"validation-*.parquet\"),\n",
    "                                \"test\": str(base_path / \"MiniPile\" / \"data\" / \"test-*.parquet\")\n",
    "                            },\n",
    "                            cache_dir=str(base_path / \"MiniPile_Cache\"),\n",
    "                            split=\"validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Hyperparameters for Pythia $160\\text{M}$ Untrained on MiniPile\n",
    "\n",
    "**Training Parameters for $160M$ for The Pile Deduplicated (*not* MiniPile)**<br>\n",
    "See [Pythia Paper](https://arxiv.org/abs/2304.01373) (p. 22) and [Pythia GitHub](https://github.com/EleutherAI/pythia/blob/main/models/160M/pythia-160m-deduped.yml):\n",
    "\n",
    "![](./img/pythia_train_params.png)\n",
    "\n",
    "- Each model gets exposed to $299,892,736,000 \\approx 300B$ tokens through training ($\\approx 1.5$ epochs on The Pile)\n",
    "- Batch size of $1024$ samples\n",
    "- Sequence length of $2048$\n",
    "- Adam optimizer with $\\beta_1 = 0.9$, $\\beta_2 = 0.95$, $\\epsilon = 1 \\times 10^{-8}$\n",
    "- Learning rates vary by model size:\n",
    "    - $70M$ model:  $10.0 \\times 10^{-4}$\n",
    "    - $160M$ model: $6.0 \\times 10^{-4}$\n",
    "    - $410M$ model: $3.0 \\times 10^{-4}$\n",
    "    - $1.0B$ model: $3.0 \\times 10^{-4}$\n",
    "    - $1.4B$ model: $2.0 \\times 10^{-4}$\n",
    "    - $2.8B$ model: $1.6 \\times 10^{-4}$\n",
    "    - $6.9B$ model: $1.2 \\times 10^{-4}$\n",
    "    - $12B$ model:  $1.2 \\times 10^{-4}$\n",
    "- train-iters $143000$\n",
    "- lr-decay-iters $143000$\n",
    "- lr-decay-style $\\text{cosine}$\n",
    "- lr-warmup $0.01$\n",
    "- weight-decay $0.01$\n",
    "- gradient-clipping $1.0$\n",
    "- lr-min $0.1 \\times \\text{optimizer.params.lr}$ (which isn't in the paper)\n",
    "- synchronize-each-layer $\\text{True}$ (i.e. gradients across all GPUs after each layer synced)\n",
    "- LR Scheduling: Decays to a minimum of $0.1\\times$ the maximum learning rate for all models\n",
    "- (Tokenizer is loaded as the same as for GPT-NeoX-20B)\n",
    "\n",
    "**Training Parameters for $160M$ for MiniPile**<br>\n",
    "See [MiniPile paper](https://arxiv.org/abs/2304.08442)\n",
    "- $1M/500/10k$ training/validation/test examples\n",
    "    - Vocab size: $32309614$\n",
    "    - Median document length: $294$\n",
    "    - Longest document length: $929633$\n",
    "\n",
    "**BERT Training Parameters for MiniPile**\n",
    "- Adam, $\\beta_1 = 0.9$, $\\beta_2 = 0.98$, $\\epsilon = 1 \\times 10^{-12}$\n",
    "- weight-decay $0.001$\n",
    "- One cycle policy with peak learning rate of $1 \\times 10^{-3}$\n",
    "- gradient-clipping $0.5$\n",
    "- Progressive batch size from $128$ to $4096$ with a linear increase over the course of training up to $300k$ steps, no warmup\n",
    "- $800k$ total training steps\n",
    "- weight averaging of the $k = 5$ latest checkpoints and $1k$ steps distance between them\n",
    "\n",
    "**T5 Training Parameters for MiniPile**\n",
    "- AdamW, matrix-wise LR scaling by its root mean square (RMS), no weight decay\n",
    "- base learning rate $0.02$\n",
    "- cosine schedule with final of $1 \\times 10^{-5}$\n",
    "- gradient-clipping $1.0$\n",
    "- batch size $288$\n",
    "- $10k$ warmup steps, $65536$ total training steps\n",
    "- weight averaging of the $k = 5$ latest checkpoints and $1k$ steps distance between them (akin to BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These training parameters are a good start, but they can only be interpreted as at most guiding, because they were applied for decoder-only and encoder-decoder models, yet not for pure decoder-only models like Pythia. Thus, if possible, one should look for approaches trained solely on MiniPile following the decoder-only paradigm for a more accurate guide to our own approach with Pythia. \n",
    "\n",
    "Luckily there exists a [GPT NeoX 122M MiniPile](https://huggingface.co/euclaise/gpt-neox-122m-minipile-digits) model that can be reverse-engineered for our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped for version mismatch.\n"
     ]
    }
   ],
   "source": [
    "# Load the training arguments from the minipile-trained decode reference model GPT-NeoX-122M:\n",
    "# https://huggingface.co/euclaise/gpt-neox-122m-minipile-digits\n",
    "\n",
    "# Newer versions fail for missing attributes, 4.30.0 is documented to have been used\n",
    "if str(transformers.__version__) == \"4.30.0\":\n",
    "    training_args = torch.load(base_path / 'training_args_gptNEO122m.bin', weights_only=False)\n",
    "    output_file = 'train_args_gptNEO122m_minipile.txt'\n",
    "    try: \n",
    "        with open(output_file, 'w') as f:\n",
    "            f.write(\"TrainingArguments attributes:\\n\")\n",
    "            for attr in dir(training_args):\n",
    "                if hasattr(training_args, attr) and not attr.startswith('_'):\n",
    "                    value = getattr(training_args, attr)\n",
    "                    f.write(f\"- {attr}: {value}\\n\")\n",
    "    except NameError as _:\n",
    "        pass # Fully ignore NameError, appears every time\n",
    "else:\n",
    "    print('Skipped for version mismatch.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GPTNeoX model card is a bit misleading, as it is stated that this model was trained exclusively on MiniPile. The tiny learning rate $5 \\times 10^{-6}$ with no weight decay implies a fine-tuning approach.\n",
    "\n",
    "I did this mostly to get a feeling for much the encoder-based model params deviate from the decoder-based model params.<br>\n",
    "I interpret the results as not too far off, e.g. we use the exact same learning rate and optimizer.<br>\n",
    "\n",
    "This implies that the training params on The Pile for Pythia $160M$ are a good starting point and we can scale these to accommodate the MiniPile dataset size and expect appropriate training effects.\n",
    "\n",
    "Core parameters are however not directly transferable: `train-iters` and therefore also `lr-decay-iters`.<br>\n",
    "For Pile deduplicated this was $143000$, but we have to scale this to the MiniPile dataset size, as the number of tokens processed by the model is crucial for the training process and could lead to overfitting and not accurately reflecting dataset knowledge retention capabilities if not adjusted properly.\n",
    "\n",
    "In other words, overshooting distorts dataset knowledge, while undershooting leads to underfitting and insufficient representation of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Byte-based scale factor: 139.609153x\n",
      "MiniPile (scaled) Train-Iters/LR-Decay-Iters: 1024.288 ~ 1024\n"
     ]
    }
   ],
   "source": [
    "# I use the byte sizes as proxy for the number of tokens, as both datasets will get tokenized with the same tokenizer\n",
    "minipile_train_bytes = 5906108510 # see https://huggingface.co/datasets/JeanKaddour/minipile/blob/main/README.md\n",
    "pile_train_bytes = 824546807506   # see https://huggingface.co/datasets/EleutherAI/the_pile_deduplicated/blob/main/dataset_infos.json\n",
    "pile_effective_epochs = 1.5       # this many epochs are actually trained in the original model (calculation isn't affected, training params below are)\n",
    "\n",
    "scale_factor = (pile_train_bytes * pile_effective_epochs) / (minipile_train_bytes * pile_effective_epochs)\n",
    "print(f\"Byte-based scale factor: {scale_factor:10.6f}x\")\n",
    "print(f\"MiniPile (scaled) Train-Iters/LR-Decay-Iters: {143000 / scale_factor:.3f} ~ {round(143000 / scale_factor)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point the $1024$ for training iterations may seem awkwardly small.<br>\n",
    "But, to reiterate, we strictly scaled it down iterations according to dataset size difference.\n",
    "\n",
    "While this may seem horrible in most other cases, as we thoroughly neuter exposure to data, this scale-correct limiting and overall lower exposure is exactly what we need here to operate relative to the original Pythia training. After all, the goal is to compare knowledge retention and generalization capabilities achievable on `The Pile Deduplicated` vs. the 'distilled' `MiniPile` under size-appropriate, similar conditions. Therefore, scaling the `train-iters` and therefore also `lr-decay-iters` using byte sizes as a proxy is actually appropriate here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now lay out the complete parameters:<br>\n",
    "With the three approach descriptions retrieved, we can take a more educated guess at the training params for Pythia $160M$ on MiniPile:\n",
    "\n",
    "- Adam optimizer (GPT NeoX and T5-Base MiniPile suggest the 'generally more stable' AdamW, but Pythia uses Adam so we keep it most similar)\n",
    "    - $\\beta_1 = 0.9$, $\\beta_2 = 0.95$, (Pythia)\n",
    "    - $\\epsilon = 1 \\times 10^{-8}$ (GPT NeoX and Pythia)\n",
    "    - learning rate $6 \\times 10^{-4}$ (Pythia)\n",
    "    - lr-schedule $\\text{cosine annealing}$ (Pythia)\n",
    "    - lr-warmup $0.01$ of total steps (Pythia)\n",
    "    - lr-min $0.1 \\times \\text{lr}$ (Pythia)\n",
    "    - weight-decay $1 \\times 10^{-2}$ (Pythia)\n",
    "- gradient-clipping $1.0$ (Pythia)\n",
    "- batch size $1024$ (Pythia, probably grad accum needed, expect multi-GPU)\n",
    "- sequence length $2048$ (Pythia)\n",
    "- **train-iters: $1024$ (MiniPile-specific)**\n",
    "- **lr-decay-iters: same as train-iters (MiniPile-specific)**\n",
    "- (won't do mixed precision for sake of most similar training conditions to Pile-trained Pythia)\n",
    "- (won't do weight averaging)\n",
    "- **Same GPT-NeoX-20B tokenizer as for Pythia-Pile**\n",
    "\n",
    "We can start training Pythia $160\\text{M}$ on MiniPile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Train Pythia $160\\text{M}$ Untrained on MiniPile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the untrained Pythia 160M tokenizer and model\n",
    "# https://stackoverflow.com/questions/64001128/load-a-pre-trained-model-from-disk-with-huggingface-transformers\n",
    "# Tokenizer is in fact a GPTNeoXTokenizer, only has a fast version available\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_path / \"pythia160m_dedup_untrained\", use_fast=True, local_files_only=True)\n",
    "empty_model = AutoModelForCausalLM.from_pretrained(base_path / \"pythia160m_dedup_untrained\", local_files_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pythia paper states a standard configuration where individual examples consist of up to $2048$ tokens.<br>\n",
    "This explains why the tokenizer doesn't contain a padding token, as the model is trained on variable-length sequences with this upper bound instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer doesn't have a pad token, use EOS as a substitute\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "def tokenize(example): \n",
    "    # seq_len = max_length = 2048 (as upper boundary, so not strict size -> no padding needed)\n",
    "    return tokenizer(example[\"text\"], \n",
    "                     truncation=True, \n",
    "                     max_length=2048,\n",
    "                     return_special_tokens_mask=True)\n",
    "\n",
    "if os.path.exists(base_path / \"minipile_train_tokenized\"):\n",
    "    minipile_train_tokenized = load_dataset(\"arrow\", data_files=str(base_path / \"minipile_train_tokenized/*.arrow\"), split=\"train\")\n",
    "    minipile_val_tokenized = load_dataset(\"arrow\", data_files=str(base_path / \"minipile_val_tokenized/*.arrow\"), split=\"train\")\n",
    "else:\n",
    "    minipile_train_tokenized = minipile_train.map(tokenize, batched=True, remove_columns=minipile_train.column_names) # retain only new fields from tokenization\n",
    "    minipile_val_tokenized = minipile_val.map(tokenize, batched=True, remove_columns=minipile_val.column_names)\n",
    "    minipile_train_tokenized.save_to_disk(base_path / \"minipile_train_tokenized\")\n",
    "    minipile_val_tokenized.save_to_disk(base_path / \"minipile_val_tokenized\")\n",
    "\n",
    "# Dynamic padding during training (mlm -> mask language model -> we're doing causal here)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs: 1\n",
      "\n",
      "GPU 0:\tNVIDIA GeForce RTX 3060\n",
      "\tTotal memory:\t\t11.76 GiB\n",
      "\tAllocated memory:\t 0.00 GiB\n",
      "\tFree memory:\t\t11.76 GiB\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    device_count = torch.cuda.device_count()\n",
    "    print(f\"Available GPUs: {device_count}\")\n",
    "    for i in range(device_count):\n",
    "        device = torch.device(f'cuda:{i}')\n",
    "        device_properties = torch.cuda.get_device_properties(device)\n",
    "        total_mem = device_properties.total_memory / (1024 ** 3)\n",
    "        allocd_mem = torch.cuda.memory_allocated(device) / (1024 ** 3)\n",
    "        free_mem = total_mem - allocd_mem\n",
    "        print(f\"\\nGPU {i}:\\t{device_properties.name}\")\n",
    "        print(f\"\\tTotal memory:\\t\\t{total_mem:.2f} GiB\")\n",
    "        print(f\"\\tAllocated memory:\\t{allocd_mem:5.2f} GiB\")\n",
    "        print(f\"\\tFree memory:\\t\\t{free_mem:.2f} GiB\")\n",
    "else:\n",
    "    print(\"No CUDA-capable GPUs available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = str(base_path / \"pythia160m_minipile_trained\")\n",
    "log_dir = str(base_path / \"160m_minipile_logs\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# https://huggingface.co/docs/transformers/v4.46.0/en/main_classes/trainer#transformers.TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1.5,            # Since train_iters gets set, use num_train_epochs=1.5 like for The Pile\n",
    "    per_device_train_batch_size=4,   # Gives an effective batch size of 1024 after grad accum\n",
    "    per_device_eval_batch_size=4,    # Same as training batch size\n",
    "    gradient_accumulation_steps=256, # Achieve a batch size of 1024\n",
    "    learning_rate=6e-4,              # Default Pythia 160M\n",
    "    weight_decay=0.01,               # Default Pythia 160M\n",
    "    max_steps=1024,                  # Adjusted for MiniPile (https://discuss.huggingface.co/t/how-does-max-steps-affect-the-number-of-samples-the-model-sees/69681)\n",
    "    lr_scheduler_type=\"cosine\",      # As per Pythia 160M paper\n",
    "    warmup_steps=int(0.01 * 1024),   # 1% of total steps for warmup\n",
    "    logging_dir=log_dir,\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,     # Frequency for evaluation during training\n",
    "    save_steps=1024,    # Save at the end of training\n",
    "    save_total_limit=1, # Only keep the most recent checkpoint\n",
    "    fp16=False,         # Not using mixed precision for comparable conditions\n",
    "    report_to=None,     # Noting this for later iterations, maybe set this as \"wandb\", \"tensorboard\" or smth\n",
    "    ddp_find_unused_parameters=False, # see https://discuss.pytorch.org/t/how-to-change-ddp-parameter-find-unused-parameters-true-to-false-during-training/130763\n",
    "    max_grad_norm=1.0,  # As per Pythia 160M paper\n",
    ")\n",
    "\n",
    "# Ensure training across multiple GPUs if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "empty_model = empty_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(empty_model.parameters(), lr=training_args.learning_rate, betas=(0.9, 0.95), eps=1e-8, weight_decay=0.01)\n",
    "\n",
    "# Train Pythia 160M Untrained on MiniPile\n",
    "# https://huggingface.co/docs/transformers/v4.46.0/en/main_classes/trainer\n",
    "trainer = Trainer(model=empty_model,\n",
    "                  args=training_args,\n",
    "                  train_dataset=minipile_train_tokenized,\n",
    "                  eval_dataset=minipile_val_tokenized,\n",
    "                  data_collator=data_collator,\n",
    "                  optimizers=(optimizer, None))\n",
    "\n",
    "scheduler = get_scheduler(name=training_args.lr_scheduler_type,\n",
    "                          optimizer=optimizer,\n",
    "                          num_warmup_steps=training_args.warmup_steps,\n",
    "                          num_training_steps=training_args.max_steps)\n",
    "\n",
    "num_batches = len(trainer.get_train_dataloader())  # Number of batches\n",
    "total_training_steps = num_batches * training_args.gradient_accumulation_steps * int(training_args.num_train_epochs)\n",
    "\n",
    "# Training loop with manual minimum learning rate enforcement\n",
    "for epoch in range(int(training_args.num_train_epochs)):\n",
    "    with tqdm(total=total_training_steps, desc=f\"Training Epoch {epoch + 1}/{int(training_args.num_train_epochs)}\") as pbar:\n",
    "        for _, batch in enumerate(trainer.get_train_dataloader()):\n",
    "            trainer.training_step(trainer.model, batch)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            for param_group in optimizer.param_groups:\n",
    "                # Manually ... ensure lr doesn't go below min_lr (Pythia wants this)\n",
    "                param_group['lr'] = max(param_group['lr'], 0.1 * training_args.learning_rate)\n",
    "            pbar.update(1)\n",
    "\n",
    "# Why is this a two-step process?!\n",
    "trainer.save_model(str(base_path / \"pythia160m_minipile_trained\")) # This saves the model weights\n",
    "tokenizer.save_pretrained(str(base_path / \"pythia160m_minipile_trained\")) # This saves the tokenizer (don't know if needed, better save than sorry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Evaluate Pythia $160\\text{M}$ MiniPile vs. Pythia $160\\text{M}$ Pile-Trained on Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-13:13:16:08,313 WARNING  [huggingface.py:95] `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "2024-11-13:13:16:08,314 INFO     [huggingface.py:481] Using model type 'default'\n",
      "2024-11-13:13:16:08,325 WARNING  [huggingface.py:275] Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "2024-11-13:13:16:08,328 INFO     [evaluator.py:164] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2024-11-13:13:16:08,329 INFO     [evaluator.py:217] Using pre-initialized model\n",
      "2024-11-13:13:16:14,701 INFO     [__init__.py:459] The tag 'arc_ca' is already registered as a group, this tag will not be registered. This may affect tasks you want to call.\n",
      "2024-11-13:13:16:14,708 INFO     [__init__.py:459] The tag 'arc_ca' is already registered as a group, this tag will not be registered. This may affect tasks you want to call.\n",
      "Using the latest cached version of the dataset since allenai/ai2_arc couldn't be found on the Hugging Face Hub\n",
      "2024-11-13:13:16:19,690 WARNING  [load.py:1431] Using the latest cached version of the dataset since allenai/ai2_arc couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'ARC-Challenge' at /home/marcus/.cache/huggingface/datasets/allenai___ai2_arc/ARC-Challenge/0.0.0/210d026faf9955653af8916fad021475a3f00453 (last modified on Tue Nov 12 21:57:47 2024).\n",
      "2024-11-13:13:16:19,693 WARNING  [cache.py:94] Found the latest cached dataset configuration 'ARC-Challenge' at /home/marcus/.cache/huggingface/datasets/allenai___ai2_arc/ARC-Challenge/0.0.0/210d026faf9955653af8916fad021475a3f00453 (last modified on Tue Nov 12 21:57:47 2024).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fa0588bbf9947889ed65aa7dae4f1e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/7.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "420c253f64dd435b8aa6d46ed1f6680c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00002.parquet:   0%|          | 0.00/269M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c99a188701e496b8ccbd64159a591fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00001-of-00002.parquet:   0%|          | 0.00/281M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2d17aa13923401fac0ba83dd44c235e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/1.14M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fbf091d94ee436390bc94eb6dbfadf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/1.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb3615b83b00479d90889694bfa86004",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/2662 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "805a73082e7042b281faf0e1b21899c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/5153 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ae1cfb7909c416699c4ca9bc6cf51c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/4869 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7affd17df7ac47179d9fdd95cbad4654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0000.parquet:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5943595f2e034c38b8bda546e824d72f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/5153 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-13:13:19:53,234 WARNING  [task.py:325] [Task: lambada_openai] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
      "2024-11-13:13:19:53,235 WARNING  [task.py:325] [Task: lambada_openai] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
      "2024-11-13:13:19:53,529 WARNING  [evaluator.py:270] Overwriting default num_fewshot of lambada_openai from None to 0\n",
      "2024-11-13:13:19:53,529 WARNING  [evaluator.py:270] Overwriting default num_fewshot of lambada_standard from None to 0\n",
      "2024-11-13:13:19:53,530 WARNING  [evaluator.py:270] Overwriting default num_fewshot of hellaswag from None to 0\n",
      "2024-11-13:13:19:53,531 WARNING  [evaluator.py:270] Overwriting default num_fewshot of winogrande from None to 0\n",
      "2024-11-13:13:19:53,532 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_anatomy from None to 0\n",
      "2024-11-13:13:19:53,533 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_astronomy from None to 0\n",
      "2024-11-13:13:19:53,534 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_college_chemistry from None to 0\n",
      "2024-11-13:13:19:53,534 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_computer_science from None to 0\n",
      "2024-11-13:13:19:53,535 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_electrical_engineering from None to 0\n",
      "2024-11-13:13:19:53,536 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_computer_security from None to 0\n",
      "2024-11-13:13:19:53,537 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_conceptual_physics from None to 0\n",
      "2024-11-13:13:19:53,538 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_college_physics from None to 0\n",
      "2024-11-13:13:19:53,538 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_physics from None to 0\n",
      "2024-11-13:13:19:53,539 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_abstract_algebra from None to 0\n",
      "2024-11-13:13:19:53,540 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_statistics from None to 0\n",
      "2024-11-13:13:19:53,541 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_college_computer_science from None to 0\n",
      "2024-11-13:13:19:53,541 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_college_mathematics from None to 0\n",
      "2024-11-13:13:19:53,542 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_chemistry from None to 0\n",
      "2024-11-13:13:19:53,542 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_college_biology from None to 0\n",
      "2024-11-13:13:19:53,543 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_machine_learning from None to 0\n",
      "2024-11-13:13:19:53,544 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_elementary_mathematics from None to 0\n",
      "2024-11-13:13:19:53,544 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_biology from None to 0\n",
      "2024-11-13:13:19:53,545 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_mathematics from None to 0\n",
      "2024-11-13:13:19:53,545 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_management from None to 0\n",
      "2024-11-13:13:19:53,546 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_global_facts from None to 0\n",
      "2024-11-13:13:19:53,547 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_marketing from None to 0\n",
      "2024-11-13:13:19:53,547 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_nutrition from None to 0\n",
      "2024-11-13:13:19:53,548 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_clinical_knowledge from None to 0\n",
      "2024-11-13:13:19:53,549 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_medical_genetics from None to 0\n",
      "2024-11-13:13:19:53,550 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_virology from None to 0\n",
      "2024-11-13:13:19:53,551 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_miscellaneous from None to 0\n",
      "2024-11-13:13:19:53,551 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_college_medicine from None to 0\n",
      "2024-11-13:13:19:53,552 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_business_ethics from None to 0\n",
      "2024-11-13:13:19:53,553 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_professional_accounting from None to 0\n",
      "2024-11-13:13:19:53,554 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_human_aging from None to 0\n",
      "2024-11-13:13:19:53,555 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_professional_medicine from None to 0\n",
      "2024-11-13:13:19:53,557 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_geography from None to 0\n",
      "2024-11-13:13:19:53,558 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_professional_psychology from None to 0\n",
      "2024-11-13:13:19:53,559 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_public_relations from None to 0\n",
      "2024-11-13:13:19:53,559 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_government_and_politics from None to 0\n",
      "2024-11-13:13:19:53,560 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_security_studies from None to 0\n",
      "2024-11-13:13:19:53,561 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_sociology from None to 0\n",
      "2024-11-13:13:19:53,562 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_psychology from None to 0\n",
      "2024-11-13:13:19:53,563 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_human_sexuality from None to 0\n",
      "2024-11-13:13:19:53,563 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_macroeconomics from None to 0\n",
      "2024-11-13:13:19:53,564 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_us_foreign_policy from None to 0\n",
      "2024-11-13:13:19:53,565 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_econometrics from None to 0\n",
      "2024-11-13:13:19:53,566 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_microeconomics from None to 0\n",
      "2024-11-13:13:19:53,567 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_moral_disputes from None to 0\n",
      "2024-11-13:13:19:53,568 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_international_law from None to 0\n",
      "2024-11-13:13:19:53,568 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_logical_fallacies from None to 0\n",
      "2024-11-13:13:19:53,569 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_world_history from None to 0\n",
      "2024-11-13:13:19:53,570 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_world_religions from None to 0\n",
      "2024-11-13:13:19:53,571 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_moral_scenarios from None to 0\n",
      "2024-11-13:13:19:53,571 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_formal_logic from None to 0\n",
      "2024-11-13:13:19:53,572 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_european_history from None to 0\n",
      "2024-11-13:13:19:53,573 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_professional_law from None to 0\n",
      "2024-11-13:13:19:53,574 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_philosophy from None to 0\n",
      "2024-11-13:13:19:53,575 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_us_history from None to 0\n",
      "2024-11-13:13:19:53,575 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_prehistory from None to 0\n",
      "2024-11-13:13:19:53,576 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_jurisprudence from None to 0\n",
      "2024-11-13:13:19:53,577 WARNING  [evaluator.py:270] Overwriting default num_fewshot of arc_challenge from None to 0\n",
      "2024-11-13:13:19:53,585 INFO     [task.py:415] Building contexts for lambada_openai on rank 0...\n",
      "100%|██████████| 5153/5153 [00:14<00:00, 350.22it/s]\n",
      "2024-11-13:13:20:08,591 INFO     [task.py:415] Building contexts for lambada_standard on rank 0...\n",
      "100%|██████████| 5153/5153 [00:14<00:00, 351.16it/s]\n",
      "2024-11-13:13:20:23,603 INFO     [task.py:415] Building contexts for hellaswag on rank 0...\n",
      "100%|██████████| 10042/10042 [00:06<00:00, 1632.85it/s]\n",
      "2024-11-13:13:20:32,752 INFO     [task.py:415] Building contexts for winogrande on rank 0...\n",
      "100%|██████████| 1267/1267 [00:00<00:00, 43889.85it/s]\n",
      "2024-11-13:13:20:32,917 INFO     [task.py:415] Building contexts for mmlu_anatomy on rank 0...\n",
      "100%|██████████| 135/135 [00:00<00:00, 383.77it/s]\n",
      "2024-11-13:13:20:33,293 INFO     [task.py:415] Building contexts for mmlu_astronomy on rank 0...\n",
      "100%|██████████| 152/152 [00:00<00:00, 381.86it/s]\n",
      "2024-11-13:13:20:33,717 INFO     [task.py:415] Building contexts for mmlu_college_chemistry on rank 0...\n",
      "100%|██████████| 100/100 [00:00<00:00, 391.57it/s]\n",
      "2024-11-13:13:20:33,991 INFO     [task.py:415] Building contexts for mmlu_high_school_computer_science on rank 0...\n",
      "100%|██████████| 100/100 [00:00<00:00, 396.67it/s]\n",
      "2024-11-13:13:20:34,260 INFO     [task.py:415] Building contexts for mmlu_electrical_engineering on rank 0...\n",
      "100%|██████████| 145/145 [00:00<00:00, 356.68it/s]\n",
      "2024-11-13:13:20:34,690 INFO     [task.py:415] Building contexts for mmlu_computer_security on rank 0...\n",
      "100%|██████████| 100/100 [00:01<00:00, 51.91it/s]\n",
      "2024-11-13:13:20:36,636 INFO     [task.py:415] Building contexts for mmlu_conceptual_physics on rank 0...\n",
      "100%|██████████| 235/235 [00:00<00:00, 379.79it/s]\n",
      "2024-11-13:13:20:37,296 INFO     [task.py:415] Building contexts for mmlu_college_physics on rank 0...\n",
      "100%|██████████| 102/102 [00:00<00:00, 374.11it/s]\n",
      "2024-11-13:13:20:37,588 INFO     [task.py:415] Building contexts for mmlu_high_school_physics on rank 0...\n",
      "100%|██████████| 151/151 [00:00<00:00, 384.47it/s]\n",
      "2024-11-13:13:20:38,010 INFO     [task.py:415] Building contexts for mmlu_abstract_algebra on rank 0...\n",
      "100%|██████████| 100/100 [00:00<00:00, 397.45it/s]\n",
      "2024-11-13:13:20:38,278 INFO     [task.py:415] Building contexts for mmlu_high_school_statistics on rank 0...\n",
      "100%|██████████| 216/216 [00:00<00:00, 377.85it/s]\n",
      "2024-11-13:13:20:38,888 INFO     [task.py:415] Building contexts for mmlu_college_computer_science on rank 0...\n",
      "100%|██████████| 100/100 [00:00<00:00, 379.15it/s]\n",
      "2024-11-13:13:20:39,171 INFO     [task.py:415] Building contexts for mmlu_college_mathematics on rank 0...\n",
      "100%|██████████| 100/100 [00:00<00:00, 378.06it/s]\n",
      "2024-11-13:13:20:39,453 INFO     [task.py:415] Building contexts for mmlu_high_school_chemistry on rank 0...\n",
      "100%|██████████| 203/203 [00:00<00:00, 398.13it/s]\n",
      "2024-11-13:13:20:39,993 INFO     [task.py:415] Building contexts for mmlu_college_biology on rank 0...\n",
      "100%|██████████| 144/144 [00:00<00:00, 386.04it/s]\n",
      "2024-11-13:13:20:40,389 INFO     [task.py:415] Building contexts for mmlu_machine_learning on rank 0...\n",
      "100%|██████████| 112/112 [00:00<00:00, 405.31it/s]\n",
      "2024-11-13:13:20:40,684 INFO     [task.py:415] Building contexts for mmlu_elementary_mathematics on rank 0...\n",
      "100%|██████████| 378/378 [00:00<00:00, 380.25it/s]\n",
      "2024-11-13:13:20:41,733 INFO     [task.py:415] Building contexts for mmlu_high_school_biology on rank 0...\n",
      "100%|██████████| 310/310 [00:00<00:00, 390.60it/s]\n",
      "2024-11-13:13:20:42,570 INFO     [task.py:415] Building contexts for mmlu_high_school_mathematics on rank 0...\n",
      "100%|██████████| 270/270 [00:00<00:00, 386.91it/s]\n",
      "2024-11-13:13:20:43,306 INFO     [task.py:415] Building contexts for mmlu_management on rank 0...\n",
      "100%|██████████| 103/103 [00:00<00:00, 382.66it/s]\n",
      "2024-11-13:13:20:43,597 INFO     [task.py:415] Building contexts for mmlu_global_facts on rank 0...\n",
      "100%|██████████| 100/100 [00:00<00:00, 348.94it/s]\n",
      "2024-11-13:13:20:43,903 INFO     [task.py:415] Building contexts for mmlu_marketing on rank 0...\n",
      "100%|██████████| 234/234 [00:00<00:00, 386.08it/s]\n",
      "2024-11-13:13:20:44,544 INFO     [task.py:415] Building contexts for mmlu_nutrition on rank 0...\n",
      "100%|██████████| 306/306 [00:00<00:00, 381.85it/s]\n",
      "2024-11-13:13:20:45,393 INFO     [task.py:415] Building contexts for mmlu_clinical_knowledge on rank 0...\n",
      "100%|██████████| 265/265 [00:00<00:00, 366.23it/s]\n",
      "2024-11-13:13:20:46,170 INFO     [task.py:415] Building contexts for mmlu_medical_genetics on rank 0...\n",
      "100%|██████████| 100/100 [00:00<00:00, 383.19it/s]\n",
      "2024-11-13:13:20:46,450 INFO     [task.py:415] Building contexts for mmlu_virology on rank 0...\n",
      "100%|██████████| 166/166 [00:00<00:00, 390.79it/s]\n",
      "2024-11-13:13:20:46,906 INFO     [task.py:415] Building contexts for mmlu_miscellaneous on rank 0...\n",
      "100%|██████████| 783/783 [00:02<00:00, 382.25it/s]\n",
      "2024-11-13:13:20:49,060 INFO     [task.py:415] Building contexts for mmlu_college_medicine on rank 0...\n",
      "100%|██████████| 173/173 [00:00<00:00, 394.34it/s]\n",
      "2024-11-13:13:20:49,530 INFO     [task.py:415] Building contexts for mmlu_business_ethics on rank 0...\n",
      "100%|██████████| 100/100 [00:00<00:00, 392.86it/s]\n",
      "2024-11-13:13:20:49,803 INFO     [task.py:415] Building contexts for mmlu_professional_accounting on rank 0...\n",
      "100%|██████████| 282/282 [00:00<00:00, 391.70it/s]\n",
      "2024-11-13:13:20:50,565 INFO     [task.py:415] Building contexts for mmlu_human_aging on rank 0...\n",
      "100%|██████████| 223/223 [00:00<00:00, 387.89it/s]\n",
      "2024-11-13:13:20:51,177 INFO     [task.py:415] Building contexts for mmlu_professional_medicine on rank 0...\n",
      "100%|██████████| 272/272 [00:00<00:00, 379.06it/s]\n",
      "2024-11-13:13:20:51,935 INFO     [task.py:415] Building contexts for mmlu_high_school_geography on rank 0...\n",
      "100%|██████████| 198/198 [00:00<00:00, 407.86it/s]\n",
      "2024-11-13:13:20:52,450 INFO     [task.py:415] Building contexts for mmlu_professional_psychology on rank 0...\n",
      "100%|██████████| 612/612 [00:01<00:00, 401.13it/s]\n",
      "2024-11-13:13:20:54,060 INFO     [task.py:415] Building contexts for mmlu_public_relations on rank 0...\n",
      "100%|██████████| 110/110 [00:00<00:00, 404.97it/s]\n",
      "2024-11-13:13:20:54,354 INFO     [task.py:415] Building contexts for mmlu_high_school_government_and_politics on rank 0...\n",
      "100%|██████████| 193/193 [00:00<00:00, 386.57it/s]\n",
      "2024-11-13:13:20:54,881 INFO     [task.py:415] Building contexts for mmlu_security_studies on rank 0...\n",
      "100%|██████████| 245/245 [00:00<00:00, 412.77it/s]\n",
      "2024-11-13:13:20:55,510 INFO     [task.py:415] Building contexts for mmlu_sociology on rank 0...\n",
      "100%|██████████| 201/201 [00:00<00:00, 412.06it/s]\n",
      "2024-11-13:13:20:56,033 INFO     [task.py:415] Building contexts for mmlu_high_school_psychology on rank 0...\n",
      "100%|██████████| 545/545 [00:01<00:00, 411.35it/s]\n",
      "2024-11-13:13:20:57,430 INFO     [task.py:415] Building contexts for mmlu_human_sexuality on rank 0...\n",
      "100%|██████████| 131/131 [00:00<00:00, 411.27it/s]\n",
      "2024-11-13:13:20:57,770 INFO     [task.py:415] Building contexts for mmlu_high_school_macroeconomics on rank 0...\n",
      "100%|██████████| 390/390 [00:00<00:00, 398.65it/s]\n",
      "2024-11-13:13:20:58,806 INFO     [task.py:415] Building contexts for mmlu_us_foreign_policy on rank 0...\n",
      "100%|██████████| 100/100 [00:00<00:00, 396.43it/s]\n",
      "2024-11-13:13:20:59,076 INFO     [task.py:415] Building contexts for mmlu_econometrics on rank 0...\n",
      "100%|██████████| 114/114 [00:00<00:00, 400.27it/s]\n",
      "2024-11-13:13:20:59,380 INFO     [task.py:415] Building contexts for mmlu_high_school_microeconomics on rank 0...\n",
      "100%|██████████| 238/238 [00:00<00:00, 400.11it/s]\n",
      "2024-11-13:13:21:00,011 INFO     [task.py:415] Building contexts for mmlu_moral_disputes on rank 0...\n",
      "100%|██████████| 346/346 [00:00<00:00, 402.84it/s]\n",
      "2024-11-13:13:21:00,918 INFO     [task.py:415] Building contexts for mmlu_international_law on rank 0...\n",
      "100%|██████████| 121/121 [00:00<00:00, 401.36it/s]\n",
      "2024-11-13:13:21:01,239 INFO     [task.py:415] Building contexts for mmlu_logical_fallacies on rank 0...\n",
      "100%|██████████| 163/163 [00:00<00:00, 406.23it/s]\n",
      "2024-11-13:13:21:01,666 INFO     [task.py:415] Building contexts for mmlu_high_school_world_history on rank 0...\n",
      "100%|██████████| 237/237 [00:00<00:00, 405.76it/s]\n",
      "2024-11-13:13:21:02,290 INFO     [task.py:415] Building contexts for mmlu_world_religions on rank 0...\n",
      "100%|██████████| 171/171 [00:00<00:00, 384.60it/s]\n",
      "2024-11-13:13:21:02,761 INFO     [task.py:415] Building contexts for mmlu_moral_scenarios on rank 0...\n",
      "100%|██████████| 895/895 [00:02<00:00, 400.60it/s]\n",
      "2024-11-13:13:21:05,138 INFO     [task.py:415] Building contexts for mmlu_formal_logic on rank 0...\n",
      "100%|██████████| 126/126 [00:00<00:00, 410.64it/s]\n",
      "2024-11-13:13:21:05,467 INFO     [task.py:415] Building contexts for mmlu_high_school_european_history on rank 0...\n",
      "100%|██████████| 165/165 [00:00<00:00, 394.15it/s]\n",
      "2024-11-13:13:21:05,915 INFO     [task.py:415] Building contexts for mmlu_professional_law on rank 0...\n",
      "100%|██████████| 1534/1534 [00:03<00:00, 406.90it/s]\n",
      "2024-11-13:13:21:09,893 INFO     [task.py:415] Building contexts for mmlu_philosophy on rank 0...\n",
      "100%|██████████| 311/311 [00:00<00:00, 414.01it/s]\n",
      "2024-11-13:13:21:10,687 INFO     [task.py:415] Building contexts for mmlu_high_school_us_history on rank 0...\n",
      "100%|██████████| 204/204 [00:00<00:00, 413.82it/s]\n",
      "2024-11-13:13:21:11,216 INFO     [task.py:415] Building contexts for mmlu_prehistory on rank 0...\n",
      "100%|██████████| 324/324 [00:00<00:00, 417.30it/s]\n",
      "2024-11-13:13:21:12,037 INFO     [task.py:415] Building contexts for mmlu_jurisprudence on rank 0...\n",
      "100%|██████████| 108/108 [00:00<00:00, 394.37it/s]\n",
      "2024-11-13:13:21:12,328 INFO     [task.py:415] Building contexts for arc_challenge on rank 0...\n",
      "100%|██████████| 1172/1172 [00:01<00:00, 738.28it/s]\n",
      "2024-11-13:13:21:14,109 INFO     [evaluator.py:489] Running loglikelihood requests\n",
      "Running loglikelihood requests: 100%|██████████| 113863/113863 [28:41<00:00, 66.14it/s] \n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bootstrapping for stddev: perplexity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:07<00:00, 12.74it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bootstrapping for stddev: perplexity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:07<00:00, 12.66it/s]\n",
      "2024-11-13:13:52:43,222 WARNING  [huggingface.py:1353] Failed to get model SHA for GPTNeoXForCausalLM(\n",
      "  (gpt_neox): GPTNeoXModel(\n",
      "    (embed_in): Embedding(50304, 768)\n",
      "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x GPTNeoXLayer(\n",
      "        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (attention): GPTNeoXSdpaAttention(\n",
      "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "          (query_key_value): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (mlp): GPTNeoXMLP(\n",
      "          (dense_h_to_4h): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (dense_4h_to_h): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): GELUActivation()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  )\n",
      "  (embed_out): Linear(in_features=768, out_features=50304, bias=False)\n",
      ") at revision main. Error: Repo id must be a string, not <class 'transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXForCausalLM'>: 'GPTNeoXForCausalLM(\n",
      "  (gpt_neox): GPTNeoXModel(\n",
      "    (embed_in): Embedding(50304, 768)\n",
      "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x GPTNeoXLayer(\n",
      "        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (attention): GPTNeoXSdpaAttention(\n",
      "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "          (query_key_value): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (mlp): GPTNeoXMLP(\n",
      "          (dense_h_to_4h): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (dense_4h_to_h): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): GELUActivation()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  )\n",
      "  (embed_out): Linear(in_features=768, out_features=50304, bias=False)\n",
      ")'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|                 Tasks                 |Version|Filter|n-shot|  Metric  |   | Value  |   |Stderr|\n",
      "|---------------------------------------|------:|------|-----:|----------|---|-------:|---|-----:|\n",
      "|arc_challenge                          |      1|none  |     0|acc       |↑  |  0.1997|±  |0.0117|\n",
      "|                                       |       |none  |     0|acc_norm  |↑  |  0.2398|±  |0.0125|\n",
      "|hellaswag                              |      1|none  |     0|acc       |↑  |  0.2903|±  |0.0045|\n",
      "|                                       |       |none  |     0|acc_norm  |↑  |  0.3136|±  |0.0046|\n",
      "|lambada_openai                         |      1|none  |     0|acc       |↑  |  0.3689|±  |0.0067|\n",
      "|                                       |       |none  |     0|perplexity|↓  | 31.2590|±  |1.1594|\n",
      "|lambada_standard                       |      1|none  |     0|acc       |↑  |  0.2333|±  |0.0059|\n",
      "|                                       |       |none  |     0|perplexity|↓  |172.7634|±  |7.7266|\n",
      "|mmlu                                   |      2|none  |      |acc       |↑  |  0.2299|±  |0.0035|\n",
      "| - humanities                          |      2|none  |      |acc       |↑  |  0.2417|±  |0.0062|\n",
      "|  - formal_logic                       |      1|none  |     0|acc       |↑  |  0.2778|±  |0.0401|\n",
      "|  - high_school_european_history       |      1|none  |     0|acc       |↑  |  0.2242|±  |0.0326|\n",
      "|  - high_school_us_history             |      1|none  |     0|acc       |↑  |  0.2500|±  |0.0304|\n",
      "|  - high_school_world_history          |      1|none  |     0|acc       |↑  |  0.2700|±  |0.0289|\n",
      "|  - international_law                  |      1|none  |     0|acc       |↑  |  0.2397|±  |0.0390|\n",
      "|  - jurisprudence                      |      1|none  |     0|acc       |↑  |  0.2593|±  |0.0424|\n",
      "|  - logical_fallacies                  |      1|none  |     0|acc       |↑  |  0.2209|±  |0.0326|\n",
      "|  - moral_disputes                     |      1|none  |     0|acc       |↑  |  0.2486|±  |0.0233|\n",
      "|  - moral_scenarios                    |      1|none  |     0|acc       |↑  |  0.2380|±  |0.0142|\n",
      "|  - philosophy                         |      1|none  |     0|acc       |↑  |  0.1865|±  |0.0221|\n",
      "|  - prehistory                         |      1|none  |     0|acc       |↑  |  0.2130|±  |0.0228|\n",
      "|  - professional_law                   |      1|none  |     0|acc       |↑  |  0.2458|±  |0.0110|\n",
      "|  - world_religions                    |      1|none  |     0|acc       |↑  |  0.3158|±  |0.0357|\n",
      "| - other                               |      2|none  |      |acc       |↑  |  0.2401|±  |0.0076|\n",
      "|  - business_ethics                    |      1|none  |     0|acc       |↑  |  0.3000|±  |0.0461|\n",
      "|  - clinical_knowledge                 |      1|none  |     0|acc       |↑  |  0.2151|±  |0.0253|\n",
      "|  - college_medicine                   |      1|none  |     0|acc       |↑  |  0.2081|±  |0.0310|\n",
      "|  - global_facts                       |      1|none  |     0|acc       |↑  |  0.1800|±  |0.0386|\n",
      "|  - human_aging                        |      1|none  |     0|acc       |↑  |  0.3094|±  |0.0310|\n",
      "|  - management                         |      1|none  |     0|acc       |↑  |  0.1748|±  |0.0376|\n",
      "|  - marketing                          |      1|none  |     0|acc       |↑  |  0.2906|±  |0.0297|\n",
      "|  - medical_genetics                   |      1|none  |     0|acc       |↑  |  0.3000|±  |0.0461|\n",
      "|  - miscellaneous                      |      1|none  |     0|acc       |↑  |  0.2375|±  |0.0152|\n",
      "|  - nutrition                          |      1|none  |     0|acc       |↑  |  0.2190|±  |0.0237|\n",
      "|  - professional_accounting            |      1|none  |     0|acc       |↑  |  0.2340|±  |0.0253|\n",
      "|  - professional_medicine              |      1|none  |     0|acc       |↑  |  0.1985|±  |0.0242|\n",
      "|  - virology                           |      1|none  |     0|acc       |↑  |  0.2831|±  |0.0351|\n",
      "| - social sciences                     |      2|none  |      |acc       |↑  |  0.2187|±  |0.0074|\n",
      "|  - econometrics                       |      1|none  |     0|acc       |↑  |  0.2368|±  |0.0400|\n",
      "|  - high_school_geography              |      1|none  |     0|acc       |↑  |  0.1869|±  |0.0278|\n",
      "|  - high_school_government_and_politics|      1|none  |     0|acc       |↑  |  0.1969|±  |0.0287|\n",
      "|  - high_school_macroeconomics         |      1|none  |     0|acc       |↑  |  0.2051|±  |0.0205|\n",
      "|  - high_school_microeconomics         |      1|none  |     0|acc       |↑  |  0.2101|±  |0.0265|\n",
      "|  - high_school_psychology             |      1|none  |     0|acc       |↑  |  0.1927|±  |0.0169|\n",
      "|  - human_sexuality                    |      1|none  |     0|acc       |↑  |  0.2595|±  |0.0384|\n",
      "|  - professional_psychology            |      1|none  |     0|acc       |↑  |  0.2516|±  |0.0176|\n",
      "|  - public_relations                   |      1|none  |     0|acc       |↑  |  0.2182|±  |0.0396|\n",
      "|  - security_studies                   |      1|none  |     0|acc       |↑  |  0.1878|±  |0.0250|\n",
      "|  - sociology                          |      1|none  |     0|acc       |↑  |  0.2488|±  |0.0306|\n",
      "|  - us_foreign_policy                  |      1|none  |     0|acc       |↑  |  0.2800|±  |0.0451|\n",
      "| - stem                                |      2|none  |      |acc       |↑  |  0.2131|±  |0.0073|\n",
      "|  - abstract_algebra                   |      1|none  |     0|acc       |↑  |  0.2200|±  |0.0416|\n",
      "|  - anatomy                            |      1|none  |     0|acc       |↑  |  0.1926|±  |0.0341|\n",
      "|  - astronomy                          |      1|none  |     0|acc       |↑  |  0.1776|±  |0.0311|\n",
      "|  - college_biology                    |      1|none  |     0|acc       |↑  |  0.2639|±  |0.0369|\n",
      "|  - college_chemistry                  |      1|none  |     0|acc       |↑  |  0.1900|±  |0.0394|\n",
      "|  - college_computer_science           |      1|none  |     0|acc       |↑  |  0.2600|±  |0.0441|\n",
      "|  - college_mathematics                |      1|none  |     0|acc       |↑  |  0.2100|±  |0.0409|\n",
      "|  - college_physics                    |      1|none  |     0|acc       |↑  |  0.2157|±  |0.0409|\n",
      "|  - computer_security                  |      1|none  |     0|acc       |↑  |  0.2800|±  |0.0451|\n",
      "|  - conceptual_physics                 |      1|none  |     0|acc       |↑  |  0.2596|±  |0.0287|\n",
      "|  - electrical_engineering             |      1|none  |     0|acc       |↑  |  0.2345|±  |0.0353|\n",
      "|  - elementary_mathematics             |      1|none  |     0|acc       |↑  |  0.2090|±  |0.0209|\n",
      "|  - high_school_biology                |      1|none  |     0|acc       |↑  |  0.1742|±  |0.0216|\n",
      "|  - high_school_chemistry              |      1|none  |     0|acc       |↑  |  0.1724|±  |0.0266|\n",
      "|  - high_school_computer_science       |      1|none  |     0|acc       |↑  |  0.2600|±  |0.0441|\n",
      "|  - high_school_mathematics            |      1|none  |     0|acc       |↑  |  0.2074|±  |0.0247|\n",
      "|  - high_school_physics                |      1|none  |     0|acc       |↑  |  0.1987|±  |0.0326|\n",
      "|  - high_school_statistics             |      1|none  |     0|acc       |↑  |  0.1528|±  |0.0245|\n",
      "|  - machine_learning                   |      1|none  |     0|acc       |↑  |  0.3125|±  |0.0440|\n",
      "|winogrande                             |      1|none  |     0|acc       |↑  |  0.4964|±  |0.0141|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Evaluation - Pythia 160M Trained on Pile\n",
    "from lm_eval import utils, simple_evaluate\n",
    "from lm_eval.models.huggingface import HFLM\n",
    "\n",
    "# Showcase uses Pythia: https://colab.research.google.com/github/EleutherAI/lm-evaluation-harness/blob/main/examples/lm-eval-overview.ipynb\n",
    "# Only genuine doc seems to be (a mess): https://github.com/EleutherAI/lm-evaluation-harness/tree/main/docs\n",
    "\n",
    "# Load model and tokenizer\n",
    "pythia_pile = AutoModelForCausalLM.from_pretrained(base_path / \"pythia160m_dedup_pile\", local_files_only=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_path / \"pythia160m_dedup_untrained\", use_fast=True, local_files_only=True)\n",
    "pythia_pile = pythia_pile.to(device)\n",
    "\n",
    "# From the HuggingFace Data Views (allenai/ai2_arc, cais/mmlu, allenai/winogrande, Rowan/hellaswag):\n",
    "# MMLU: {'answer': 1, 'choices': ['0', '4', '2', '6'], 'question': 'Find the degree for the given field extension Q(sqrt(2), sqrt(3), sqrt(18)) over Q.', 'subject': 'abstract_algebra'}\n",
    "# ARC-C: {'id': 'Mercury_7175875', 'question': 'An astronomer observes that a planet rotates faster after a meteorite impact. Which is the most likely effect of this increase in rotation?', 'choices': {'text': ['Planetary density will decrease.', 'Planetary years will become longer.', 'Planetary days will become shorter.', 'Planetary gravity will become stronger.'], 'label': ['A', 'B', 'C', 'D']}, 'answerKey': 'C'}\n",
    "# Winogrande: \n",
    "# HellaSwag: \n",
    "# Lambada:\n",
    "\n",
    "batch_size_hflm = 1\n",
    "\n",
    "# Found that in here of all things: https://github.com/pytorch/ao/blob/e2301e9dba91fa962d673fdc3b3f0002856a3ba7/torchao/_models/_eval.py#L17-L22\n",
    "# https://github.com/EleutherAI/lm-evaluation-harness/blob/main/lm_eval/models/huggingface.py\n",
    "pythia_pile_hflm = HFLM(pretrained=pythia_pile,\n",
    "                        tokenizer=tokenizer,\n",
    "                        batch_size=batch_size_hflm)\n",
    "\n",
    "# Thankfully both MMLU and ARC are available in the lm_eval.tasks module\n",
    "# https://github.com/EleutherAI/lm-evaluation-harness/blob/main/lm_eval/tasks/arc\n",
    "# https://github.com/EleutherAI/lm-evaluation-harness/tree/main/lm_eval/tasks/mmlu\n",
    "# Initially evaluator.evaluate looked promising, but I don't understand it and this works\n",
    "# Found simple_evaluate in https://github.com/EleutherAI/lm-evaluation-harness/blob/main/docs/interface.md\n",
    "results = simple_evaluate(model=pythia_pile_hflm,\n",
    "                          tasks=[\"arc_challenge\", \"mmlu\", \"winogrande\", \"hellaswag\", \"lambada\"], # I have no idea how to inject pre-downloaded datasets here, I gave up on that\n",
    "                          num_fewshot=0,  # Pythia paper stated they used zero-shot\n",
    "                          batch_size=batch_size_hflm,\n",
    "                          device=\"cuda\",\n",
    "                          limit=None)\n",
    "\n",
    "# Save for reference (for proof of table below)\n",
    "with open('02_eval_160M_pretrained.txt', 'w') as f:\n",
    "    f.write(str(results))\n",
    "\n",
    "# Manually saved this to 02_eval_160M_pretrained_table.txt\n",
    "# This was nicely documented. Not. https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/eleuther_eval.py\n",
    "print(utils.make_table(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pro of this is that the processing is standardized and I know this works with Pythia models because the doc uses Pythia as the de-facto use case example\n",
    "    - However, with this implementation I have no idea what's actually going on under the hood\n",
    "    - Doesn't feel right, I'll read up on this and write more about it here. But the pipeline works\n",
    "- How do we know this benchmarking pipeline actually benchmarks correctly?\n",
    "    - Utilize the reported Pythia Benchmarks from the paper and compare\n",
    "\n",
    "![](./img/pythia_paper_dedup_benchmarks.png)\n",
    "\n",
    "I initially reported to benchmark only on ARC-Challenge and MMLU.<br>\n",
    "After reporting this, I quickly concluded that I wanted to use more benchmarks, specifically some of the ones mentioned in the Pythia paper to cross reference whether the evaluation pipeline itself is correct. Also, if that were to be the case, we would have a more information-rich benchmarking report. Who wouldn't want that?\n",
    "\n",
    "The overlapping confidence intervals suggest that the performances on WinoGrande and ARC-Challenge are consistent with the reported results.<br>\n",
    "This is narrowly not the case for Lambada (OpenAI).<br>\n",
    "Note that the benchmarking approach used here also reports larger `stderrs`.<br>\n",
    "I suppose the differences come from different versions of the LM-Eval harness (I further assume EleutherAI used their own LM Eval harness to test their models, still, speculative) being used and thus processing differences. I can't deem this to be a source of error in any way though.<br>\n",
    "I interpret the deviation of results for Lambada (OpenAI) as a result of evaluation harness variance too, given the otherwise very consistent results.\n",
    "\n",
    "### ARC-Challenge\n",
    "\n",
    "### MMLU\n",
    "\n",
    "### HellaSwag\n",
    "\n",
    "### WinoGrande\n",
    "\n",
    "### Lambada (OpenAI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
