{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking Pythia $160\\text{M}$ pre-trained on The Pile vs. Pythia $160\\text{M}$ trained on MiniPile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objectives:\n",
    "- [x] Prepare (Download) two models - **Pythia $160M$ Untrained** and **Pythia $160M$ fully Pile-trained**\n",
    "- [x] Load MiniPile Dataset from disk\n",
    "- [ ] Train **Pythia $160M$ Untrained** on MiniPile (according to the MiniPile paper) *and save the model*\n",
    "- [ ] Evaluate the performance of **Pythia $160M$ Pile-trained** and **Pythia $160M$ Untrained** on MMLU and ARC benchmarks (decoder-only-applicable benchmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers datasets torch accelerate evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import transformers\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import snapshot_download\n",
    "from transformers import AutoModelForSequenceClassification, pipeline, EvalPrediction\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/mnt/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Download Pythia $160\\text{M}$ Untrained and Pythia $160\\text{M}$ Pile-Trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_model(down_dir: str, target_folder: str, cache_folder: str, repo_id: str, branch: str = \"main\") -> None:\n",
    "    down_dir = Path(down_dir)\n",
    "    target_dir = down_dir / target_folder\n",
    "    cache_dir = down_dir / cache_folder\n",
    "\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"Downloading {repo_id}/{branch}...\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            snapshot_download(\n",
    "                repo_id,\n",
    "                repo_type=\"model\",\n",
    "                revision=branch,\n",
    "                cache_dir=str(cache_dir),\n",
    "                local_dir=str(target_dir)\n",
    "            )\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Download attempt failed: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading EleutherAI/pythia-160m-deduped/step0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dabdbef7ca34c1abf506ca34b793a34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "download_model(down_dir=base_dir, target_folder=\"pythia160m_dedup_untrained\", \n",
    "               cache_folder=\"pythia160m_dedup_untrained_Cache\",\n",
    "               repo_id=\"EleutherAI/pythia-160m-deduped\", branch=\"step0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading EleutherAI/pythia-160m-deduped/main...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f69c415025ee4a1c91070cafd30839de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://huggingface.co/EleutherAI/pythia-160m/blob/main/README.md states:\n",
    "# \"[...] final step 143000 corresponds exactly to the model checkpoint on the main branch of each model.\"\n",
    "download_model(down_dir=base_dir, target_folder=\"pythia160m_dedup_pile\", \n",
    "               cache_folder=\"pythia160m_dedup_pile_Cache\",\n",
    "               repo_id=\"EleutherAI/pythia-160m-deduped\", branch=\"main\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Load MiniPile Dataset from Disk\n",
    "\n",
    "We expect the MiniPile dataset to already have been downloaded to disk at an earlier point.<br>\n",
    "The logic for this can be found in the `01_get_piles` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading minipile from the local directory \n",
    "# https://stackoverflow.com/questions/77020278/how-to-load-a-huggingface-dataset-from-local-path\n",
    "# https://github.com/MK2112/mobileYOLOv3/blob/main/mobileyolov3-cocotext.ipynb\n",
    "# Split is named exactly like with the original dataset https://huggingface.co/datasets/JeanKaddour/minipile\n",
    "\n",
    "base_path = Path(base_dir)\n",
    "\n",
    "print('Loading MiniPile train + val datasets...')\n",
    "minipile_train = load_dataset(\"parquet\",\n",
    "                              data_files={\n",
    "                                  \"train\": str(base_path / \"MiniPile\" / \"data\" / \"train-*.parquet\"),\n",
    "                                  \"validation\": str(base_path / \"MiniPile\" / \"data\" / \"validation-*.parquet\"),\n",
    "                                  \"test\": str(base_path / \"MiniPile\" / \"data\" / \"test-*.parquet\")\n",
    "                              },\n",
    "                              cache_dir=str(base_path / \"MiniPile_Cache\"),\n",
    "                              split=\"train\")\n",
    "\n",
    "minipile_val = load_dataset(\"parquet\",\n",
    "                            data_files={\n",
    "                                \"train\": str(base_path / \"MiniPile\" / \"data\" / \"train-*.parquet\"),\n",
    "                                \"validation\": str(base_path / \"MiniPile\" / \"data\" / \"validation-*.parquet\"),\n",
    "                                \"test\": str(base_path / \"MiniPile\" / \"data\" / \"test-*.parquet\")\n",
    "                            },\n",
    "                            cache_dir=str(base_path / \"MiniPile_Cache\"),\n",
    "                            split=\"validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Hyperparameters for Pythia $160\\text{M}$ Untrained on MiniPile\n",
    "\n",
    "**Training Parameters for $160M$ for The Pile Deduplicated (*not* MiniPile)**<br>\n",
    "See [Pythia Paper](https://arxiv.org/abs/2304.01373) (p. 22) and [Pythia GitHub](https://github.com/EleutherAI/pythia/blob/main/models/160M/pythia-160m-deduped.yml).\n",
    "- Each model gets exposed to $299,892,736,000 \\approx 300B$ tokens through training ($\\approx 1.5$ epochs on The Pile)\n",
    "- Batch size of $1024$ samples\n",
    "- Sequence length of $2048$\n",
    "- Adam optimizer with $\\beta_1 = 0.9$, $\\beta_2 = 0.95$, $\\epsilon = 1 \\times 10^{-8}$\n",
    "- Learning rates vary by model size:\n",
    "    - $70M$ model:  $10.0 \\times 10^{-4}$\n",
    "    - $160M$ model: $6.0 \\times 10^{-4}$\n",
    "    - $410M$ model: $3.0 \\times 10^{-4}$\n",
    "    - $1.0B$ model: $3.0 \\times 10^{-4}$\n",
    "    - $1.4B$ model: $2.0 \\times 10^{-4}$\n",
    "    - $2.8B$ model: $1.6 \\times 10^{-4}$\n",
    "    - $6.9B$ model: $1.2 \\times 10^{-4}$\n",
    "    - $12B$ model:  $1.2 \\times 10^{-4}$\n",
    "- train-iters $143000$\n",
    "- lr-decay-iters $143000$\n",
    "- lr-decay-style $\\text{cosine}$\n",
    "- lr-warmup $0.01$\n",
    "- weight-decay $0.01$\n",
    "- gradient-clipping $1.0$\n",
    "- lr-min $0.1 \\times \\text{optimizer.params.lr}$ (which isn't in the paper)\n",
    "- synchronize-each-layer $\\text{True}$ (i.e. gradients across all GPUs after each layer synced)\n",
    "- LR Scheduling: Decays to a minimum of $0.1\\times$ the maximum learning rate for all models\n",
    "- (Tokenizer is loaded as the same as for GPT-NeoX-20B)\n",
    "\n",
    "**Training Parameters for $160M$ for MiniPile**<br>\n",
    "See [MiniPile paper](https://arxiv.org/abs/2304.08442)\n",
    "- $1M/500/10k$ training/validation/test examples\n",
    "    - Vocab size: $32309614$\n",
    "    - Median document length: $294$\n",
    "    - Longest document length: $929633$\n",
    "\n",
    "**BERT Training Parameters for MiniPile**\n",
    "- Adam, $\\beta_1 = 0.9$, $\\beta_2 = 0.98$, $\\epsilon = 1 \\times 10^{-12}$\n",
    "- weight-decay $0.001$\n",
    "- One cycle policy with peak learning rate of $1 \\times 10^{-3}$\n",
    "- gradient-clipping $0.5$\n",
    "- Progressive batch size from $128$ to $4096$ with a linear increase over the course of training up to $300k$ steps, no warmup\n",
    "- $800k$ total training steps\n",
    "- weight averaging of the $k = 5$ latest checkpoints and $1k$ steps distance between them\n",
    "\n",
    "**T5 Training Parameters for MiniPile**\n",
    "- AdamW, matrix-wise LR scaling by its root mean square (RMS), no weight decay\n",
    "- base learning rate $0.02$\n",
    "- cosine schedule with final of $1 \\times 10^{-5}$\n",
    "- gradient-clipping $1.0$\n",
    "- batch size $288$\n",
    "- $10k$ warmup steps, $65536$ total training steps\n",
    "- weight averaging of the $k = 5$ latest checkpoints and $1k$ steps distance between them (akin to BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These training parameters are a good start, but they can only be interpreted as at most guiding, because they were applied for decoder-only and encoder-decoder models, yet not for pure decoder-only models like Pythia. Thus, if possible, one should look for approaches trained solely on MiniPile following the decoder-only paradigm for a more accurate guide to our own approach with Pythia. \n",
    "\n",
    "Luckily there exists a [GPT NeoX 122M MiniPile](https://huggingface.co/euclaise/gpt-neox-122m-minipile-digits) model that can be reverse-engineered for our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training arguments from the minipile-trained decode reference model GPT-NeoX-122M:\n",
    "# https://huggingface.co/euclaise/gpt-neox-122m-minipile-digits\n",
    "\n",
    "# Newer versions fail for missing attributes, 4.30.0 is documented to have been used\n",
    "if str(transformers.__version__) == \"4.30.0\":\n",
    "    training_args = torch.load(base_path / 'training_args_gptNEO122m.bin', weights_only=False)\n",
    "    output_file = 'train_args_gptNEO122m_minipile.txt'\n",
    "    try: \n",
    "        with open(output_file, 'w') as f:\n",
    "            f.write(\"TrainingArguments attributes:\\n\")\n",
    "            for attr in dir(training_args):\n",
    "                if hasattr(training_args, attr) and not attr.startswith('_'):\n",
    "                    value = getattr(training_args, attr)\n",
    "                    f.write(f\"- {attr}: {value}\\n\")\n",
    "    except NameError as _:\n",
    "        pass # Fully ignore NameError, appears every time\n",
    "else:\n",
    "    print('Skipped for version mismatch.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GPTNeoX model card is a bit misleading, as it is stated that this model was trained exclusively on MiniPile. The tiny learning rate $5 \\times 10^{-6}$ with no weight decay implies a fine-tuning approach.\n",
    "\n",
    "I did this mostly to get a feeling for much the encoder-based model params deviate from the decoder-based model params.<br>\n",
    "I interpret the results as not too far off, e.g. we use the exact same learning rate and optimizer.<br>\n",
    "\n",
    "This implies that the training params on The Pile for Pythia $160M$ are a good starting point and we can scale these to accommodate the MiniPile dataset size and expect appropriate training effects.\n",
    "\n",
    "Core parameters are however not directly transferable: `train-iters` and therefore also `lr-decay-iters`.<br>\n",
    "For Pile deduplicated this was $143000$, but we have to scale this to the MiniPile dataset size, as the number of tokens processed by the model is crucial for the training process and could lead to overfitting and not accurately reflecting dataset knowledge retention capabilities if not adjusted properly.\n",
    "\n",
    "In other words, overshooting distorts dataset knowledge, while undershooting leads to underfitting and insufficient representation of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I use the byte sizes as proxy for the number of tokens, as both datasets will get tokenized with the same tokenizer\n",
    "minipile_train_bytes = 5906108510 # see https://huggingface.co/datasets/JeanKaddour/minipile/blob/main/README.md\n",
    "pile_train_bytes = 824546807506   # see https://huggingface.co/datasets/EleutherAI/the_pile_deduplicated/blob/main/dataset_infos.json\n",
    "pile_effective_epochs = 1.5       # this many epochs are actually trained in the original model (calculation isn't affected, training params below are)\n",
    "\n",
    "scale_factor = (pile_train_bytes * pile_effective_epochs) / (minipile_train_bytes * pile_effective_epochs)\n",
    "print(f\"Byte-based scale factor: {scale_factor:10.6f}x\")\n",
    "print(f\"MiniPile (scaled) Train-Iters/LR-Decay-Iters: {143000 / scale_factor:.3f} ~ {round(143000 / scale_factor)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point the $1024$ for training iterations may seem awkwardly small.<br>\n",
    "But, to reiterate, we strictly scaled it down iterations according to dataset size difference.\n",
    "\n",
    "While this may seem horrible in most other cases, as we thoroughly neuter exposure to data, this scale-correct limiting and overall lower exposure is exactly what we need here to operate relative to the original Pythia training. After all, the goal is to compare knowledge retention and generalization capabilities achievable on `The Pile Deduplicated` vs. the 'distilled' `MiniPile` under size-appropriate, similar conditions. Therefore, scaling the `train-iters` and therefore also `lr-decay-iters` using byte sizes as a proxy is actually appropriate here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now lay out the complete parameters:<br>\n",
    "With the three approach descriptions retrieved, we can take a more educated guess at the training params for Pythia $160M$ on MiniPile:\n",
    "\n",
    "- Adam optimizer (GPT NeoX and T5-Base MiniPile suggest the 'generally more stable' AdamW, but Pythia uses Adam so we keep it most similar)\n",
    "    - $\\beta_1 = 0.9$, $\\beta_2 = 0.95$, (Pythia)\n",
    "    - $\\epsilon = 1 \\times 10^{-8}$ (GPT NeoX and Pythia)\n",
    "    - learning rate $6 \\times 10^{-4}$ (Pythia)\n",
    "    - lr-schedule $\\text{cosine annealing}$ (Pythia)\n",
    "    - lr-warmup $0.01$ of total steps (Pythia)\n",
    "    - lr-min $0.1 \\times \\text{lr}$ (Pythia)\n",
    "    - weight-decay $1 \\times 10^{-2}$ (Pythia)\n",
    "- gradient-clipping $1.0$ (Pythia)\n",
    "- batch size $1024$ (Pythia, probably grad accum needed, expect multi-GPU)\n",
    "- sequence length $2048$ (Pythia)\n",
    "- **train-iters: $1024$ (MiniPile-specific)**\n",
    "- **lr-decay-iters: same as train-iters (MiniPile-specific)**\n",
    "- (won't do mixed precision for sake of most similar training conditions to Pile-trained Pythia)\n",
    "- (won't do weight averaging)\n",
    "- **Same GPT-NeoX-20B tokenizer as for Pythia-Pile**\n",
    "\n",
    "We can start training Pythia $160\\text{M}$ on MiniPile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Train Pythia $160\\text{M}$ Untrained on MiniPile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the untrained Pythia 160M tokenizer and model\n",
    "# https://stackoverflow.com/questions/64001128/load-a-pre-trained-model-from-disk-with-huggingface-transformers\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_path / \"pythia160m_dedup_untrained\", use_fast=True, local_files_only=True)\n",
    "empty_model = AutoModelForCausalLM.from_pretrained(base_path / \"pythia160m_dedup_untrained\", local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(example): # seq_len = max_length = 2048 (always)\n",
    "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=2048)\n",
    "\n",
    "minipile_train_tokenized = minipile_train.map(tokenize, batched=True)\n",
    "minipile_train_tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"]) # new fields from tokenizing\n",
    "\n",
    "# Not really needed, but we have it, might as well make it serve as a reference for investigation of the model's performance\n",
    "minipile_val_tokenized = minipile_val.map(tokenize, batched=True)\n",
    "minipile_val_tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])  # new fields from tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n",
    "    torch.distributed.init_process_group(\"nccl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = str(base_path / \"pythia160m_minipile_trained\")\n",
    "log_dir = str(base_path / \"160m_minipile_logs\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# https://huggingface.co/docs/transformers/v4.46.0/en/main_classes/trainer#transformers.TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1.5,            # Since train_iters gets set, use num_train_epochs=1.5 like for The Pile\n",
    "    per_device_train_batch_size=8,   # Gives an effective batch size of 1024 after grad accum\n",
    "    per_device_eval_batch_size=8,    # Same as training batch size\n",
    "    gradient_accumulation_steps=128, # Achieve a batch size of 1024\n",
    "    learning_rate=6e-4,              # Default Pythia 160M\n",
    "    weight_decay=0.01,               # Default Pythia 160M\n",
    "    max_steps=1024,                  # Adjusted for MiniPile\n",
    "    lr_scheduler_type=\"cosine\",      # As per Pythia 160M paper\n",
    "    warmup_steps=int(0.01 * 1024),   # 1% of total steps for warmup\n",
    "    logging_dir=log_dir,\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,     # Frequency for evaluation during training\n",
    "    save_steps=1024,    # Save at the end of training\n",
    "    save_total_limit=1, # Only keep the most recent checkpoint\n",
    "    fp16=False,         # Not using mixed precision for comparable conditions\n",
    "    report_to=\"none\",   # Noting this for later iterations, maybe set this as \"wandb\", \"tensorboard\" or smth\n",
    "    ddp_find_unused_parameters=False, # see https://discuss.pytorch.org/t/how-to-change-ddp-parameter-find-unused-parameters-true-to-false-during-training/130763\n",
    ")\n",
    "\n",
    "# Ensure training across multiple GPUs if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "empty_model = empty_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Pythia 160M Untrained on MiniPile\n",
    "# https://huggingface.co/docs/transformers/v4.46.0/en/main_classes/trainer\n",
    "trainer = Trainer(model=empty_model,\n",
    "                  args=training_args,\n",
    "                  train_dataset=minipile_train_tokenized,\n",
    "                  eval_dataset=minipile_val_tokenized)\n",
    "\n",
    "trainer.train()  # TODO: Export this to a script, then run as `torchrun --nproc_per_node=>>NUM GPUs<< <script_something_160m>.py`\n",
    "\n",
    "# Why is this a two-step process?!\n",
    "trainer.save_model(str(base_path / \"pythia160m_minipile_trained\")) # This saves the model weights\n",
    "tokenizer.save_pretrained(str(base_path / \"pythia160m_minipile_trained\")) # This saves the tokenizer (don't know if needed, better save than sorry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Evaluate Pythia $160\\text{M}$ MiniPile vs. Pythia $160\\text{M}$ Pile-Trained on MMLU and ARC Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_dataset(down_dir: str, target_folder: str, cache_folder: str, repo_id: str) -> None:\n",
    "    down_dir = Path(down_dir)\n",
    "    target_dir = down_dir / target_folder\n",
    "    cache_dir = down_dir / cache_folder\n",
    "\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"Downloading {repo_id}...\")\n",
    "\n",
    "    # I tried fiddling with the os.environs, cause I wanted to use the load_dataset function\n",
    "    # but we actually don't need that, snapshot_download suffices fully\n",
    "    while True:\n",
    "        try:\n",
    "            snapshot_download(repo_id, repo_type=\"dataset\", cache_dir=str(cache_dir), local_dir=str(target_dir))\n",
    "            break\n",
    "        except Exception as _:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading cais/mmlu...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc124e069b434216bc798f6e12b1f379",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 181 files:   0%|          | 0/181 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "download_dataset(down_dir=base_dir, target_folder=\"MMLU\", cache_folder=\"MMLU_Cache\",\n",
    "                 repo_id=\"cais/mmlu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading allenai/ai2_arc...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43ab87dc1c874cf2889b8b37cc8aec89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "download_dataset(down_dir=base_dir, target_folder=\"ARC\", cache_folder=\"ARC_Cache\",\n",
    "                 repo_id=\"allenai/ai2_arc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pythia_minipile = AutoModelForCausalLM.from_pretrained(base_path / \"minipile_trained_pythia160m\", local_files_only=True)\n",
    "pythia_pile = AutoModelForCausalLM.from_pretrained(base_path / \"pythia160m_dedup_pile\", local_files_only=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_path / \"pythia160m_dedup_untrained\", use_fast=True, local_files_only=True)\n",
    "\n",
    "pythia_minipile.to(device)\n",
    "pythia_pile.to(device)\n",
    "\n",
    "# References https://github.com/hendrycks/test/blob/master/evaluate.py (TODO: References maybe too strongly. Check this thoroughly.)\n",
    "# {'answer': 1, 'choices': ['0', '4', '2', '6'], 'question': 'Find the degree for the given field extension Q(sqrt(2), sqrt(3), sqrt(18)) over Q.', 'subject': 'abstract_algebra'}\n",
    "def choice_log_probs_mmlu(model, tokenizer, question, options):\n",
    "    log_probs = []\n",
    "    for choice in options:\n",
    "        input_text = f\"{question} {choice}\"\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        # Extract log probability of last token of the choice\n",
    "        logits = outputs.logits[0, -1, :] # Get logits for the last token\n",
    "        log_prob = torch.log_softmax(logits, dim=-1)  # Softmax -> convert logits to probabilities\n",
    "        choice_log_prob = log_prob[tokenizer.convert_tokens_to_ids(choice)].item()  # Get log prob of choice\n",
    "        log_probs.append(choice_log_prob)\n",
    "    best_choice_idx = int(np.argmax(log_probs)) # Choose answer with highest log prob\n",
    "    return best_choice_idx\n",
    "\n",
    "def bench_mmlu(model, tokenizer, dataset):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for example in dataset:\n",
    "        question = example[\"question\"]\n",
    "        options = example[\"choices\"]\n",
    "        correct_answer = example[\"answer\"]\n",
    "        predicted_choice_idx = choice_log_probs_mmlu(model, tokenizer, question, options)\n",
    "        predicted_answer = chr(65 + predicted_choice_idx)\n",
    "        if predicted_answer == str(correct_answer).strip():\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structuring of ARC and MMLU seem fairly associable; try to use a similar log prob approach\n",
    "# {'id': 'Mercury_7175875', 'question': 'An astronomer observes that a planet rotates faster after a meteorite impact. Which is the most likely effect of this increase in rotation?', 'choices': {'text': ['Planetary density will decrease.', 'Planetary years will become longer.', 'Planetary days will become shorter.', 'Planetary gravity will become stronger.'], 'label': ['A', 'B', 'C', 'D']}, 'answerKey': 'C'}\n",
    "def choice_log_probs_arc(model, tokenizer, question, options):\n",
    "    log_probs = []\n",
    "    for choice in options:\n",
    "        input_text = f\"{question} {choice}\"\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        logits = outputs.logits[0, -1, :]\n",
    "        log_prob = torch.log_softmax(logits, dim=-1)\n",
    "        choice_log_prob = log_prob[tokenizer.convert_tokens_to_ids(choice)].item()\n",
    "        log_probs.append(choice_log_prob)\n",
    "    best_choice_idx = int(np.argmax(log_probs)) # Choose answer with highest log prob\n",
    "    return best_choice_idx\n",
    "\n",
    "def bench_arc_challenge(model, tokenizer, dataset):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for example in dataset:\n",
    "        question = example[\"question\"]\n",
    "        options = example[\"choices\"][\"text\"]\n",
    "        correct_answer = example[\"answerKey\"]\n",
    "        \n",
    "        predicted_choice_idx = choice_log_probs_arc(model, tokenizer, question, options)\n",
    "        predicted_answer = chr(65 + predicted_choice_idx)  # Convert index to label ('A', 'B', 'C', 'D')\n",
    "        \n",
    "        if predicted_answer == correct_answer:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    \n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading MMLU test dataset...')\n",
    "mmlu_all = load_dataset(\"parquet\",\n",
    "                        data_files={\n",
    "                            \"train\": str(base_path / \"MMLU\" / \"all\" / \"auxiliary_train-*.parquet\"),\n",
    "                            \"dev\": str(base_path / \"MMLU\" / \"all\" / \"dev-*.parquet\"),\n",
    "                            \"validation\": str(base_path / \"MMLU\" / \"all\" / \"validation-*.parquet\"),\n",
    "                            \"test\": str(base_path / \"MMLU\" / \"all\" / \"test-*.parquet\"),\n",
    "                        },\n",
    "                        cache_dir=str(base_path / \"MMLU_Cache\"),\n",
    "                        split=\"test\")\n",
    "\n",
    "print('Loading ARC test dataset...')\n",
    "arc_challenge = load_dataset(\"parquet\",\n",
    "                            data_files={\n",
    "                                \"train\": str(base_path / \"ARC\" / \"ARC-Challenge\" / \"train-*.parquet\"),\n",
    "                                \"validation\": str(base_path / \"ARC\" / \"ARC-Challenge\" / \"validation-*.parquet\"),\n",
    "                                \"test\": str(base_path / \"ARC\" / \"ARC-Challenge\" / \"test-*.parquet\"),\n",
    "                            },\n",
    "                            cache_dir=str(base_path / \"ARC_Cache\"),\n",
    "                            split=\"test\")\n",
    "\n",
    "# Evaluate accuracy on MMLU and ARC datasets\n",
    "mmlu_accuracy_minipile = bench_mmlu(pythia_minipile, tokenizer, mmlu_all)\n",
    "mmlu_accuracy_pile = bench_mmlu(pythia_pile, tokenizer, mmlu_all)\n",
    "arc_accuracy_minipile = bench_arc_challenge(pythia_minipile, tokenizer, arc_challenge)\n",
    "arc_accuracy_pile = bench_arc_challenge(pythia_pile, tokenizer, arc_challenge)\n",
    "\n",
    "# Save results to JSON\n",
    "with open(str(base_dir / \"benchmark_results.json\"), \"w\") as f:\n",
    "    json.dump({\n",
    "        \"MMLU\": {\n",
    "            \"minipile\": mmlu_accuracy_minipile,\n",
    "            \"pile\": mmlu_accuracy_pile\n",
    "        },\n",
    "        \"ARC\": {\n",
    "            \"minipile\": arc_accuracy_minipile,\n",
    "            \"pile\": arc_accuracy_pile\n",
    "        }\n",
    "    }, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
