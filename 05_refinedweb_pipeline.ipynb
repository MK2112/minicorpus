{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying the MiniPile Pipeline to RefinedWeb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objectives:**\n",
    "\n",
    "All of this is optional as per the original task proposal.<br>\n",
    "But its fun!\n",
    "\n",
    "- [x] Analyze the RefinedWeb dataset\n",
    "- [.] Adapting SuperMiniPile pipeline for RefinedWeb, aiming for creating an equally performant yet smaller dataset (MiniRefinedWeb)\n",
    "- [] Train Pythia $160\\text{M}$ on RefinedWeb and MiniRefinedWeb, evaluate on MMLU and ARC-Challenge\n",
    "- [] Train Pythia $1.4\\text{B}$ with MiniRefinedWeb, evaluate pipeline performance on the MMLU and ARC benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Analyze the RefinedWeb dataset\n",
    "\n",
    "Other than The Pile, RefinedWeb is not as clearly structurally assembled.<br>\n",
    "Where The Pile is made up from combining varying datasubsets, RefinedWeb is a collection of web pages that have been refined by:\n",
    "- discarding irrelevant pages \n",
    "- applying a set of heuristics to remove low-quality content and \n",
    "- applying deduplication techniques to further remove near-duplicate pages\n",
    "\n",
    "RefinedWeb consists of $968,000,015$ rows, amounting to $1.68\\text{TB}$ of data.<br>\n",
    "Each entry consists of the following columns:\n",
    "- `content`: The textually representable content of the page\n",
    "- `url`: The URL of the page\n",
    "- `timestamp`: The timestamp of the page's last update\n",
    "- `dump`: Which dump the page was found in (referring to CommonCrawl dumps)\n",
    "- `segment`: The segment of the dump this page was found in\n",
    "- `image_urls`: URLs of images found on this page\n",
    "\n",
    "For our purposes, we will focus on the `content` column.<br>\n",
    "We have to assume content is as varied as the web, making it hard to cluster or categorize.<br>\n",
    "The key insight from MiniPile is that using semantic embeddings to express document relationships at some cluster resolution.<br>\n",
    "RefinedWeb, with its 968 million web pages, requires a methodical approach to determine an optimal number of clusters. At best, said method should be applicable to other unstructured, maybe even larger datasets.\n",
    "\n",
    "## The $k\\text{atch}$\n",
    "\n",
    "Approaching the issue from first principles, in order to find a reasonable $k$, I propose this staged approach:\n",
    "- Randomly sample $n$ document indices from RefinedWeb, where $n$ is a **representative** fraction (say $1.5\\%$ to $2\\%$) of the dataset\n",
    "- On this subset, perform silhouette analysis for $k$ in a range of $[100, 500]$ (or higher and lower, this is me winging it) in steps of $50$\n",
    "- Plot the silhouette scores and determine the optimal $k$ based on the elbow method\n",
    "- Use this $k$ to cluster the entire (then embedded) dataset\n",
    "- Continue with the MiniPile pipeline from there.\n",
    "\n",
    "## Ditching $k$-Means\n",
    "\n",
    "Taking a step back, we need some form of preprocessing step to find $k$ for MiniPile's k-Means clustering. Why not drop k-Means altogether and use a more flexible clustering algorithm? I have to admit, I got thoroughly scared from my attempt at using HDBSCAN on The Pile earlier on. \n",
    "That attempt would lift the need for $k$, but HDBSCAN can't be batched, thus it will scale horribly.\n",
    "\n",
    "However, ditching the need for a $k$ would allow us to more reasonable generalize MiniPile's pipeline across datasets, where we don't even need to know their content structure beforehand.<br>\n",
    "I consulted literature, looking ideally for a large-scale- and high-dimensional-applicable clustering algorithm.\n",
    "\n",
    "Specifically, I found the following papers (not all of them are related to the problem at hand, but they are interesting nonetheless):\n",
    "- [Using Projection-Based Clustering to Find Distance- and Density-Based Clusters in High-Dimensional Data (Thrun, M. C. & Ultsch, Alfred. 2020)](https://link.springer.com/article/10.1007/s00357-020-09373-2)\n",
    "- [An Efficient Density-based Clustering Algorithm for Higher-Dimensional Data (Boonchoo, et al. 2018)](https://arxiv.org/pdf/1801.06965)\n",
    "- [Swarm Intelligence for Self-Organized Clustering (Thrun, M. C. & Ultsch, Afred. 2021)](https://arxiv.org/abs/2106.05521)\n",
    "- [DPM: Fast and scalable clustering algorithm for large scale high dimensional datasets (Ghanem, et al. 2014)](https://ieeexplore.ieee.org/document/7050427)\n",
    "- [K-DBSCAN: Identifying Spatial Clusters with Differing Density Levels](https://ieeexplore.ieee.org/document/7544972/)\n",
    "- [FINDIT: a fast and intelligent subspace clustering algorithm using dimension voting (Woo, et al. 2004)](https://www.sciencedirect.com/science/article/abs/pii/S0950584903001411?via%3Dihub)\n",
    "- [TeraHAC: Hierarchical Agglomerative Clustering of Trillion-Edge Graphs (Dhulipala, et al. 2023)](https://arxiv.org/abs/2308.03578)\n",
    "- [A simple rapid sample-based clustering for large-scale data (Chen, et al. 2024)](https://www.sciencedirect.com/science/article/abs/pii/S0952197624007097)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $k$-less Clustering with PCA'd BIRCH\n",
    "\n",
    "An algorithm that by design could work with and cluster larger datasets without the need for a $k$ would be the Balanced Iterative Reducing and Clustering using Hierarchies (BIRCH) algorithm.\n",
    "\n",
    "Papers related to this approach:\n",
    "- [Efficient Clustering of High-Dimensional Data Sets with Application to Reference Matching (McCallum, et al. 2000)](https://pubs.dbs.uni-leipzig.de/dc/files/McCallum2000Efficientclusteringofhighdimensionaldatasetswith.pdf)\n",
    "- [Improve BIRCH algorithm for big data clustering (Ramadhani, et al. 2020)](https://iopscience.iop.org/article/10.1088/1757-899X/725/1/012090)\n",
    "- [Using Projection-Based Clustering to Find Distance- and Density-Based Clusters in High-Dimensional Data (Thrun, M. C. & Ultsch, Alfred. 2020)](https://link.springer.com/article/10.1007/s00357-020-09373-2)\n",
    "- [Surpassing Cosine Similarity for Multidimensional Comparisons: Dimension Insensitive Euclidean Metric (DIEM) (Tessari, et al. 2024)](https://arxiv.org/abs/2407.08623)\n",
    "\n",
    "We would ditch $k$-Means to instead, incrementally, build the Clustering Feature Tree, which then merges subclusters hierarchically based on density thresholds.<br>\n",
    "Except for this tree structure being constantly in memory, we could now also feed the data into BIRCH in batched fashion.\n",
    "\n",
    "To further accelerate the clustering process and compact both the 768-dimensional embeddings and thus the tree during clustering, we could apply Principal Component Analysis (PCA) before forwarding embeddings into BIRCH, effectively applying the findings of [(Ramadhani, et al. 2020)](https://iopscience.iop.org/article/10.1088/1757-899X/725/1/012090) and [(Thrun, M. C. & Ultsch, Alfred. 2020)](https://link.springer.com/article/10.1007/s00357-020-09373-2).\n",
    "\n",
    "In this context, an issue that I wondered about with the original MiniPile approach was the employing of cosine similarity as measure for differentiation for $768$-dimensional embeddings during the $k$-Means clustering. The higher the dimensionality, the less expressiveness/nuance we can attain from metrics like cosine distance, as also discussed by [(Tessari, et al. 2024)](https://arxiv.org/abs/2407.08623). Therefore, applying PCA could help increase the effectiveness of cosine similarity.<br>\n",
    "Furthermore, a lowered dimensionality for clustering could help prevent excessive splits in the CF Tree caused by (potentially more often in high dimensionality occuring) sparsities in data, leading to a more compacted tree and less memory usage.<br>\n",
    "This would make the approach more scalable.\n",
    "\n",
    "Structurally similar to how we approached $k$-Means, the steps for the pipeline would then be, processing the embedded shards incrementally:\n",
    "- Cluster-Tree Emergence:\n",
    "    - Apply PCA to the embeddings, reducing the dimensionality to $n$,\n",
    "    - Feed the PCA-transformed embeddings into BIRCH, make that tree grow with checkpoints every $m$ shards (that is, save a checkpoint of the tree),\n",
    "    - (We could again read an ominous `End_Here` flag set by the embedding step, to stop clustering (we could run off-set with embedding again))\n",
    "- Clustering:\n",
    "    - Assign all embeddings to the final tree,\n",
    "    - Merge subclusters based on density thresholds/cosine similarity.\n",
    "\n",
    "For a start, the E5-base-4k embedding step was realized with `05_embed_refinedweb_turbo.py`.<br>\n",
    "I also experimented with a more distributed (across machines) approach to the embedding step, which would subsection the dataset into shard spaces covered per each running instance, and I used argparse for that in `05_embed_refinedweb_sectioned.py`, but I didn't employ that in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Right here is where I had to cut it.<br>\n",
    "Active embedding efforts on RefinedWeb began around christmas 2024 with as much resources I could muster, but that didn't suffice to get the job done in time.<br>\n",
    "I also diverted larger, temporarily available resource pockets to the investigation of ablation studies on MiniPile, specifically the tiny, nano and pico versions.<br>\n",
    "Still, I believe the theoretical groundwork layed out for MiniRefinedWeb could as-is be picked up to be realized later."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minipile",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
