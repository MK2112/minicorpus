{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Step Counts\n",
    "\n",
    "For chapter 4, we not only want to find a more effective way to build a MiniPile, but we also want to downsize MiniPile further. The former produces datasets of roughly the same example count, and dimensionality, but the latter strictly doesn't.\n",
    "\n",
    "For downsizing MiniPile and evaluating these downsized datasets correctly, we need to adjust the step count for the training. We will use the same calculations as performed in chapter 2 to find the optimal step count for the training of the MiniPile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/vol/tmp/koppelmm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncompressed byte size of the Parquet dataset: 4774049161 bytes\n",
      "Byte-based scale factor: 172.714352x\n",
      "MiniPile (scaled) Train-Iters/LR-Decay-Iters: 827.957 ~ 828\n"
     ]
    }
   ],
   "source": [
    "def get_uncomp_dataset_size(dataset_path):\n",
    "    size_bytes = 0\n",
    "    for root, _, files in os.walk(dataset_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.parquet'):\n",
    "                # Only evaluate what's really part of the dataset\n",
    "                pq_table = pq.read_table(os.path.join(root, file))\n",
    "                size_bytes += sum(col.nbytes for col in pq_table.columns)\n",
    "    return size_bytes\n",
    "\n",
    "dataset_path = Path(base_dir) / 'MiniPile_DensityNano'\n",
    "uncompressed_size = get_uncomp_dataset_size(dataset_path)\n",
    "print(f\"Uncompressed byte size of the Parquet dataset: {uncompressed_size} bytes\")\n",
    "\n",
    "# I use the byte sizes as proxy for the number of tokens, as both datasets will get tokenized with the same tokenizer\n",
    "minipile_train_bytes = uncompressed_size\n",
    "pile_train_bytes = 824546807506   # see https://huggingface.co/datasets/EleutherAI/the_pile_deduplicated/blob/main/dataset_infos.json\n",
    "pile_effective_epochs = 1.5       # this many epochs are actually trained in the original model (calculation isn't affected, training params below are)\n",
    "\n",
    "scale_factor = (pile_train_bytes * pile_effective_epochs) / (minipile_train_bytes * pile_effective_epochs)\n",
    "print(f\"Byte-based scale factor: {scale_factor:10.6f}x\")\n",
    "print(f\"MiniPile (scaled) Train-Iters/LR-Decay-Iters: {143000 / scale_factor:.3f} ~ {round(143000 / scale_factor)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minipile",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
