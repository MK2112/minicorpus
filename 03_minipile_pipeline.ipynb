{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recreating and Improving MiniPile Dataset Creation\n",
    "\n",
    "**Objectives:**\n",
    "- [.] Implement and verify MiniPileâ€™s filtering pipeline according to [Kaddour (2023)](https://arxiv.org/abs/2304.08442), but intended for decoder-only model use\n",
    "- [] Evaluate and compare performances of Pythia $160\\text{M}$ pretrained on The Pile vs. trained on the *newly, self-created MiniPile* on MMLU and ARC-Challenge\n",
    "- [] Improve the dataset creation process, create new SuperMiniPile dataset (ideally smaller and more information-retaining)\n",
    "- [] Train Pythia $160\\text{M}$ on SuperMiniPile, evaluate on MMLU and ARC-Challenge\n",
    "- [] Evaluate and compare performances of Pythia $1.4\\text{B}$ pretrained on The Pile vs. trained on the created MiniPile on the MMLU and ARC benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset,  Dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from huggingface_hub import snapshot_download\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/mnt/data\"\n",
    "base_path = Path(base_dir)\n",
    "\n",
    "def download_model(down_dir: str, target_folder: str, cache_folder: str, repo_id: str, branch: str = \"main\") -> None:\n",
    "    down_dir = Path(down_dir)\n",
    "    target_dir = down_dir / target_folder\n",
    "    cache_dir = down_dir / cache_folder\n",
    "\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"Downloading {repo_id}/{branch}...\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            snapshot_download(\n",
    "                repo_id,\n",
    "                repo_type=\"model\",\n",
    "                revision=branch,\n",
    "                cache_dir=str(cache_dir),\n",
    "                local_dir=str(target_dir)\n",
    "            )\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Download attempt failed: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Recreating MiniPile Dataset Creation\n",
    "\n",
    "(1) document embedding extraction,<br>\n",
    "(2) clustering of embeddings, and<br>\n",
    "(3) human-guided exclusion of unwanted clusters\n",
    "\n",
    "- 22 datasubset sources\n",
    "- 5.91 KiB mean document size (before deduplication)\n",
    "\n",
    "### Document Embedding Extraction\n",
    "\n",
    "- MiniPile paper uses term \"document\": I assume as they are quite large, this refers to individual training examples from \"The Pile-Deduplicated\"\n",
    "- \"The Pile Deduplicated\" predominantly contains english text, as stated in the Pile paper\n",
    "- `E5-Large` does not require performing sentence-splitting beforehand, I was misguided by the example code at https://huggingface.co/intfloat/e5-large\n",
    "- I will use `E5-Large` with one \"sentence\" actually being one \"document\" for MiniPile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading intfloat/e5-large/main...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a37527833424ed78cd5986f4bd966a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "879f57deb8664c27bfb4ccfa766a11fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1650 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Starting point is the deduplicated The Pile\n",
    "# Infer embeddings for all documents using E5-Large\n",
    "\n",
    "# https://huggingface.co/intfloat/e5-large\n",
    "download_model(down_dir=base_dir, target_folder=\"e5-large\", \n",
    "               cache_folder=\"e5-large_Cache\",\n",
    "               repo_id=\"intfloat/e5-large\") # Chose this because nothing beyond E5-Large was specified\n",
    "\n",
    "e5_large = SentenceTransformer(str(base_path / \"e5-large\"), local_files_only=True) # no .from_pretrained() here\n",
    "\n",
    "# https://huggingface.co/datasets/EleutherAI/the_pile_deduplicated\n",
    "pile_dedup = load_dataset(\"parquet\",\n",
    "                          data_files={\n",
    "                              \"train\": str(base_path / \"Pile_Deduplicated\" / \"data\" / \"train-*.parquet\"),\n",
    "                          },\n",
    "                          cache_dir=str(base_path / \"MiniPile_Cache\"),\n",
    "                          split=\"train\",\n",
    "                          streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the model and the local-stream for The Pile, we iterate through the dataset and extract the embeddings for each document.<br>\n",
    "For convenience and later processing, we will assemble an embedding dataset.\n",
    "\n",
    "Thing is, when creating the embedding dataset, we need to make sure that embedding indices match the document indices in the original dataset.<br>\n",
    "This is strictly necessary for the filtering step later on.<br>\n",
    "To ensure the above code's resulting embedding dataset is correctly aligned with the original dataset, I ran the following code for a small subset of `16384` documents:\n",
    "\n",
    "```python\n",
    "saved_embeddings = load_dataset(\"parquet\", data_files=str(embd_dir / \"shard_*.parquet\"), split=\"train\")\n",
    "\n",
    "for index in tqdm(range(len(saved_embeddings)), desc=\"Verifying embeddings\"):\n",
    "    # Newly embed the text at this index\n",
    "    original_text = next(iter(pile_dedup.skip(index).take(1)))['text']\n",
    "    generated_embedding = e5_large.encode(original_text, show_progress_bar=False)\n",
    "    # Newly encoded embedding should correspond to the saved embedding at this index -> index consistency\n",
    "    saved_embedding = saved_embeddings[index]['embedding']\n",
    "    if not np.allclose(generated_embedding, saved_embedding, atol=1e-5):\n",
    "        print(f\"Mismatch found at index: {index}\")\n",
    "```\n",
    "\n",
    "No mismatches were found, which means we can scale the embedding set creation to the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c61491d2895e47a7ab50237b275087ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:59, 59.45s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecb2ece496b64011b4f39014f77736d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [01:53, 56.05s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "888884d19fa54764aaa451ac34a2d9ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [02:51, 56.92s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96a0e75947394e079b147abfaed8b24d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d311332b5ad49cd8ef6092a0f970566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [03:54, 59.35s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd52f46fc3a0431b878152562db946cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [04:51, 58.46s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8c1e1985ef840d98124a36116cf44b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [05:46, 57.59s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c839650bd6ba450cb0b3aaee1e1e473e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [06:45, 57.86s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02669168cc794bcb8c87ee419b554864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70a9827b12d44221ba0308ab52bc6013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [07:42, 57.65s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "243d8f6ac67a4bb2bee4ad323a19c803",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [08:39, 57.47s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "375ae38785184f40947ee8b79e612efc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [09:36, 57.35s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cbb5b615b424c6f9b78421153a9a853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [10:31, 56.56s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbca84ca64f542a0a9a885fb0026853a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb325703e3304c5fb4332666ca723423",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [11:28, 56.84s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "611112d4fa544cab9a74ed795bfd3484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [12:26, 57.01s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f12b91aa96b44371bd1d490705a6b9af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [13:21, 56.57s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2f31e1b305b4f58830527fe37cc5b7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [14:18, 56.64s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae1574375a5f421cb8212904bced1f14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38831e8cb1484c54b44cb5f309b24643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [15:16, 61.12s/it]\n"
     ]
    }
   ],
   "source": [
    "# Took the example code from the intfloat/e5-large page\n",
    "embd_dir = base_path / Path(\"Pile_Deduplicated_Embeddings\")\n",
    "embd_dir.mkdir(exist_ok=True)\n",
    "\n",
    "batch_size = 1024\n",
    "shard_size = batch_size ** 2 # shard embed count upper bound\n",
    "\n",
    "embedding_shard = []\n",
    "shard_index = 0\n",
    "\n",
    "def save_shard(embeddings, output_dir, shard_index):\n",
    "    shard_path = output_dir / f\"shard_{shard_index:08d}.parquet\"\n",
    "    dataset = Dataset.from_dict({\"embedding\": embeddings})\n",
    "    dataset.to_parquet(str(shard_path))\n",
    "\n",
    "# Didn't know tqdm could be used like that\n",
    "for batch_idx, batch in tqdm(enumerate(pile_dedup.iter(batch_size=batch_size))):\n",
    "    batch_embds = e5_large.encode(batch['text'], show_progress_bar=True) # Set this to False, good for debug but clutters like hell\n",
    "    embedding_shard.extend(batch_embds)\n",
    "    \n",
    "    if len(embedding_shard) >= shard_size:\n",
    "        save_shard(embedding_shard, embd_dir, shard_index)\n",
    "        shard_index += 1\n",
    "        embedding_shard = []\n",
    "\n",
    "# Append remaining embeddings\n",
    "if embedding_shard != []:\n",
    "    save_shard(embedding_shard, embd_dir, shard_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minipile",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
