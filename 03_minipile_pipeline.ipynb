{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recreating and Improving MiniPile\n",
    "\n",
    "**Objectives:**\n",
    "- [x] Implement and verify MiniPile’s filtering pipeline according to [Kaddour, Jean (2023)](https://arxiv.org/abs/2304.08442), but intended for decoder-only model use\n",
    "- [x] Evaluate and compare performances of Pythia $160\\text{M}$ Pile Deduplicated vs. trained on the *newly, self-created MiniPile* on MMLU and ARC-Challenge\n",
    "- [x] Evaluate and compare performances of Pythia $1.4\\text{B}$ Pile Deduplicated vs. trained on the *newly, self-created MiniPile* on MMLU and ARC-Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import snapshot_download\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "base_dir = \"/vol/tmp/koppelmm\"\n",
    "base_path = Path(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_model(down_dir: str, target_folder: str, cache_folder: str, repo_id: str, branch: str = \"main\") -> None:\n",
    "    down_dir = Path(down_dir)\n",
    "    target_dir = down_dir / target_folder\n",
    "    cache_dir = down_dir / cache_folder\n",
    "\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"Downloading {repo_id}/{branch}...\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            snapshot_download(\n",
    "                repo_id,\n",
    "                repo_type=\"model\",\n",
    "                revision=branch,\n",
    "                cache_dir=str(cache_dir),\n",
    "                local_dir=str(target_dir)\n",
    "            )\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Download attempt failed: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Recreating The MiniPile Dataset Creation Pipeline\n",
    "\n",
    "(1) document embedding extraction,<br>\n",
    "(2) clustering of embeddings, and<br>\n",
    "(3) human-guided exclusion of unwanted clusters<br>\n",
    "(4) mini-pile distillation\n",
    "\n",
    "- 22 data subset sources\n",
    "- 5.91 KiB mean document size (before deduplication)\n",
    "\n",
    "### Document Embedding Extraction\n",
    "\n",
    "- MiniPile paper uses term \"document\": This refers to individual training examples from \"The Pile-Deduplicated\"\n",
    "- \"The Pile Deduplicated\" predominantly contains english text, as stated in the Pile paper\n",
    "- `E5-Large` does not require performing sentence-splitting beforehand, I was misguided by the example code at https://huggingface.co/intfloat/e5-large\n",
    "- `E5-Large` scales poorly to the dataset size under the conditions imposed by the HU Berlin cluster. I will use `E5-Base-4k` instead.\n",
    "- `E5-Base-4k` (https://huggingface.co/dwzhu/e5-base-4k) performs worse than `E5-Large`, as reported e.g. by [Poljarević, Peter. 2024](https://mono.software/2024/11/07/testing-embedding-models-rag/) but has roughly half the parameter count and is therefore more efficient to use\n",
    "- We attempt to mitigate the reported/expectable performance losses by using a larger text window size of $1024$ tokens instead of the $512$ tokens used by `E5-Large` by default.\n",
    "\n",
    "Given the smaller model and The Pile, we iterate through the dataset and extract the embedding for each document.<br>\n",
    "The script I initially implemented for the embedding step is `03_embed_pile_dedup.py`.<br>\n",
    "The approach layed out therein conceptually worked, but it had to be thoroughly memory-optimized to run for as long as needed for our Pile dataset.<br>\n",
    "The optimized script I ultimately ran for this step is `03_embed_pile_dedup_turbo.py`.\n",
    "\n",
    "The embedding step produces as artifact a copy of the original dataset with the embeddings added as a column.<br>\n",
    "The embedding process is resumable, results are persisted in multiple parquet files, one after another, in the folder `Pile_Deduplicated_Embd`.<br>\n",
    "\n",
    "**Note that I intended to upload this embedded version of the Pile to HuggingFace for strict reproducibility.**<br>\n",
    "**This idea was cut short by a change in HuggingFace's pricing policy, effective January 2025, prohibiting the free sharing of datasets >500GB - a threshold which this new dataset crosses (801 GB) due to the added embeddings.**\n",
    "\n",
    "Furthermore, note that the embedding step, through its parallel processing, is in no way guaranteed to maintain the original order of the documents.<br>\n",
    "In fact, this is the reason for why I elected to build the dataset copy with the embeddings as a column in the first place, to ensure the correct alignment of the embeddings with the documents while still being able to leverage parallel processing. Because, after all, time was the deciding factor.<br>\n",
    "The shuffling is therefore not a problem as such, but we have to base the clustering and therefore all following processes on this new, shuffled dataset.\n",
    "\n",
    "In other words, all artifacts produced after the embedding step will relate not to the original dataset, but the embedded dataset, e.g. when referring to entries by index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering of Embeddings\n",
    "\n",
    "- Batchified $k$-means clustering, a term only used in the MiniPile paper: This must stand for **mini-batch k-means clustering**\n",
    "- Cosine distance between normalized embeddings\n",
    "- Cluster Count of $k=220$ ($10$ clusters per source)\n",
    "- Batch size $16384$\n",
    "\n",
    "Architecturally, I built the clustering step to be fully independent of the embedding step, so as to be able to run the partial fitting concurrently with the latter, saving ~4 days of processing time total.\n",
    "\n",
    "The clustering step is implemented in `03_cluster_pile_embed.py`. As soon as the embedding step finishes, a text file is produced, signaling the clustering step to conclude model fitting and start predicting. The centroids are saved and can be found in `MiniPile_BatchKMeans/cluster_centers.npy`. Intermediary centroid results have been omitted from this repository.\n",
    "\n",
    "Each embedding from the newly created dataset is assigned to one of the $220$ clusters. This cluster information as well as the distance of the data point to the centroid are stored in JSONL files, with the entries in order of appearance in the embedded Pile dataset.<br>\n",
    "Each clustering result looks like this: `{\"idx\": 0, \"cluster\": 5, \"distance\": 0.20949329195756128}`.<br>\n",
    "Each Pile-Embedded document is referred to only by its index, crunching the cluster results' memory requirements down.\n",
    "\n",
    "Beyond the JSONL files containing the cluster assignments, the clustering step produces a verification file `MiniPile_BatchKMeans/cluster_results_metadata.json` to indicate whether all chunks and all data points therein have been processed and how results have been saved. We can see from this file that the original dataset size of $134,318,121$ documents has been captured and thus processed, lending more credibility to the clustering results.\n",
    "\n",
    "Additionally, the clustering step produces a file `MiniPile_BatchKMeans/cluster_info_for_inspection.json`.<br>\n",
    "Per cluster index, this file contains the following information:\n",
    "- `closest`: The top 5 closest documents to the cluster centroid\n",
    "    - `text`: The associated text excerpt (for memory reasons)\n",
    "    - `distance`: The cosine distance to the cluster centroid\n",
    "- `farthest`: The top 5 farthest documents from the cluster centroid (again, excerpts, in same format)\n",
    "- `total_examples`: The number of documents assigned to this cluster\n",
    "- `average_distance`: The average cosine distance of all documents to the cluster centroid\n",
    "- `sum_distance`: The sum of all cosine distances of all documents to the cluster centroid\n",
    "\n",
    "The three latter information points are intended to help with the human-guided exclusion of unwanted clusters, but moreover, they may help in improving the dataset creation process later on, as they can provide insights into the spread of the data.\n",
    "\n",
    "After these files were attained, I ran `03_sort_pile_clusters.py` to save the cluster assignment entries organized into one file per cluster / separated by cluster. This is to facilitate a more effective cluster exclusion process during later minipile creation steps.\n",
    "\n",
    "For now, the clustering step is concluded, producing one JSONL file per cluster with information about each cluster assignment per document.<br>\n",
    "This intermediary dataset can be found here: [https://huggingface.co/datasets/Marcus2112/pile_dedup_embeddings_clusters_k220](https://huggingface.co/datasets/Marcus2112/pile_dedup_embeddings_clusters_k220).\n",
    "\n",
    "Note that while the entry count is exactly identical, the size is not. Where the original MiniPile occupied $3.14GB$, we now require $3.71GB$.\n",
    "Several factors play into this:\n",
    "- We use a slightly different embedding model, which may cause (at least occasional) devations in embedding and thus may lead to differently shaped clusters\n",
    "- We select clusters by hand, allowing interpretation of selection categories to roam free. I thus may have selected $38$ clusters according to the paper's categories, but my understanding of enforcing them during selection may differ from the author.\n",
    "- The random sampling happened to select different examples per cluster.\n",
    "\n",
    "Again, note that these indices are not per-se applicable to the original dataset, but to the embedded dataset.\n",
    "\n",
    "### Human-Guided Cluster Exclusion\n",
    "\n",
    "At this point, especially due to the `MiniPile_BatchKMeans/cluster_info_for_inspection.json` file, we can start the human-guided exclusion of unwanted clusters.<br>\n",
    "I strictly adhered to the paper and only sorted out clusters of the layed out categories, which I found to be well identifyable through the 10 examples per cluster.\n",
    "\n",
    "The categories with the clusters I sorted out are as follows:\n",
    "- Near Duplicates ($10, 15, 16, 22, 26, 28, 35, 37, 46, 51, 57, 64, 86, 87, 64, 102, 111, 114, 152, 163, 166, 218$)\n",
    "- Pornography ($167$)\n",
    "- Navigation Bars ($39, 88, 101, 155$)\n",
    "- Product Specifications ($61, 200$)\n",
    "- Long lists of named entities ($40, 44, 78, 90, 99, 103, 181, 196, 219$)\n",
    "\n",
    "### MiniPile Distillation\n",
    "\n",
    "With the cluster analysis concluded, we can now proceed to the distillation of the MiniPile dataset.<br>\n",
    "This dataset touches on all the artifacts produced in the previous steps:<br>\n",
    "- The embedded Pile dataset, from which we extract the documents\n",
    "- The cluster assignments, from which we extract the documents assigned to the remaining clusters\n",
    "- The cluster exclusion list, from which we exclude the documents assigned to the unwanted clusters\n",
    "\n",
    "The distillation step is implemented in `03_distill_pile_embed.py`.<br>\n",
    "The script is written to most exactly and efficiently (we have loads of I/O to perform) extract the correct documents according to our random sampling across the remaining clusters.\n",
    "\n",
    "The distillation step produces a new dataset, the MiniPile, which is a subset of the original Pile dataset.<br>\n",
    "The created dataset is exactly $1,010,500$ documents large, as intended. The dataset is shuffled, to spread cluster entries evenly across the dataset's splits.<br>\n",
    "Additionally, I added a column `pile_idx` to each entry, denoting the original index of the document in the Pile dataset.<br>\n",
    "This helped in making sure that the dataset actually captures a subset derived from across the entire embedded Pile.\n",
    "\n",
    "The resulting self-created MiniPile can be found here: [https://huggingface.co/datasets/Marcus2112/minipile_reproduction](https://huggingface.co/datasets/Marcus2112/minipile_reproduction).\n",
    "\n",
    "I now went on and adapted the `02_train_160M.py` script to train Pythia $160\\text{M}$ on the newly created MiniPile.<br>\n",
    "The adapted script is `03_train_160M_reproduction.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Evaluate Pythia $160\\text{M}$ Pile vs. Pythia $160\\text{M}$ MiniPile (recreated)\n",
    "\n",
    "We will use an exact copy of the training and the test setup previously used for benchmarking Pythia $160\\text{M}$-Pile and $160\\text{M}$-MiniPile-Original.<br>\n",
    "Training is performed with the script `03_train_160M_reproduction.py`.\n",
    "\n",
    "The trained model can be found here: [https://huggingface.co/Marcus2112/pythia-160m-minipile_reproduction](https://huggingface.co/Marcus2112/pythia-160m-minipile_reproduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from lm_eval import utils, simple_evaluate\n",
    "from lm_eval.models.huggingface import HFLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluation - Pythia 160M Trained on Self-Created MiniPile\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "pythia_minipile = AutoModelForCausalLM.from_pretrained(base_path / \"pythia160m_minipile_Reproduction_trained\", local_files_only=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_path / \"pythia160m_dedup_untrained\", use_fast=True, local_files_only=True) # Use exact same tokenizer\n",
    "pythia_minipile = pythia_minipile.to(device)\n",
    " \n",
    "batch_size_hflm = 1\n",
    "\n",
    "pythia_minipile_hflm = HFLM(pretrained=pythia_minipile,\n",
    "                        tokenizer=tokenizer,\n",
    "                        batch_size=batch_size_hflm)\n",
    "\n",
    "results = simple_evaluate(model=pythia_minipile_hflm,\n",
    "                          tasks=[\"arc_challenge\", \"mmlu\", \"winogrande\", \"hellaswag\", \"lambada\", \"blimp\"],\n",
    "                          num_fewshot=0,\n",
    "                          batch_size=batch_size_hflm,\n",
    "                          device=\"cuda\",\n",
    "                          limit=None)\n",
    "\n",
    "with open('03_eval_160M_minipile_reproduction.txt', 'w') as f:\n",
    "    f.write(str(results))\n",
    "\n",
    "print(utils.make_table(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can consider this point a moment of truth for the study project.\n",
    "\n",
    "If benchmark scores indicate significant deviation, a multitude of causes could be the reason, e.g.:\n",
    "- Embedding process may be logically flawed\n",
    "- Switch for `e5-base-4k` may not pay off when looking at embedding accuracy (our gamble for performance didn't work)\n",
    "- Clustering process may be logically flawed\n",
    "- Hand-selected clusters do not match the categories as layed out by the paper (interpretative difference mount to fundamentally different dataset characteristics)\n",
    "- The index-based logical system used for saving memory and processing time may be inconsistent, leading us to sample wildly inaccurately\n",
    "- The paper may not have been clear enough and actually clusters were sampled from with regards to their size, and not flat out equal sample counts across each of them\n",
    "- All or some of the above.\n",
    "\n",
    "However, if benchmark scores *do not* indicate significant deviation, this implies:\n",
    "- The approach as layed out by the paper is reproducible\n",
    "- The pipeline built until now works and can be safely improved upon\n",
    "- The project from here on out may shift towards optimization and generalization, contributing to furthering the working approach\n",
    "\n",
    "I split the results comparison into two parts. The first table compares the just now trained `160M Reproduction` against `160M Pile Deduplicated`, while the second table compares `160M Reproduction` against `160M MiniPile`.\n",
    "\n",
    "### 160M Pile-Deduplicated vs. 160M MiniPile\n",
    "\n",
    "| Benchmark        | Measure    |     | 160M MiniPile                | 160M Pile Deduplicated |\n",
    "| ---------------- | ---------- | --- | ---------------------------- | ---------------------- |\n",
    "| ARC-Challenge    | acc        | ↑   | **0.2125 ± 0.0120**          | 0.1997 ± 0.0117        |\n",
    "| MMLU             | acc        | ↑   | **0.2699 ± 0.0037**          | 0.2299 ± 0.0035        |\n",
    "| HellaSwag        | acc        | ↑   | 0.2560 ± 0.0044              | **0.2903 ± 0.0045**    |\n",
    "| WinoGrande       | acc        | ↑   | 0.4720 ± 0.0140              | **0.4964 ± 0.0141**    |\n",
    "| Lambada (OpenAI) | acc        | ↑   | 0.0000 ± 0.0000              | **0.3689 ± 0.0067**    |\n",
    "| Lambada (OpenAI) | perplexity | ↓   | 3033175.2693 ± 288926.5827   | **31.2589 ± 1.1594**   |\n",
    "| Lambada (Std)    | acc        | ↑   | 0.0000 ± 0.0000              | **0.2335 ± 0.0059**    |\n",
    "| Lambada (Std)    | perplexity | ↓   | 27067951.3461 ± 2710040.1910 | **172.7619 ± 7.7265**  |\n",
    "| BLiMP            | acc        | ↑   | 0.5194 ± 0.0018              | **0.7294 ± 0.0015**    |\n",
    "\n",
    "### 160M Pile-Deduplicated vs. 160M Reproduction\n",
    "\n",
    "| Benchmark        | Measure    |     | 160M Pile Deduplicated | 160M Reproduction           | Percentage Difference of Means | 95% Confidence Interval       | Interpretation                            |\n",
    "| ---------------- | ---------- | --- | ---------------------- | --------------------------- | ------------------------------ | ----------------------------- | ----------------------------------------- |\n",
    "| ARC-Challenge    | acc        | ↑   | **0.1997 ± 0.0117**    | 0.1894 ± 0.0115             | -5.1577                        | (0.0219; -0.0425)             | **Difference not significant**                |\n",
    "| MMLU             | acc        | ↑   | **0.2299 ± 0.0035**    | 0.2295 ± 0.0035             | -0.1740                        | (0.0093; -0.0101)             | **Difference not significant**                |\n",
    "| HellaSwag        | acc        | ↑   | **0.2903 ± 0.0045**    | 0.2604 ± 0.0044             | -10.2997                       | (-0.0176; -0.0422)            | Pile Deduplicated-trained better          |\n",
    "| WinoGrande       | acc        | ↑   | 0.4964 ± 0.0141        | **0.5122 ± 0.0140**         | 3.1829                         | (0.0547; -0.0231)             | **Difference not significant**                |\n",
    "| Lambada (OpenAI) | acc        | ↑   | **0.3689 ± 0.0067**    | 0.0000 ± 0.0000             | -100.0                         | (-0.3558; -0.3820)            | Pile Deduplicated-trained severely better |\n",
    "| Lambada (OpenAI) | perplexity | ↓   | **31.2589 ± 1.1594**   | 1854408.3999 ± 148101.5978  | 5932317.3272                   | (2144656.2727; 1564098.0093)  | Pile Deduplicated-trained severely better |\n",
    "| Lambada (Std)    | acc        | ↑   | **0.2335 ± 0.0059**    | 0.0000 ± 0.0000             | -100.0                         | (-0.2219; -0.2451)            | Pile Deduplicated-trained severely better |\n",
    "| Lambada (Std)    | perplexity | ↓   | **172.7619 ± 7.7265**  | 11927123.2514 ± 1063672.928 | 6903692.5905                   | (14011749.4290; 9842151.5500) | Pile Deduplicated-trained severely better |\n",
    "| BLiMP            | acc        | ↑   | **0.7294 ± 0.0015**    | 0.5481 ± 0.0017             | -24.8560                       | (-0.1769; -0.1857)            | Pile Deduplicated-trained better          |\n",
    "\n",
    "### 160 MiniPile vs. 160M Reproduction\n",
    "\n",
    "| Benchmark        | Measure    |     | 160M MiniPile               | 160M Reproduction               | Percentage Difference of Means | 95% Confidence Interval         | Interpretation             |\n",
    "| ---------------- | ---------- | --- | --------------------------- | ------------------------------- | ------------------------------ | ------------------------------- | -------------------------- |\n",
    "| ARC-Challenge    | acc        | ↑   | **0.2125 ± 0.0120**         | 0.1894 ± <br>0.0115             | -10.8706                       | (0.0095; -0.0577)               | **Difference not significant** |\n",
    "| MMLU             | acc        | ↑   | **0.2699 ± 0.0037**         | 0.2295 ± 0.0035                 | -14.9685                       | (-0.0304; -0.0504)              | MiniPile-trained better    |\n",
    "| HellaSwag        | acc        | ↑   | 0.2560 ± 0.0044             | **0.2604 ± 0.0044**             | 1.7188                         | (0.0166; -0.0078)               | **Difference not significant** |\n",
    "| WinoGrande       | acc        | ↑   | 0.4720 ± 0.0140             | **0.5122 ± 0.0140**             | 8.5169                         | (0.0790; 0.0014)                | **Reproduction better**       |\n",
    "| Lambada (OpenAI) | acc        | ↑   | 0.0000 ± 0.0000             | 0.0000 ± 0.0000                 | -                              | -                               | -                          |\n",
    "| Lambada (OpenAI) | perplexity | ↓   | 3033175.2693 ± 288926.5827  | **1854408.3999 ± 148101.5978**  | -38.8625                       | (-542407.4980; -1815126.2408)   | **Reproduction severely better** |\n",
    "| Lambada (Std)    | acc        | ↑   | 0.0000 ± 0.0000             | 0.0000 ± 0.0000                 | -                              | -                               | -                          |\n",
    "| Lambada (Std)    | perplexity | ↓   | 27067951.3460 ± 2710040.191 | **11927123.2514 ± 1063672.928** | -55.9364                       | (-9434663.1814; -20846993.0080) | **Reproduction severely better** |\n",
    "| BLiMP            | acc        | ↑   | 0.5194 ± 0.0018             | **0.5481 ± 0.0017**             | 5.5256                         | (0.0336; 0.0238)                | **Reproduction better**          |\n",
    "\n",
    "Not only can we see a match with the original MiniPile across all but one (MMLU) benchmark, but we substantially increased the model's capability on both Lambada benchmarks when evaluated for perplexity, and achieved better results on WinoGrande and BLiMP. \n",
    "\n",
    "This is interesting, because we composed `MiniPile_Reproduction` with the exact same amount of samples.<br>\n",
    "While this is the case, the dataset is ~500MB (in text, rest is additional idx column values + metadata) larger, but this difference, which itself can be explained by sampling and embedding expression deviations, has brought down perplexity by ~39% and ~56% respectively.\n",
    "\n",
    "**What does this noticable upwards trend on the perplexities imply?**\n",
    "I consider Lambada (OpenAI) and Lambada (Std) as benchmarks that aim to be useful for checking whether dataset reduction impacts capability to capture even subtle, general grammatical distinctions. The Lambadas realize this via evaluating grammatical knowledge across $67$ distinct linguistic phenomena/subtasks in English.\n",
    "It seems that the data sampled represents nuances of the English language better and in such a way that the trained model could generalize grammatical rules from across the identified clusters much more easily.\n",
    "\n",
    "I conclude that the selection and assembly of `MiniPile_Reproduction` has been successful to that extent that it can be tried out for training on Pythia $1.4B$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Evaluate Pythia $1.4\\text{B}$ Pile vs. Pythia $1.4\\text{B}$ MiniPile (recreated)\n",
    "\n",
    "Once again, we will use an exact copy of the training and the test setup previously used for benchmarking Pythia $160\\text{M}$-Pile and $160\\text{M}$-MiniPile-Original.<br>\n",
    "Critically, parameters like the size-specific learning rate and system-wise accomodated batch size have been adjusted, while step count is kept the same.<br>\n",
    "Training is performed with the script `03_train_1.4B_reproduction.py`.\n",
    "\n",
    "| Benchmark        | Measure    |     | 1.4B Pile Pile Deduplicated | 1.4B MiniPile Reproduction | Percentage Difference of Means | 95% Confidence Interval       | Interpretation                 |\n",
    "| ---------------- | ---------- | --- | -------------------- | -------------------------- | ------------------------------ | ----------------------------- | ------------------------------ |\n",
    "| ARC-Challenge    | acc        | ↑   | **0.2600 ± 0.0130**  | 0.1928 ± 0.0115            | -25.8462                       | (-0.0332; -0.1012)            | 1.4B Pile better               |\n",
    "| MMLU             | acc        | ↑   | **0.2388 ± 0.0036**  | 0.2295 ± 0.0035            | **-3.8945**                    | (0.0005; -0.0191)             | **Difference not significant** |\n",
    "| HellaSwag        | acc        | ↑   | **0.4177 ± 0.0049**  | 0.2584 ± 0.0044            | -38.1374                       | (-0.1464; -0.1722)            | 1.4B Pile better               |\n",
    "| WinoGrande       | acc        | ↑   | **0.5730 ± 0.0140**  | 0.5091 ± 0.0141            | **-11.1518**                   | (-0.0250; -0.1028)            | 1.4B Pile better               |\n",
    "| Lambada (OpenAI) | acc        | ↑   | **0.6202 ± 0.0068**  | 0.0000 ± 0.0000            | -100.0                         | (-0.6069; -0.6335)            | 1.4B Pile better               |\n",
    "| Lambada (OpenAI) | perplexity | ↓   | **6.1041 ± 0.1531**  | 1520707.8702 ± 115261.3664 | 24912792.4854                  | (1746614.0442; 1294789.4880)  | 1.4B Pile much better          |\n",
    "| Lambada (Std)    | acc        | ↑   | **0.4898 ± 0.0070**  | 0.0000 ± 0.0000            | -100.0                         | (-0.4761; -0.5035)            | 1.4B Pile better               |\n",
    "| Lambada (Std)    | perplexity | ↓   | **11.2448 ± 0.3305** | 8651201.8876 ± 735161.5236 | 76935033.4626                  | (10092107.2291; 7210274.0565) | 1.4B Pile much better          |\n",
    "| BLiMP            | acc        | ↑   | **0.8154 ± 0.0013**  | 0.5397 ± 0.0016            | -33.8116                       | (-0.2717; -0.2797)            | 1.4B Pile better               |\n",
    "| ARC-Easy         | acc        | ↑   | **0.6174 ± 0.0100**  | 0.2673 ± 0.0091            | -56.7055                       | (-0.3236; -0.3766)            | 1.4B Pile better               |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minipile",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
