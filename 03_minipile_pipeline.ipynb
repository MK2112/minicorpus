{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recreating and Improving MiniPile Dataset Creation\n",
    "\n",
    "**Objectives:**\n",
    "- [x] Implement and verify MiniPile’s filtering pipeline according to [Kaddour (2023)](https://arxiv.org/abs/2304.08442), but intended for decoder-only model use\n",
    "- [x] Evaluate and compare performances of Pythia $160\\text{M}$ pretrained on The Pile vs. trained on the *newly, self-created MiniPile* on MMLU and ARC-Challenge\n",
    "- [.] Evaluate and compare performances of Pythia $1.4\\text{B}$ pretrained on The Pile vs. trained on the *newly, self-created MiniPile* on MMLU and ARC-Challenge\n",
    "- [.] Improve the dataset creation process, create new SuperMiniPile dataset (ideally smaller and more information-retaining)\n",
    "- [] Evaluate Pythia $160\\text{M}$ on SuperMiniPile on MMLU and ARC-Challenge\n",
    "- [] Evaluate and compare performances of Pythia $1.4\\text{B}$ pretrained on The Pile vs. trained on SuperMiniPile on the MMLU and ARC benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import snapshot_download\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "base_dir = \"/vol/tmp/koppelmm\"\n",
    "base_path = Path(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_model(down_dir: str, target_folder: str, cache_folder: str, repo_id: str, branch: str = \"main\") -> None:\n",
    "    down_dir = Path(down_dir)\n",
    "    target_dir = down_dir / target_folder\n",
    "    cache_dir = down_dir / cache_folder\n",
    "\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"Downloading {repo_id}/{branch}...\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            snapshot_download(\n",
    "                repo_id,\n",
    "                repo_type=\"model\",\n",
    "                revision=branch,\n",
    "                cache_dir=str(cache_dir),\n",
    "                local_dir=str(target_dir)\n",
    "            )\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Download attempt failed: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Recreating The MiniPile Dataset Creation Pipeline\n",
    "\n",
    "(1) document embedding extraction,<br>\n",
    "(2) clustering of embeddings, and<br>\n",
    "(3) human-guided exclusion of unwanted clusters<br>\n",
    "(4) mini-pile distillation\n",
    "\n",
    "- 22 data subset sources\n",
    "- 5.91 KiB mean document size (before deduplication)\n",
    "\n",
    "### Document Embedding Extraction\n",
    "\n",
    "- MiniPile paper uses term \"document\": This refers to individual training examples from \"The Pile-Deduplicated\"\n",
    "- \"The Pile Deduplicated\" predominantly contains english text, as stated in the Pile paper\n",
    "- `E5-Large` does not require performing sentence-splitting beforehand, I was misguided by the example code at https://huggingface.co/intfloat/e5-large\n",
    "- `E5-Large` scales poorly to the dataset size under the conditions imposed by the HU Berlin cluster. I will use `E5-Base-4k` instead.\n",
    "- `E5-Base-4k` performs slightly worse than `E5-Large`, but has roughly half the parameter count and is therefore more efficient to use\n",
    "- We attempt to mitigate the reported/expectable performance losses by using a larger text window size of $1024$ tokens instead of the $512$ tokens used by `E5-Large` by default.\n",
    "\n",
    "Given the smaller model and The Pile, we iterate through the dataset and extract the embedding for each document.<br>\n",
    "The script I initially implemented for the embedding step is `03_embed_pile_dedup.py`.<br>\n",
    "The approach layed out therein conceptually worked, but it had to be thoroughly memory-optimized to run for as long as needed for our Pile dataset.<br>\n",
    "The optimized script I ultimately ran for this step is `03_embed_pile_dedup_turbo.py`.\n",
    "\n",
    "The embedding step produces as artifact a copy of the original dataset with the embeddings added as a column.<br>\n",
    "The embedding process is resumable, results are persisted in multiple parquet files, one after another, in the folder `Pile_Deduplicated_Embd`.<br>\n",
    "\n",
    "**Note that I intended to upload this embedded version of the Pile to HuggingFace for strict reproducibility.**<br>\n",
    "**This idea was cut short by a change in HuggingFace's pricing policy, effective January 2025, prohibiting the free sharing of datasets >500GB - a threshold which this new dataset crosses (801 GB) due to the added embeddings.**\n",
    "\n",
    "Furthermore, note that the embedding step, through its parallel processing, is in no way guaranteed to maintain the original order of the documents.<br>\n",
    "In fact, this is the reason for why I elected to build the dataset copy with the embeddings as a column in the first place, to ensure the correct alignment of the embeddings with the documents while still being able to leverage parallel processing. Because, after all, time was the deciding factor.<br>\n",
    "The shuffling is therefore not a problem as such, but we have to base the clustering and therefore all following processes on this new, shuffled dataset.\n",
    "\n",
    "In other words, all artifacts produced after the embedding step will relate not to the original dataset, but the embedded dataset, e.g. when referring to entries by index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering of Embeddings\n",
    "\n",
    "- Batchified $k$-means clustering, a term only used in the MiniPile paper: This must stand for **mini-batch k-means clustering**\n",
    "- Cosine distance between normalized embeddings\n",
    "- Cluster Count of $k=220$ ($10$ clusters per source)\n",
    "- Batch size $16384$\n",
    "\n",
    "Architecturally, I built the clustering step to be fully independent of the embedding step, so as to be able to run the partial fitting concurrently with the latter, saving ~4 days of processing time total.\n",
    "\n",
    "The clustering step is implemented in `03_cluster_pile_embed.py`. As soon as the embedding step finishes, a text file is produced, signaling the clustering step to conclude model fitting and start predicting. The centroids are saved and can be found in `MiniPile_BatchKMeans/cluster_centers.npy`. Intermediary centroid results have been omitted from this repository.\n",
    "\n",
    "Each embedding from the newly created dataset is assigned to one of the $220$ clusters. This cluster information as well as the distance of the data point to the centroid are stored in JSONL files, with the entries in order of appearance in the embedded Pile dataset.<br>\n",
    "Each clustering result looks like this: `{\"idx\": 0, \"cluster\": 5, \"distance\": 0.20949329195756128}`.<br>\n",
    "Each Pile-Embedded document is referred to only by its index, crunching the cluster results' memory requirements down.\n",
    "\n",
    "Beyond the JSONL files containing the cluster assignments, the clustering step produces a verification file `MiniPile_BatchKMeans/cluster_results_metadata.json` to indicate whether all chunks and all data points therein have been processed and how results have been saved. We can see from this file that the original dataset size of $134,318,121$ documents has been captured and thus processed, lending more credibility to the clustering results.\n",
    "\n",
    "Additionally, the clustering step produces a file `MiniPile_BatchKMeans/cluster_info_for_inspection.json`.<br>\n",
    "Per cluster index, this file contains the following information:\n",
    "- `closest`: The top 5 closest documents to the cluster centroid\n",
    "    - `text`: The associated text excerpt (for memory reasons)\n",
    "    - `distance`: The cosine distance to the cluster centroid\n",
    "- `farthest`: The top 5 farthest documents from the cluster centroid (again, excerpts, in same format)\n",
    "- `total_examples`: The number of documents assigned to this cluster\n",
    "- `average_distance`: The average cosine distance of all documents to the cluster centroid\n",
    "- `sum_distance`: The sum of all cosine distances of all documents to the cluster centroid\n",
    "\n",
    "The three latter information points are intended to help with the human-guided exclusion of unwanted clusters, but moreover, they may help in improving the dataset creation process later on, as they can provide insights into the spread of the data.\n",
    "\n",
    "After these files were attained, I ran `03_sort_pile_clusters.py` to save the cluster assignment entries organized into one file per cluster / separated by cluster. This is to facilitate a more effective cluster exclusion process during later minipile creation steps.\n",
    "\n",
    "For now, the clustering step is concluded, producing one JSONL file per cluster with information about each cluster assignment per document.<br>\n",
    "This intermediary dataset can be found here: [https://huggingface.co/datasets/Marcus2112/pile_dedup_embeddings_clusters](https://huggingface.co/datasets/Marcus2112/pile_dedup_embeddings_clusters).\n",
    "\n",
    "Note that while the entry count is exactly identical, the size is not. Where the original MiniPile occupied $3.14GB$, we now require $3.71GB$.\n",
    "Several factors play into this:\n",
    "- We use a slightly different embedding model, which may cause (at least occasional) devations in embedding and thus may lead to differently shaped clusters\n",
    "- We select clusters by hand, allowing interpretation of selection categories to roam free. I thus may have selected $38$ clusters according to the paper's categories, but my understanding of enforcing them during selection may differ from the author.\n",
    "- The random sampling happened to select different examples per cluster.\n",
    "\n",
    "Again, note that these indices are not per-se applicable to the original dataset, but to the embedded dataset.\n",
    "\n",
    "### Human-Guided Cluster Exclusion\n",
    "\n",
    "At this point, especially due to the `MiniPile_BatchKMeans/cluster_info_for_inspection.json` file, we can start the human-guided exclusion of unwanted clusters.<br>\n",
    "I strictly adhered to the paper and only sorted out clusters of the layed out categories, which I found to be well identifyable through the 10 examples per cluster.\n",
    "\n",
    "The categories with the clusters I sorted out are as follows:\n",
    "- Near Duplicates ($10, 15, 16, 22, 26, 28, 35, 37, 46, 51, 57, 64, 86, 87, 64, 102, 111, 114, 152, 163, 166, 218$)\n",
    "- Pornography ($167$)\n",
    "- Navigation Bars ($39, 88, 101, 155$)\n",
    "- Product Specifications ($61, 200$)\n",
    "- Long lists of named entities ($40, 44, 78, 90, 99, 103, 181, 196, 219$)\n",
    "\n",
    "### MiniPile Distillation\n",
    "\n",
    "With the cluster analysis concluded, we can now proceed to the distillation of the MiniPile dataset.<br>\n",
    "This dataset touches on all the artifacts produced in the previous steps:<br>\n",
    "- The embedded Pile dataset, from which we extract the documents\n",
    "- The cluster assignments, from which we extract the documents assigned to the remaining clusters\n",
    "- The cluster exclusion list, from which we exclude the documents assigned to the unwanted clusters\n",
    "\n",
    "The distillation step is implemented in `03_distill_pile_embed.py`.<br>\n",
    "The script is written to most exactly and efficiently (we have loads of I/O to perform) extract the correct documents according to our random sampling across the remaining clusters.\n",
    "\n",
    "The distillation step produces a new dataset, the MiniPile, which is a subset of the original Pile dataset.<br>\n",
    "The created dataset is exactly $1,010,500$ documents large, as intended. The dataset is shuffled, to spread cluster entries evenly across the dataset's splits.<br>\n",
    "Additionally, I added a column `pile_idx` to each entry, denoting the original index of the document in the Pile dataset.<br>\n",
    "This helped in making sure that the dataset actually captures a subset derived from across the entire embedded Pile.\n",
    "\n",
    "The resulting self-created MiniPile can be found here: [https://huggingface.co/datasets/Marcus2112/minipile_recreation](https://huggingface.co/datasets/Marcus2112/minipile_recreation).\n",
    "\n",
    "I now went on and adapted the `02_train_160M.py` script to train Pythia $160\\text{M}$ on the newly created MiniPile.<br>\n",
    "The adapted script is `03_train_160M_recreation.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Evaluate Pythia $160\\text{M}$ Pile vs. Pythia $160\\text{M}$ MiniPile (recreated)\n",
    "\n",
    "We will use an exact copy of the training and the test setup previously used for benchmarking Pythia $160\\text{M}$-Pile and $160\\text{M}$-MiniPile-Original.<br>\n",
    "Training is performed with the script `03_train_160M_recreation.py`.\n",
    "\n",
    "The trained model can be found here: [https://huggingface.co/Marcus2112/pythia-160m-minipile_recreation](https://huggingface.co/Marcus2112/pythia-160m-minipile_recreation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from lm_eval import utils, simple_evaluate\n",
    "from lm_eval.models.huggingface import HFLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluation - Pythia 160M Trained on Self-Created MiniPile\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "pythia_minipile = AutoModelForCausalLM.from_pretrained(base_path / \"pythia160m_minipile_Recreation_trained\", local_files_only=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_path / \"pythia160m_dedup_untrained\", use_fast=True, local_files_only=True) # Use exact same tokenizer\n",
    "pythia_minipile = pythia_minipile.to(device)\n",
    " \n",
    "batch_size_hflm = 1\n",
    "\n",
    "pythia_minipile_hflm = HFLM(pretrained=pythia_minipile,\n",
    "                        tokenizer=tokenizer,\n",
    "                        batch_size=batch_size_hflm)\n",
    "\n",
    "results = simple_evaluate(model=pythia_minipile_hflm,\n",
    "                          tasks=[\"arc_challenge\", \"mmlu\", \"winogrande\", \"hellaswag\", \"lambada\", \"blimp\"],\n",
    "                          num_fewshot=0,\n",
    "                          batch_size=batch_size_hflm,\n",
    "                          device=\"cuda\",\n",
    "                          limit=None)\n",
    "\n",
    "with open('03_eval_160M_minipile_recreation.txt', 'w') as f:\n",
    "    f.write(str(results))\n",
    "\n",
    "print(utils.make_table(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can consider this point a moment of truth for the study project.\n",
    "\n",
    "If benchmark scores indicate significant deviation, a multitude of causes could be the reason, e.g.:\n",
    "- Embedding process may be logically flawed\n",
    "- Switch for `e5-base-4k` may not pay off when looking at embedding accuracy (our gamble for performance didn't work)\n",
    "- Clustering process may be logically flawed\n",
    "- Hand-selected clusters do not match the categories as layed out by the paper (interpretative difference mount to fundamentally different dataset characteristics)\n",
    "- The index-based logical system used for saving memory and processing time may be inconsistent, leading us to sample wildly inaccurately\n",
    "- The paper may not have been clear enough and actually clusters were sampled from with regards to their size, and not flat out equal sample counts across each of them\n",
    "- All or some of the above.\n",
    "\n",
    "However, if benchmark scores *do not* indicate significant deviation, this implies:\n",
    "- The approach as layed out by the paper is reproducible\n",
    "- The pipeline built until now works and can be safely improved upon\n",
    "- The project from here on out may shift towards optimization and generalization, contributing to furthering the working approach\n",
    "\n",
    "I split the results comparison into two parts. The first table compares the just now trained `160M Recreation` against `160M Pile Deduplicated`, while the second table compares `160M Recreation` against `160M MiniPile`.\n",
    "\n",
    "### 160M Pile-Deduplicated vs. 160M Recreation\n",
    "\n",
    "| Benchmark        | Measure    |     | 160M Pile Deduplicated | 160M Recreation             | Percentage Difference of Means | 95% Confidence Interval       | Interpretation                            |\n",
    "| ---------------- | ---------- | --- | ---------------------- | --------------------------- | ------------------------------ | ----------------------------- | ----------------------------------------- |\n",
    "| ARC-Challenge    | acc        | ↑   | **0.1997 ± 0.0117**    | 0.1894 ± 0.0115             | -5.1577                        | (0.0219; -0.0425)             | **Difference not significant**                |\n",
    "| MMLU             | acc        | ↑   | **0.2299 ± 0.0035**    | 0.2295 ± 0.0035             | -0.1740                        | (0.0093; -0.0101)             | **Difference not significant**                |\n",
    "| HellaSwag        | acc        | ↑   | **0.2903 ± 0.0045**    | 0.2604 ± 0.0044             | -10.2997                       | (-0.0176; -0.0422)            | Pile Deduplicated-trained better          |\n",
    "| WinoGrande       | acc        | ↑   | 0.4964 ± 0.0141        | **0.5122 ± 0.0140**         | 3.1829                         | (0.0547; -0.0231)             | **Difference not significant**                |\n",
    "| Lambada (OpenAI) | acc        | ↑   | **0.3689 ± 0.0067**    | 0.0000 ± 0.0000             | -100.0                         | (-0.3558; -0.3820)            | Pile Deduplicated-trained severely better |\n",
    "| Lambada (OpenAI) | perplexity | ↓   | **31.2589 ± 1.1594**   | 1854408.3999 ± 148101.5978  | 5932317.3272                   | (2144656.2727; 1564098.0093)  | Pile Deduplicated-trained severely better |\n",
    "| Lambada (Std)    | acc        | ↑   | **0.2335 ± 0.0059**    | 0.0000 ± 0.0000             | -100.0                         | (-0.2219; -0.2451)            | Pile Deduplicated-trained severely better |\n",
    "| Lambada (Std)    | perplexity | ↓   | **172.7619 ± 7.7265**  | 11927123.2514 ± 1063672.928 | 6903692.5905                   | (14011749.4290; 9842151.5500) | Pile Deduplicated-trained severely better |\n",
    "| BLiMP            | acc        | ↑   | **0.7294 ± 0.0015**    | 0.5481 ± 0.0017             | -24.8560                       | (-0.1769; -0.1857)            | Pile Deduplicated-trained better          |\n",
    "\n",
    "### 160 MiniPile vs. 160M Recreation\n",
    "\n",
    "| Benchmark        | Measure    |     | 160M MiniPile               | 160M Recreation                 | Percentage Difference of Means | 95% Confidence Interval         | Interpretation             |\n",
    "| ---------------- | ---------- | --- | --------------------------- | ------------------------------- | ------------------------------ | ------------------------------- | -------------------------- |\n",
    "| ARC-Challenge    | acc        | ↑   | **0.2125 ± 0.0120**         | 0.1894 ± <br>0.0115             | -10.8706                       | (0.0095; -0.0577)               | **Difference not significant** |\n",
    "| MMLU             | acc        | ↑   | **0.2699 ± 0.0037**         | 0.2295 ± 0.0035                 | -14.9685                       | (-0.0304; -0.0504)              | MiniPile-trained better    |\n",
    "| HellaSwag        | acc        | ↑   | 0.2560 ± 0.0044             | **0.2604 ± 0.0044**             | 1.7188                         | (0.0166; -0.0078)               | **Difference not significant** |\n",
    "| WinoGrande       | acc        | ↑   | 0.4720 ± 0.0140             | **0.5122 ± 0.0140**             | 8.5169                         | (0.0790; 0.0014)                | **Recreation better**          |\n",
    "| Lambada (OpenAI) | acc        | ↑   | 0.0000 ± 0.0000             | 0.0000 ± 0.0000                 | -                              | -                               | -                          |\n",
    "| Lambada (OpenAI) | perplexity | ↓   | 3033175.2693 ± 288926.5827  | **1854408.3999 ± 148101.5978**  | -38.8625                       | (-542407.4980; -1815126.2408)   | **Recreation severely better** |\n",
    "| Lambada (Std)    | acc        | ↑   | 0.0000 ± 0.0000             | 0.0000 ± 0.0000                 | -                              | -                               | -                          |\n",
    "| Lambada (Std)    | perplexity | ↓   | 27067951.3460 ± 2710040.191 | **11927123.2514 ± 1063672.928** | -55.9364                       | (-9434663.1814; -20846993.0080) | **Recreation severely better** |\n",
    "| BLiMP            | acc        | ↑   | 0.5194 ± 0.0018             | **0.5481 ± 0.0017**             | 5.5256                         | (0.0336; 0.0238)                | **Recreation better**          |\n",
    "\n",
    "Not only can we see a match with the original MiniPile across all but one (MMLU) benchmark, but we substantially increased the model's capability on both Lambada benchmarks when evaluated for perplexity, and achieved better results on WinoGrande and BLiMP. \n",
    "\n",
    "This is interesting, because we composed `MiniPile_Recreation` with the exact same amount of samples.<br>\n",
    "While this is the case, the dataset is ~500MB (in text, rest is additional idx column values + metadata) larger, but this difference, which itself can be explained by sampling and embedding expression deviations, has brought down perplexity by ~39% and ~56% respectively.\n",
    "\n",
    "**What does this noticable upwards trend on the perplexities imply?**\n",
    "I consider Lambada (OpenAI) and Lambada (Std) as benchmarks that aim to be useful for checking whether dataset reduction impacts capability to capture even subtle, general grammatical distinctions. The Lambadas realize this via evaluating grammatical knowledge across $67$ distinct linguistic phenomena/subtasks in English.\n",
    "It seems that the data sampled represents nuances of the English language better and in such a way that the trained model could generalize grammatical rules from across the identified clusters much more easily.\n",
    "\n",
    "I conclude that the selection and assembly of `MiniPile_Recreation` has been successful to that extent that it can be tried out for training on Pythia $1.4B$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Evaluate Pythia $1.4\\text{B}$ Pile vs. Pythia $1.4\\text{B}$ MiniPile (recreated)\n",
    "\n",
    "Once again, we will use an exact copy of the training and the test setup previously used for benchmarking Pythia $160\\text{M}$-Pile and $160\\text{M}$-MiniPile-Original.<br>\n",
    "Critically, parameters like the size-specific learning rate and system-wise accomodated batch size have been adjusted, while step count is kept the same.<br>\n",
    "Training is performed with the script `03_train_1.4B_recreation.py`.\n",
    "\n",
    "| Benchmark        | Measure    |     | 1.4B Pile Pretrained | 1.4B MiniPile Recreation | Percentage Difference of Means | 95% Confidence Interval | Interpretation |\n",
    "| ---------------- | ---------- | --- | -------------------- | ------------------------ | ------------------------------ | ----------------------- | -------------- |\n",
    "| ARC-Challenge    | acc        | ↑   | 0.2600 ± 0.0130      |                          |                                |                         |                |\n",
    "| MMLU             | acc        | ↑   | 0.2388 ± 0.0036      |                          |                                |                         |                |\n",
    "| HellaSwag        | acc        | ↑   | 0.4177 ± 0.0049      |                          |                                |                         |                |\n",
    "| WinoGrande       | acc        | ↑   | 0.5730 ± 0.0140      |                          |                                |                         |                |\n",
    "| Lambada (OpenAI) | acc        | ↑   | 0.6202 ± 0.0068      |                          |                                |                         |                |\n",
    "| Lambada (OpenAI) | perplexity | ↓   | 6.1041 ± 0.1531      |                          |                                |                         |                |\n",
    "| Lambada (Std)    | acc        | ↑   | 0.4898 ± 0.0070      |                          |                                |                         |                |\n",
    "| Lambada (Std)    | perplexity | ↓   | 11.2448 ± 0.3305     |                          |                                |                         |                |\n",
    "| BLiMP            | acc        | ↑   | 0.8154 ± 0.0013      |                          |                                |                         |                |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Improve the Dataset Creation Process\n",
    "\n",
    "The results and implementation specifics from the reproduction directly affect the investigation for improvements.<br>\n",
    "**The aim is to investigate different ideas for creating a new SuperMiniPile dataset that is ideally smaller and/or more information-retaining.**\n",
    "\n",
    "### Idea 1 - Cluster-Proportionate Sampling\n",
    "\n",
    "The original MiniPile dataset was created by sampling *equal* amounts of documents from each of the non-excluded clusters. This results in a MiniPile that cannot represent the original dataset's cluster distribution anymore, but rather imposes a uniform distribution across the clusters, no matter their size or importance/'weight'.\n",
    "\n",
    "For a first improvement attempt, named 'Proportionate', we keep as close to the reproduction code as possible. But, instead of sampling equal amounts of documents from each remaining cluster, we sample a proportionate amount of documents based on cluster sizes (by document count). This requires to make the amount of data points an upper bound rather than a fixed requirement, as we may not be able to sample the exact amount of documents from each cluster by their size. We just don't want to go over the MiniPile-document count.\n",
    "\n",
    "As a side note, the original script was observed as being relatively memory-demanding and, moreover, cache-guzzling.<br>\n",
    "The parquet data-retrieval process was to blame for that, and got thoroughly improved, ditching efforts to wrestle with Pandas (the library), replacing it with Numpy and using explicit cache management, which in turn lifted needs for deep copying.\n",
    "\n",
    "This idea's distillation script is implemented in `03_distill_pile_embed_idea_1_proportionate.py`.<br>\n",
    "Based on thus assembled $1,010,409$ documents, a Pythia $160\\text{M}$ was trained for evaluation.<br>\n",
    "This idea's trained model can be found here: [https://huggingface.co/Marcus2112/pythia-160m-minipile_proportionate](https://huggingface.co/Marcus2112/pythia-160m-minipile_proportionate)\n",
    "\n",
    "The benchmark results are as follows, compared to Pile, MiniPile and MiniPile Recreation:\n",
    "\n",
    "| Benchmark        | Measure    |     | 160M Pile Deduplicated | 160M MiniPile               | 160M Recreation             | 160M Proportionate           |\n",
    "| ---------------- | ---------- | --- | ---------------------- | --------------------------- | --------------------------- | ---------------------------- |\n",
    "| ARC-Challenge    | acc        | ↑   | 0.1997 ± 0.0117        | **0.2125 ± 0.0120**         | 0.1894 ± 0.0115             | 0.1928 ± 0.0115              |\n",
    "| MMLU             | acc        | ↑   | 0.2299 ± 0.0035        | **0.2699 ± 0.0037**         | 0.2295 ± 0.0035             | 0.2295 ± 0.0035              |\n",
    "| HellaSwag        | acc        | ↑   | **0.2903 ± 0.0045**    | 0.2560 ± 0.0044             | 0.2604 ± 0.0044             | 0.2613 ± 0.0044              |\n",
    "| WinoGrande       | acc        | ↑   | 0.4964 ± 0.0141        | 0.4720 ± 0.0140             | **0.5122 ± 0.0140**         | 0.5051 ± 0.0141              |\n",
    "| Lambada (OpenAI) | acc        | ↑   | **0.3689 ± 0.0067**    | 0.0000 ± 0.0000             | 0.0000 ± 0.0000             | 0.0000 ± 0.0000              |\n",
    "| Lambada (OpenAI) | perplexity | ↓   | **31.2589 ± 1.1594**   | 3033175.2693 ± 288926.5827  | 1854408.3999 ± 148101.5978  | 2214257.4651 ± 184064.6008   |\n",
    "| Lambada (Std)    | acc        | ↑   | **0.2335 ± 0.0059**    | 0.0000 ± 0.0000             | 0.0000 ± 0.0000             | 0.0000 ± 0.0000              |\n",
    "| Lambada (Std)    | perplexity | ↓   | **172.7619 ± 7.7265**  | 27067951.3460 ± 2710040.191 | 11927123.2514 ± 1063672.928 | 15143084.5983 ± 1387627.8650 |\n",
    "| BLiMP            | acc        | ↑   | **0.7294 ± 0.0015**    | 0.5194 ± 0.0018             | 0.5481 ± 0.0017             | 0.5452 ± 0.0017              |\n",
    "\n",
    "The proportional sampling approach yields a dataset that is ~300 MB smaller than the Recreation dataset, while also containing $91$ entries less.<br>\n",
    "At the same time, proportional sampling yields close to equal results in all but the perplexity benchmarks, where it underperforms compared to the Recreation dataset, yet still beats the original MiniPile by a large margin.\n",
    "\n",
    "> We seem to have lost some highly informative examples from smaller clusters when scaling down their representation to match cluster proportions.\n",
    "\n",
    "*The proportional sampling approach is not a clear improvement over the recreation*, but rather a compromise between the original MiniPile and the Recreation dataset (tending towards the latter in results though). This is interesting, as it implies the uniform sampling approach may have been more effective in capturing the dataset's most speaking examples than the proportional sampling approach. I do not think that is a fault of the proportional sampling approach, but rather a symptom arising from the original dataset's composition. \n",
    "\n",
    "> Even though we incorporate the original dataset topology more directly, this doesn't necessarily come along with a better focus on the most informative examples.\n",
    "\n",
    "Proportional sampling is neither a failure nor a universal solution, and neither is uniform sampling, but the dismissal of either approach would be premature.<br>\n",
    "Instead, sampling styles should be context-dependent.<br>\n",
    "I assume that when performance on specific downstream tasks, such as language modeling, is stated as the primary goal, incorporating adaptive weighting mechanisms that emphasize the contribution of critical clusters might yield superior results.\n",
    "\n",
    "I conclude that, while itself not a clear improvement, the proportional sampling idea is valid for e.g. approaches where it is important to capture the original dataset's cluster distribution more closely. If the main goal is retaining performance, however, future ideas should investigate how to find a content-based weighting factor per cluster.\n",
    "\n",
    "### Idea 2 - Hybrid Sampling (Lossi)\n",
    "\n",
    "In order to sample documents considered most representative and informative, the original MiniPile uses a one-shot proxy-based geometric sampling strategy. After all, the pipeline doesn't really 'select' documents by their content, but by their embedding's position in the cluster space, relative to other documents by proxy of the centroid.<br>\n",
    "Beyond that, once clusters have been determined and selected, the pipeline samples randomly across each cluster.<br>\n",
    "The comparison of different subset assembly techniques performed by [(Guo et al. 2022)](https://arxiv.org/abs/2204.08499) concludes that random sampling can be considered a very robust baseline for custom subset selection efforts, adaptable to various tasks.\n",
    "\n",
    "I deduct that employing cluster-wise random sampling, as performed already, while effective, fast and versatile, can not explicitly consider the point-by-point actual degree of informativeness. To an extent, we rely on luck and sampling spread for finding a best MiniPile distillate.\n",
    "\n",
    "Instead, what if we could utilize the heavy lifting by the embedding and clustering steps, while adding into the process an instance for information-based guiding of cluster sample factor weighting and specific document selection, which could thus address the findings of Idea 1 (Proportionate Sampling)?\n",
    "\n",
    "**Idea 2, called Lossi (Loss-informed Sampling)**, utilizes much of the existing pipeline for time and efficiency sake.\n",
    "\n",
    "Lossi as a whole consists of several adaptations.<br>\n",
    "The main idea is to use a small proxy model to determine the informativeness of each cluster and then sample documents from each cluster proportionate to their informativeness. We can do this at two points during dataset assembly, which is why idea 2.1 investiagtes the first point, and idea 2.2 the first with the second point combined. \n",
    "\n",
    "What do I mean by that?\n",
    "\n",
    "**Idea 2.1** covers these adaptations:\n",
    "- Per cluster: Uniformly sample $n$ (e.g. $1,000$) documents and determine their loss with a small Pythia $70\\text{M}$ proxy model\n",
    "- Use the mean loss as a heuristic for the cluster's informativeness and weight the cluster's representation in the final dataset by this value\n",
    "\n",
    "**Idea 2.2** covers these adaptations:\n",
    "- The loss-proportional sampling information from Idea 2.1 is used to guide the cluster-wise random sampling process\n",
    "- Per cluster: Randomly sample $1.5\\times$ the amount of documents we want to end up with from each non-excluded cluster\n",
    "- Per cluster: Calculate the loss for each sampled document with a small Pythia $70\\text{M}$ proxy model which itself was pretrained halfway through (`step72000`) The Pile Deduped.\n",
    "- Per cluster: Sort the documents by their loss and select the top half of the documents with the highest loss for the final dataset\n",
    "- We continue with the dataset assembly as before after that.\n",
    "\n",
    "Note that we do this information-sorted slicing per each cluster and *not* globally as to still represent the cluster distribution in the final dataset. Otherwise, some clusters might not be represented at all anymore, because other cluster's examples just plainly induced more perplexity in the proxy.\n",
    "\n",
    "Note also that we select the smallest Pythia model trained half-way through the Pile, which assumes that the Pile is shuffled with regards to cluster assignments.<br>\n",
    "We can verify that this is the case by looking at the unsorted cluster assignment results, written in order of appearance of documents in the embedded Pile dataset.\n",
    "\n",
    "Also, I distinctly chose to use a small proxy model for this. MiniPile was intended for use in constrained academic settings, so we have to make do with small models that are most universally trainable, while not being too small as not to allow for their proxy use. Same goes for why the $70\\text{M}$ model is only trained to half of the full Pile dataset.\n",
    "\n",
    "**Effectively, Lossi is a one-shot proxy-based geometric sampling approach that is guided by a loss-based importance heuristic.**\n",
    "\n",
    "### Idea 2.1 - Loss-informed Proportionate Cluster Sampling\n",
    "\n",
    "This idea's distillation script is implemented in `03_distill_pile_embed_idea_2.1_lossi_1.py` and `03_distill_pile_embed_idea_2.1_lossi_2.py`.<br>\n",
    "Based on the $$ documents large dataset, a Pythia $160\\text{M}$ was trained for evaluation.<br>\n",
    "This idea's trained model can be found here: [https://huggingface.co/Marcus2112/pythia-160m-minipile_loss-sampled](https://huggingface.co/Marcus2112/pythia-160m-minipile_loss-sampled)\n",
    "\n",
    "The benchmark results are as follows, compared to the original MiniPile and the Recreation:\n",
    "\n",
    "| Benchmark        | Measure    |     | 160M MiniPile                | 160M Lossi 1                     | Percentage Difference of Means | 95% Confidence Interval         | Interpretation             |\n",
    "| ---------------- | ---------- | --- | ---------------------------- | -------------------------------- | ------------------------------ | ------------------------------- | -------------------------- |\n",
    "| ARC-Challenge    | acc        | ↑   | **0.2125 ± 0.0120**          | 0.1980 ± 0.0116                  | -6.8235                        | (0.0182; -0.0472)               | Different not significant  |\n",
    "| MMLU             | acc        | ↑   | **0.2699 ± 0.0037**          | 0.2295 ± 0.0035                  | -14.9685                       | (-0.0304; -0.0504)              | MiniPile-trained better    |\n",
    "| HellaSwag        | acc        | ↑   | 0.2560 ± 0.0044              | **0.2599 ± 0.0044**              | 1.5234                         | (0.0161; -0.0083)               | Different not significant  |\n",
    "| WinoGrande       | acc        | ↑   | 0.4720 ± 0.0140              | **0.5107 ± 0.0140**              | 8.1992                         | (0.0775; -0.0001)               | Difference not significant |\n",
    "| Lambada (OpenAI) | acc        | ↑   | 0.0000 ± 0.0000              | 0.0000 ± 0.0000                  | -                              | -                               | -                          |\n",
    "| Lambada (OpenAI) | perplexity | ↓   | 3033175.2693 ± 288926.5827   | **2116445.1732 ± 175403.0579**   | -30.2234                       | (-254247.7681; -1579212.4241)   | Lossi 1 severely better    |\n",
    "| Lambada (Std)    | acc        | ↑   | 0.0000 ± 0.0000              | 0.0000 ± 0.0000                  | -                              | -                               | -                          |\n",
    "| Lambada (Std)    | perplexity | ↓   | 27067951.3461 ± 2710040.1910 | **14896599.9251 ± 1366937.5470** | -44.9659                       | (-6222231.2223; -18120471.6197) | Lossi 1 severely better    |\n",
    "| BLiMP            | acc        | ↑   | 0.5194 ± 0.0018              | **0.5492 ± 0.0017**              | 5.7374                         | (0.0347; 0.0249)                | Lossi 1 better             |\n",
    "\n",
    "| Benchmark        | Measure    |     | 160M Recreation                  | 160M Lossi 1                 | Percentage Difference of Means | 95% Confidence Interval      | Interpretation             |\n",
    "| ---------------- | ---------- | --- | -------------------------------- | ---------------------------- | ------------------------------ | ---------------------------- | -------------------------- |\n",
    "| ARC-Challenge    | acc        | ↑   | 0.1894 ± 0.0115                  | **0.1980 ± 0.0116**          | 4.5407                         | (0.0406; -0.0234)            | Difference not significant |\n",
    "| MMLU             | acc        | ↑   | 0.2295 ± 0.0035                  | 0.2295 ± 0.0035              | 0.0000                         | (0.0097; -0.0097)            | Difference not significant |\n",
    "| HellaSwag        | acc        | ↑   | **0.2604 ± 0.0044**              | 0.2599 ± 0.0044              | -0.1920                        | (0.0117; -0.0127)            | Difference not significant |\n",
    "| WinoGrande       | acc        | ↑   | **0.5122 ± 0.0140**              | 0.5107 ± 0.0140              | -0.2929                        | (0.0373; -0.0403)            | Difference not significant |\n",
    "| Lambada (OpenAI) | acc        | ↑   | 0.0000 ± 0.0000                  | 0.0000 ± 0.0000              | -                              | -                            | -                          |\n",
    "| Lambada (OpenAI) | perplexity | ↓   | **1854408.3999 ± 148101.5978**   | 2116445.1732 ± 175403.0579   | 14.1305                        | (711985.1414; -187911.5948)  | Difference not significant |\n",
    "| Lambada (Std)    | acc        | ↑   | 0.0000 ± 0.0000                  | 0.0000 ± 0.0000              | -                              | -                            | -                          |\n",
    "| Lambada (Std)    | perplexity | ↓   | **11927123.2514 ± 1063672.9280** | 14896599.9251 ± 1366937.5470 | 24.8968                        | (6364250.0618; -425296.7144) | Difference not significant |\n",
    "| BLiMP            | acc        | ↑   | 0.5481 ± 0.0017                  | **0.5492 ± 0.0017**          | 0.2007                         | (0.0058; -0.0036)            | Difference not significant |\n",
    "\n",
    "We observe a clear improvement over the original MiniPile. However, we can't attribute that to the loss-informed sampling approach, as the Recreation dataset is remarkably equal in performance. This implies that the measures taken unwittingly during the recreation process pose a more effective set of improvements than the loss-informed sampling approach itself. These measures include:\n",
    "\n",
    "- The embedding process, which during the Recreation uses a slightly worse model than the original MiniPile, but processes larger context sizes for the embeddings (1024 instead of 512)\n",
    "- The clustering process, which may induce different cluster formations based on the different embeddings\n",
    "- The interpretability inherent in the cluster exclusion process, which may have led to a more effective dataset\n",
    "\n",
    "This can imply that changes in the embedding and clustering process may be more effective than the loss-informed sampling approach itself.<br>\n",
    "I conclude that it is quite surprising that scaling the cluster proportions based on perceived informativeness doesn't yield a clear improvement over the uniform sampling approach. This may be due to the fact that the proxy model is not able to capture the full complexity of the dataset, and thus the loss-informed sampling approach may not be able to capture the most informative examples. This in turn would put the loss-informed sampling approach at a disadvantage compared to the uniform sampling approach due to decreased efficiency.\n",
    "\n",
    "### Idea 3 - Density-based Proportionate Cluster Sampling\n",
    "\n",
    "An inherent danger with loss-based informativeness approximation is the potential for skewing the dataset towards harder examples, which could make the dataset less representative of general tasks. While loss can be considered a related indicator, it is not a direct measure of informativeness.\n",
    "\n",
    "Taking a step back with this insight, we should re-focus that MiniPile's goal is to capture the most representative subset of the 'insights' gainable from the original Pile dataset. To achieve that, we can look at diversity, which could improve generalization, especially when requiring broad informational coverage.\n",
    "\n",
    "Specifically, sampling inversely proportional to the cluster density will prioritize sparse regions of the dataset and thus the gaining of a broad coverage of the dataset's information. However, as lunch is never free, over-sampling/representing sparse clusters will likely introduce noise, especially when cluster sparsity correlates with low-quality examples. One could argue that we already excluded the most uninteresting, thus noise-inducing clusters, but the potential for noise itself is in no way mitigated (/mitigatable?). Density-based sampling therefore has to be understood as a trade-off between spread for representation and accidental capturing of noise.\n",
    "\n",
    "And still, density-based sampling could be helpful in capturing a most diverse set of examples, which in turn could be beneficial for generalization.\n",
    "But, we have to find a way to mitigate noise and over-representation at least to some extent.\n",
    "\n",
    "I therefore propose a density-based sampling approach that calculates cluster contribution proportions like so:\n",
    "$$\\text{Cluster Proportion} = \\frac{|C_i|}{|\\bigcup_{j} C_j|} \\cdot (1 - \\omega \\cdot \\rho(C_i))$$\n",
    "\n",
    "where $|C_i|$ is the number of documents in cluster $i$, $|\\bigcup_{j} C_j|$ is the total number of documents in all clusters, and $\\rho(C_i)$ is the density of cluster $i$. The impact of the density is scaled by the hyperparameter $\\omega$, reducing the factor of over-representation of thoroughly sparse clusters.<br>\n",
    "I set $\\omega = 0.5$ with the intention of having neither cluster size nor density completely dominate the proportion calculation.\n",
    "\n",
    "### Idea 4 - Hierarchical Embedding through Sparse Sampling (HESS)\n",
    "\n",
    "A fourth idea, named HESS, aims to improve the dataset creation process entirely without the need for a proxy model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Evaluate Pythia $160\\text{M}$ SuperMiniPile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Evaluate Pythia $1.4\\text{B}$ Pretrained vs. Pythia $1.4\\text{B}$ SuperMiniPile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "def average_pool(last_hidden_states, attention_mask):\n",
    "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_embedding(text, tokenizer, model):\n",
    "    inputs = tokenizer(f\"query: {text}\", max_length=1024, padding=\"max_length\", truncation=True, return_tensors='pt')\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = average_pool(outputs.last_hidden_state, inputs['attention_mask'])\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "    return embeddings.cpu().numpy()[0]\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_path = \"/mnt/data/e5-base-4k\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n",
    "model = AutoModel.from_pretrained(model_path, attn_implementation=\"sdpa\")\n",
    "model.eval()\n",
    "model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Directory containing parquet files\n",
    "parquet_dir = Path(\"/mnt/data/Pile_Deduplicated_Embd\")\n",
    "\n",
    "for i in range(3):\n",
    "    parquet_file = parquet_dir / f\"shard_{i:09d}.parquet\"\n",
    "    if not parquet_file.exists():\n",
    "        print(f\"File {parquet_file} not found. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    dataset = load_dataset(\"parquet\", data_files=str(parquet_file))[\"train\"]\n",
    "    print(f\"Iterating over shard {i}....\")\n",
    "    for entry in tqdm(dataset):\n",
    "        stored_embedding = np.array(entry['embedding'])\n",
    "        text = entry['text']\n",
    "        # Generate new embedding\n",
    "        new_embedding = get_embedding(text, tokenizer, model)\n",
    "        # Convert both for fp16 precision\n",
    "        stored_embedding_fp16 = stored_embedding.astype(np.float16)\n",
    "        new_embedding_fp16 = new_embedding.astype(np.float16)\n",
    "        cosine_similarity = np.dot(stored_embedding_fp16, new_embedding_fp16) / (np.linalg.norm(stored_embedding_fp16) * np.linalg.norm(new_embedding_fp16))\n",
    "        # 1.0 as in identical, 0.0 as in orthogonal\n",
    "        if cosine_similarity != 1.0:\n",
    "            print(f\"Text: {text}\")\n",
    "            print(f\"Cosine similarity: {cosine_similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Specify the path to your Parquet file\n",
    "data_files_1 = {\"train\": \"/vol/tmp/koppelmm/Pile_Deduplicated_Embd/shard_000000001.parquet\"}\n",
    "data_files_2 = {\"train\": \"/vol/tmp/koppelmm/Pile_Deduplicated_Embd/shard_000000002.parquet\"}\n",
    "\n",
    "# Load the dataset\n",
    "dataset_1 = load_dataset(\"parquet\", data_files=data_files_1, split=\"train\")\n",
    "dataset_2 = load_dataset(\"parquet\", data_files=data_files_2, split=\"train\")\n",
    "\n",
    "# Print the number of entries in the dataset\n",
    "print(f\"Number of entries in shard 0: {len(dataset_1)} ({128*8192})\")\n",
    "print(f\"Number of entries in shard 1: {len(dataset_2)} ({128*8192})\")\n",
    "\n",
    "# Print the first entry from each dataset\n",
    "first_entry_1 = dataset_1[0]\n",
    "first_entry_2 = dataset_2[0]\n",
    "\n",
    "print(\"First entry in shard 0:\")\n",
    "print(first_entry_1)\n",
    "\n",
    "print(\"First entry in shard 1:\")\n",
    "print(first_entry_2)\n",
    "\n",
    "# Check if the first entries are different\n",
    "if first_entry_1 == first_entry_2:\n",
    "    print(\"The first entries are identical.\")\n",
    "else:\n",
    "    print(\"The first entries are different.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Unexpected heap contents: [{'value': 95}, {'value': 96}, {'value': 97}, {'value': 98}, {'value': 99}] vs. expected [{'value': 5}, {'value': 6}, {'value': 7}, {'value': 8}, {'value': 9}]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 36\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m result \u001b[38;5;241m==\u001b[39m expected, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected heap contents: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll tests passed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m \u001b[43mtest_update_heap\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 21\u001b[0m, in \u001b[0;36mtest_update_heap\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m result \u001b[38;5;241m=\u001b[39m [entry[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(heap, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m0\u001b[39m])]\n\u001b[1;32m     19\u001b[0m expected \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m: i} \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m)]\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m result \u001b[38;5;241m==\u001b[39m expected, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected heap contents: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m vs. expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Test descending order heap (keep smallest items based on `value`)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m heap \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mAssertionError\u001b[0m: Unexpected heap contents: [{'value': 95}, {'value': 96}, {'value': 97}, {'value': 98}, {'value': 99}] vs. expected [{'value': 5}, {'value': 6}, {'value': 7}, {'value': 8}, {'value': 9}]"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "\n",
    "def update_heap(heap, item, key_func, max_size=5, reverse=False):\n",
    "    key = key_func(item)\n",
    "    heapq.heappush(heap, (key if not reverse else -key, item))\n",
    "    if len(heap) > max_size:\n",
    "        heapq.heappop(heap)\n",
    "\n",
    "def test_update_heap():\n",
    "    # Test ascending order heap (keep largest items based on `value`)\n",
    "    heap = []\n",
    "    items = [{'value': i} for i in range(100)]  # Items with values from 0 to 9\n",
    "\n",
    "    for item in items:\n",
    "        update_heap(heap, item, key_func=lambda x: x['value'], max_size=5, reverse=False)\n",
    "\n",
    "    # Extract the final items from the heap, sorted by `value`\n",
    "    result = [entry[1] for entry in sorted(heap, key=lambda x: x[0])]\n",
    "    expected = [{'value': i} for i in range(5, 10)]\n",
    "\n",
    "    assert result == expected, f\"Unexpected heap contents: {result} vs. expected {expected}\"\n",
    "\n",
    "    # Test descending order heap (keep smallest items based on `value`)\n",
    "    heap = []\n",
    "    for item in items:\n",
    "        update_heap(heap, item, key_func=lambda x: x['value'], max_size=5, reverse=True)\n",
    "\n",
    "    # Extract the final items from the heap, sorted by `value`\n",
    "    result = [entry[1] for entry in sorted(heap, key=lambda x: x[0], reverse=True)]\n",
    "    expected = [{'value': i} for i in range(5)]\n",
    "\n",
    "    assert result == expected, f\"Unexpected heap contents: {result}\"\n",
    "\n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "test_update_heap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import cpu_count\n",
    "\n",
    "print(cpu_count() // 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minipile",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
