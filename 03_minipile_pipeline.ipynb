{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recreating and Improving MiniPile Dataset Creation\n",
    "\n",
    "**Objectives:**\n",
    "- [.] Implement and verify MiniPileâ€™s filtering pipeline according to [Kaddour (2023)](https://arxiv.org/abs/2304.08442), but intended for decoder-only model use\n",
    "- [] Evaluate and compare performances of Pythia $160\\text{M}$ pretrained on The Pile vs. trained on the *newly, self-created MiniPile* on MMLU and ARC-Challenge\n",
    "- [] Improve the dataset creation process, create new SuperMiniPile dataset (ideally smaller and more information-retaining)\n",
    "- [] Train Pythia $160\\text{M}$ on SuperMiniPile, evaluate on MMLU and ARC-Challenge\n",
    "- [] Evaluate and compare performances of Pythia $1.4\\text{B}$ pretrained on The Pile vs. trained on the created MiniPile on the MMLU and ARC benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset,  Dataset\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from huggingface_hub import snapshot_download\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/mnt/data\"\n",
    "base_path = Path(base_dir)\n",
    "\n",
    "def download_model(down_dir: str, target_folder: str, cache_folder: str, repo_id: str, branch: str = \"main\") -> None:\n",
    "    down_dir = Path(down_dir)\n",
    "    target_dir = down_dir / target_folder\n",
    "    cache_dir = down_dir / cache_folder\n",
    "\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"Downloading {repo_id}/{branch}...\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            snapshot_download(\n",
    "                repo_id,\n",
    "                repo_type=\"model\",\n",
    "                revision=branch,\n",
    "                cache_dir=str(cache_dir),\n",
    "                local_dir=str(target_dir)\n",
    "            )\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Download attempt failed: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Recreating The MiniPile Dataset Creation Pipeline\n",
    "\n",
    "(1) document embedding extraction,<br>\n",
    "(2) clustering of embeddings, and<br>\n",
    "(3) human-guided exclusion of unwanted clusters\n",
    "\n",
    "- 22 data subset sources\n",
    "- 5.91 KiB mean document size (before deduplication)\n",
    "\n",
    "### Document Embedding Extraction\n",
    "\n",
    "- MiniPile paper uses term \"document\": I assume as they are quite large, this refers to individual training examples from \"The Pile-Deduplicated\"\n",
    "- \"The Pile Deduplicated\" predominantly contains english text, as stated in the Pile paper\n",
    "- `E5-Large` does not require performing sentence-splitting beforehand, I was misguided by the example code at https://huggingface.co/intfloat/e5-large\n",
    "- I will use `E5-Large` with one \"sentence\" actually being one \"document\" for MiniPile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading intfloat/e5-large/main...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58042febeed5484cadf2d0c35fa8f40d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "854aaf3d2d274dd6b377f7f38677c869",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1650 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Starting point is the deduplicated The Pile\n",
    "# Infer embeddings for all documents using E5-Large\n",
    "\n",
    "# https://huggingface.co/intfloat/e5-large\n",
    "download_model(down_dir=base_dir, target_folder=\"e5-large\", \n",
    "               cache_folder=\"e5-large_Cache\",\n",
    "               repo_id=\"intfloat/e5-large\") # Chose this because nothing beyond E5-Large was specified\n",
    "\n",
    "e5_large = SentenceTransformer(str(base_path / \"e5-large\"), local_files_only=True) # no .from_pretrained() here\n",
    "\n",
    "# https://huggingface.co/datasets/EleutherAI/the_pile_deduplicated\n",
    "pile_dedup = load_dataset(\"parquet\",\n",
    "                          data_files={\n",
    "                              \"train\": str(base_path / \"Pile_Deduplicated\" / \"data\" / \"train-*.parquet\"),\n",
    "                          },\n",
    "                          cache_dir=str(base_path / \"MiniPile_Cache\"),\n",
    "                          split=\"train\",\n",
    "                          streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the model and the local-stream for The Pile, we iterate through the dataset and extract the embeddings for each document.<br>\n",
    "For convenience and later processing, we will assemble an embedding dataset.\n",
    "\n",
    "Thing is, when creating the embedding dataset, we need to make sure that embedding indices match the document indices in the original dataset.<br>\n",
    "This is strictly necessary for the filtering step later on.<br>\n",
    "To ensure the above code's resulting embedding dataset is correctly aligned with the original dataset, I ran the following code for a small subset of `16384` documents:\n",
    "\n",
    "```python\n",
    "saved_embeddings = load_dataset(\"parquet\", data_files=str(embd_dir / \"shard_*.parquet\"), split=\"train\")\n",
    "\n",
    "for index in tqdm(range(len(saved_embeddings)), desc=\"Verifying embeddings\"):\n",
    "    # Newly embed the text at this index\n",
    "    original_text = next(iter(pile_dedup.skip(index).take(1)))['text']\n",
    "    generated_embedding = e5_large.encode(original_text, show_progress_bar=False)\n",
    "    # Newly encoded embedding should correspond to the saved embedding at this index -> index consistency\n",
    "    saved_embedding = saved_embeddings[index]['embedding']\n",
    "    if not np.allclose(generated_embedding, saved_embedding, atol=1e-5):\n",
    "        print(f\"Mismatch found at index: {index}\")\n",
    "```\n",
    "No mismatches were found, which means we can scale the embedding set creation to the full dataset.\n",
    "\n",
    "I know had a bit of a problem.<br>\n",
    "Embedding \"The Pile Deduplicated\" with `E5-Large` will foreseeably require *a lot* of time and resources.<br>\n",
    "But, the embedding will possibly be needed in raw format at multiple stages: For recreating the MiniPile dataset, and also as (potential) basis for the SuperMiniPile improvement.<br>\n",
    "I am thinking that if an error occurs during either the embedding or clustering phase, it could complicate recovery efforts.\n",
    "Therefore, I see more flexibility in creating the embedding dataset *separately* and then continuing with it for clustering and filtering.\n",
    "\n",
    "This is the code I tested with the above code snippet and ultimately used to build the `Pile_Deduplicated_Embedded` dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Took the example code from the intfloat/e5-large page\n",
    "embd_dir = base_path / Path(\"Pile_Deduplicated_Embeddings\")\n",
    "embd_dir.mkdir(exist_ok=True)\n",
    "\n",
    "batch_size = 1024\n",
    "shard_size = batch_size ** 2  # shard embed count upper bound\n",
    "\n",
    "embedding_shard = []\n",
    "shard_index = 0\n",
    "\n",
    "def save_shard(embeddings, output_dir, shard_index):\n",
    "    shard_path = output_dir / f\"shard_{shard_index:09d}.parquet\"\n",
    "    dataset = Dataset.from_dict({\"embedding\": embeddings})\n",
    "    dataset.to_parquet(str(shard_path))\n",
    "\n",
    "# Didn't know tqdm could be used like that\n",
    "for batch_idx, batch in tqdm(enumerate(pile_dedup.iter(batch_size=batch_size))):\n",
    "    batch_embds = e5_large.encode(batch['text'], show_progress_bar=True) # Set this to False, good for debug but clutters like hell\n",
    "    embedding_shard.extend(batch_embds)\n",
    "    \n",
    "    if len(embedding_shard) >= shard_size:\n",
    "        save_shard(embedding_shard, embd_dir, shard_index)\n",
    "        shard_index += 1\n",
    "        embedding_shard = []\n",
    "\n",
    "# Append remaining\n",
    "if embedding_shard != []:\n",
    "    save_shard(embedding_shard, embd_dir, shard_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ran this code as the script `03_embed_pile_dedup.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering of Embeddings\n",
    "\n",
    "- Batchified $k$-means clustering, a term only used in the MiniPile paper: This must stand for **mini-batch k-means clustering**\n",
    "- Cosine distance between normalized embeddings\n",
    "- Cluster Count of $k=220$ ($10$ clusters per source)\n",
    "- Batch size $16384$ (I made sure, once again, yes, MiniPile uses \"The Pile Deduplicated\"; this is slowly getting to me)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple\n",
    "from datasets import load_dataset, Dataset\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "class CosineMiniBatchKMeans(MiniBatchKMeans):\n",
    "    def _partial_fit(self, X: np.ndarray, y: Optional[np.ndarray] = None) -> MiniBatchKMeans:\n",
    "        X_normalized = normalize(X, norm='l2', axis=1)\n",
    "        return super()._partial_fit(X_normalized, y)\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        X_normalized = normalize(X, norm='l2', axis=1)\n",
    "        return super().predict(X_normalized)\n",
    "\n",
    "def cluster_embd_batch(batch_embeddings: np.ndarray,kmeans: CosineMiniBatchKMeans,training: bool = True) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    cluster_ids = kmeans._partial_fit(batch_embeddings) if training else kmeans.predict(batch_embeddings)\n",
    "    distances = np.sum(normalize(batch_embeddings, norm='l2', axis=1) * normalize(kmeans.cluster_centers_, norm='l2', axis=1)[cluster_ids], axis=1) # Cosine distances to assigned centroids\n",
    "    return cluster_ids, distances\n",
    "\n",
    "base_dir = \"/mnt/data\"\n",
    "embd_dir = Path(base_dir) / \"Pile_Deduplicated_Embeddings\"\n",
    "output_dir = Path(base_dir) / \"Pile_Deduplicated_Clusters\"\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "k_clusters = 220\n",
    "batch_size = 16384\n",
    "\n",
    "embeddings = load_dataset(\"parquet\",\n",
    "                          data_files=str(embd_dir / \"shard_*.parquet\"),\n",
    "                          split=\"train\",\n",
    "                          streaming=True)\n",
    "\n",
    "kmeans = CosineMiniBatchKMeans(n_clusters=k_clusters,\n",
    "                               batch_size=batch_size,\n",
    "                               random_state=42,\n",
    "                               init='k-means++',\n",
    "                               max_iter=100,\n",
    "                               verbose=1)\n",
    "\n",
    "# First pass: Train the clustering model\n",
    "print(\"Training clustering model...\")\n",
    "for batch in tqdm(embeddings.iter(batch_size=batch_size)):\n",
    "    batch_embeddings = np.array(batch['embedding'])\n",
    "    _ = cluster_embd_batch(batch_embeddings, kmeans, training=True)\n",
    "\n",
    "# Second pass: Assign clusters and compute distances\n",
    "print(\"\\nAssigning clusters and computing distances...\")\n",
    "\n",
    "current_shard = []\n",
    "shard_size = batch_size * 16 # Adjust based on available memory\n",
    "shard_idx = 0\n",
    "\n",
    "for batch in tqdm(embeddings.iter(batch_size=batch_size)):\n",
    "    batch_embeddings = np.array(batch['embedding'])\n",
    "    cluster_ids, distances = cluster_embd_batch(batch_embeddings,\n",
    "                                                kmeans,\n",
    "                                                training=False)\n",
    "    \n",
    "    current_shard.extend([{\"embedding\": emb,\n",
    "                           \"cluster_id\": int(cid),\n",
    "                           \"centroid_distance\": float(dist)} for emb, cid, dist in zip(batch_embeddings, cluster_ids, distances)])\n",
    "    \n",
    "    if len(current_shard) >= shard_size:\n",
    "        shard_path = output_dir / f\"clustered_shard_{shard_idx:09d}.parquet\"\n",
    "        Dataset.from_list(current_shard).to_parquet(str(shard_path))\n",
    "        current_shard = []\n",
    "        shard_idx += 1\n",
    "\n",
    "# Save remaining\n",
    "if current_shard:\n",
    "    shard_path = output_dir / f\"clustered_shard_{shard_idx:09d}.parquet\"\n",
    "    Dataset.from_list(current_shard).to_parquet(str(shard_path))\n",
    "\n",
    "# Save clustering model itself\n",
    "np.save(output_dir / \"cluster_centroids.npy\",\n",
    "        kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Improve the Dataset Creation Process\n",
    "\n",
    "Ideas:\n",
    "\n",
    "- DBSCAN-like clustering for better focus on outliers, generalizability of the approach beyond \"The Pile Deduplicated\"\n",
    "- weighted $k$-means to account for different subset sizes and complexities\n",
    "- \"Findings indicate that it is not the proportion of tokens occupied by high-utility data that aids acquisition, but rather the proportion of training steps assigned to such data\" [On the effect of curriculum learning with developmental data for grammar acquisition (Opper, et al. 2023)](https://aclanthology.org/2023.conll-babylm.pdf)\n",
    "\n",
    "- https://openreview.net/pdf?id=7D5EECbOaf9\n",
    "- https://arxiv.org/pdf/2406.03057\n",
    "- https://arxiv.org/pdf/2210.15809\n",
    "- https://arxiv.org/pdf/2204.08499\n",
    "- https://arxiv.org/pdf/2303.09540\n",
    "- https://arxiv.org/pdf/2308.12284"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minipile",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
