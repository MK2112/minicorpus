{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recreating and Improving MiniPile Dataset Creation\n",
    "\n",
    "**Objectives:**\n",
    "- [x] Implement and verify MiniPile’s filtering pipeline according to [Kaddour (2023)](https://arxiv.org/abs/2304.08442), but intended for decoder-only model use\n",
    "- [x] Evaluate and compare performances of Pythia $160\\text{M}$ pretrained on The Pile vs. trained on the *newly, self-created MiniPile* on MMLU and ARC-Challenge\n",
    "- [.] Evaluate and compare performances of Pythia $1.4\\text{B}$ pretrained on The Pile vs. trained on the *newly, self-created MiniPile* on MMLU and ARC-Challenge\n",
    "- [.] Improve the dataset creation process, create new SuperMiniPile dataset (ideally smaller and more information-retaining)\n",
    "- [] Evaluate Pythia $160\\text{M}$ on SuperMiniPile on MMLU and ARC-Challenge\n",
    "- [] Evaluate and compare performances of Pythia $1.4\\text{B}$ pretrained on The Pile vs. trained on SuperMiniPile on the MMLU and ARC benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import snapshot_download\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "base_dir = \"/vol/tmp/koppelmm\"\n",
    "base_path = Path(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_model(down_dir: str, target_folder: str, cache_folder: str, repo_id: str, branch: str = \"main\") -> None:\n",
    "    down_dir = Path(down_dir)\n",
    "    target_dir = down_dir / target_folder\n",
    "    cache_dir = down_dir / cache_folder\n",
    "\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"Downloading {repo_id}/{branch}...\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            snapshot_download(\n",
    "                repo_id,\n",
    "                repo_type=\"model\",\n",
    "                revision=branch,\n",
    "                cache_dir=str(cache_dir),\n",
    "                local_dir=str(target_dir)\n",
    "            )\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Download attempt failed: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Recreating The MiniPile Dataset Creation Pipeline\n",
    "\n",
    "(1) document embedding extraction,<br>\n",
    "(2) clustering of embeddings, and<br>\n",
    "(3) human-guided exclusion of unwanted clusters<br>\n",
    "(4) mini-pile distillation\n",
    "\n",
    "- 22 data subset sources\n",
    "- 5.91 KiB mean document size (before deduplication)\n",
    "\n",
    "### Document Embedding Extraction\n",
    "\n",
    "- MiniPile paper uses term \"document\": This refers to individual training examples from \"The Pile-Deduplicated\"\n",
    "- \"The Pile Deduplicated\" predominantly contains english text, as stated in the Pile paper\n",
    "- `E5-Large` does not require performing sentence-splitting beforehand, I was misguided by the example code at https://huggingface.co/intfloat/e5-large\n",
    "- `E5-Large` scales poorly to the dataset size under the conditions imposed by the HU Berlin cluster. I will use `E5-Base-4k` instead.\n",
    "- `E5-Base-4k` performs slightly worse than `E5-Large`, but has roughly half the parameter count and is therefore more efficient to use\n",
    "- We attempt to mitigate the reported/expectable performance losses by using a larger text window size of $1024$ tokens instead of the $512$ tokens used by `E5-Large` by default.\n",
    "\n",
    "Given the smaller model and The Pile, we iterate through the dataset and extract the embedding for each document.<br>\n",
    "The script I initially implemented for the embedding step is `03_embed_pile_dedup.py`.<br>\n",
    "The approach layed out therein conceptually worked, but it had to be thoroughly memory-optimized to run for as long as needed for our Pile dataset.<br>\n",
    "The optimized script I ultimately ran for this step is `03_embed_pile_dedup_turbo.py`.\n",
    "\n",
    "The embedding step produces as artifact a copy of the original dataset with the embeddings added as a column.<br>\n",
    "The embedding process is resumable, results are persisted in multiple parquet files, one after another, in the folder `Pile_Deduplicated_Embd`.<br>\n",
    "\n",
    "**Note that I intended to upload this embedded version of the Pile to HuggingFace for strict reproducibility.**<br>\n",
    "**This idea was cut short by a change in HuggingFace's pricing policy, effective January 2025, prohibiting the free sharing of datasets >500GB - a threshold which this new dataset crosses (801 GB) due to the added embeddings.**\n",
    "\n",
    "Furthermore, note that the embedding step, through its parallel processing, is in no way guaranteed to maintain the original order of the documents.<br>\n",
    "In fact, this is the reason for why I elected to build the dataset copy with the embeddings as a column in the first place, to ensure the correct alignment of the embeddings with the documents while still being able to leverage parallel processing. Because, after all, time was the deciding factor.<br>\n",
    "The shuffling is therefore not a problem as such, but we have to base the clustering and therefore all following processes on this new, shuffled dataset.\n",
    "\n",
    "In other words, all artifacts produced after the embedding step will relate not to the original dataset, but the embedded dataset, e.g. when referring to entries by index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering of Embeddings\n",
    "\n",
    "- Batchified $k$-means clustering, a term only used in the MiniPile paper: This must stand for **mini-batch k-means clustering**\n",
    "- Cosine distance between normalized embeddings\n",
    "- Cluster Count of $k=220$ ($10$ clusters per source)\n",
    "- Batch size $16384$\n",
    "\n",
    "Architecturally, I built the clustering step to be fully independent of the embedding step, so as to be able to run the partial fitting concurrently with the latter, saving ~4 days of processing time total.\n",
    "\n",
    "The clustering step is implemented in `03_cluster_pile_embed.py`. As soon as the embedding step finishes, a text file is produced, signaling the clustering step to conclude model fitting and start predicting. The centroids are saved and can be found in `MiniPile_BatchKMeans/cluster_centers.npy`. Intermediary centroid results have been omitted from this repository.\n",
    "\n",
    "Each embedding from the newly created dataset is assigned to one of the $220$ clusters. This cluster information as well as the distance of the data point to the centroid are stored in JSONL files, with the entries in order of appearance in the embedded Pile dataset.<br>\n",
    "Each clustering result looks like this: `{\"idx\": 0, \"cluster\": 5, \"distance\": 0.20949329195756128}`.<br>\n",
    "Each Pile-Embedded document is referred to only by its index, crunching the cluster results' memory requirements down.\n",
    "\n",
    "Beyond the JSONL files containing the cluster assignments, the clustering step produces a verification file `MiniPile_BatchKMeans/cluster_results_metadata.json` to indicate whether all chunks and all data points therein have been processed and how results have been saved. We can see from this file that the original dataset size of $134,318,121$ documents has been captured and thus processed, lending more credibility to the clustering results.\n",
    "\n",
    "Additionally, the clustering step produces a file `MiniPile_BatchKMeans/cluster_info_for_inspection.json`.<br>\n",
    "Per cluster index, this file contains the following information:\n",
    "- `closest`: The top 5 closest documents to the cluster centroid\n",
    "    - `text`: The associated text excerpt (for memory reasons)\n",
    "    - `distance`: The cosine distance to the cluster centroid\n",
    "- `farthest`: The top 5 farthest documents from the cluster centroid (again, excerpts, in same format)\n",
    "- `total_examples`: The number of documents assigned to this cluster\n",
    "- `average_distance`: The average cosine distance of all documents to the cluster centroid\n",
    "- `sum_distance`: The sum of all cosine distances of all documents to the cluster centroid\n",
    "\n",
    "The three latter information points are intended to help with the human-guided exclusion of unwanted clusters, but moreover, they may help in improving the dataset creation process later on, as they can provide insights into the spread of the data.\n",
    "\n",
    "After these files were attained, I ran `03_sort_pile_clusters.py` to save the cluster assignment entries organized into one file per cluster / separated by cluster. This is to facilitate a more effective cluster exclusion process during later minipile creation steps.\n",
    "\n",
    "For now, the clustering step is concluded, producing one JSONL file per cluster with information about each cluster assignment per document.<br>\n",
    "This intermediary dataset can be found here: [https://huggingface.co/datasets/Marcus2112/pile_dedup_embeddings_clusters](https://huggingface.co/datasets/Marcus2112/pile_dedup_embeddings_clusters).\n",
    "\n",
    "Note that while the entry count is exactly identical, the size is not. Where the original MiniPile occupied $3.14GB$, we now require $3.71GB$.\n",
    "Several factors play into this:\n",
    "- We use a slightly different embedding model, which may cause (at least occasional) devations in embedding and thus may lead to differently shaped clusters\n",
    "- We select clusters by hand, allowing interpretation of selection categories to roam free. I thus may have selected $38$ clusters according to the paper's categories, but my understanding of enforcing them during selection may differ from the author.\n",
    "- The random sampling happened to select different examples per cluster.\n",
    "\n",
    "Again, note that these indices are not per-se applicable to the original dataset, but to the embedded dataset.\n",
    "\n",
    "### Human-Guided Cluster Exclusion\n",
    "\n",
    "At this point, especially due to the `MiniPile_BatchKMeans/cluster_info_for_inspection.json` file, we can start the human-guided exclusion of unwanted clusters.<br>\n",
    "I strictly adhered to the paper and only sorted out clusters of the layed out categories, which I found to be well identifyable through the 10 examples per cluster.\n",
    "\n",
    "The categories with the clusters I sorted out are as follows:\n",
    "- Near Duplicates ($10, 15, 16, 22, 26, 28, 35, 37, 46, 51, 57, 64, 86, 87, 64, 102, 111, 114, 152, 163, 166, 218$)\n",
    "- Pornography ($167$)\n",
    "- Navigation Bars ($39, 88, 101, 155$)\n",
    "- Product Specifications ($61, 200$)\n",
    "- Long lists of named entities ($40, 44, 78, 90, 99, 103, 181, 196, 219$)\n",
    "\n",
    "### MiniPile Distillation\n",
    "\n",
    "With the cluster analysis concluded, we can now proceed to the distillation of the MiniPile dataset.<br>\n",
    "This dataset touches on all the artifacts produced in the previous steps:<br>\n",
    "- The embedded Pile dataset, from which we extract the documents\n",
    "- The cluster assignments, from which we extract the documents assigned to the remaining clusters\n",
    "- The cluster exclusion list, from which we exclude the documents assigned to the unwanted clusters\n",
    "\n",
    "The distillation step is implemented in `03_distill_pile_embed.py`.<br>\n",
    "The script is written to most exactly and efficiently (we have loads of I/O to perform) extract the correct documents according to our random sampling across the remaining clusters.\n",
    "\n",
    "The distillation step produces a new dataset, the MiniPile, which is a subset of the original Pile dataset.<br>\n",
    "The created dataset is exactly $1,010,500$ documents large, as intended. The dataset is shuffled, to spread cluster entries evenly across the dataset's splits.<br>\n",
    "Additionally, I added a column `pile_idx` to each entry, denoting the original index of the document in the Pile dataset.<br>\n",
    "This helped in making sure that the dataset actually captures a subset derived from across the entire embedded Pile.\n",
    "\n",
    "The resulting self-created MiniPile can be found here: [https://huggingface.co/datasets/Marcus2112/minipile_recreation](https://huggingface.co/datasets/Marcus2112/minipile_recreation).\n",
    "\n",
    "I now went on and adapted the `02_train_160M.py` script to train Pythia $160\\text{M}$ on the newly created MiniPile.<br>\n",
    "The adapted script is `03_train_160M_recreation.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Evaluate Pythia $160\\text{M}$ Pile vs. Pythia $160\\text{M}$ MiniPile (recreated)\n",
    "\n",
    "We will use an exact copy of the training and the test setup previously used for benchmarking Pythia $160\\text{M}$-Pile and $160\\text{M}$-MiniPile-Original.<br>\n",
    "Training is performed with the script `03_train_160M_recreation.py`.\n",
    "\n",
    "The trained model can be found here: [https://huggingface.co/Marcus2112/pythia-160m-minipile_recreation](https://huggingface.co/Marcus2112/pythia-160m-minipile_recreation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from lm_eval import utils, simple_evaluate\n",
    "from lm_eval.models.huggingface import HFLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluation - Pythia 160M Trained on Self-Created MiniPile\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "pythia_minipile = AutoModelForCausalLM.from_pretrained(base_path / \"pythia160m_minipile_Recreation_trained\", local_files_only=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_path / \"pythia160m_dedup_untrained\", use_fast=True, local_files_only=True) # Use exact same tokenizer\n",
    "pythia_minipile = pythia_minipile.to(device)\n",
    " \n",
    "batch_size_hflm = 1\n",
    "\n",
    "pythia_minipile_hflm = HFLM(pretrained=pythia_minipile,\n",
    "                        tokenizer=tokenizer,\n",
    "                        batch_size=batch_size_hflm)\n",
    "\n",
    "results = simple_evaluate(model=pythia_minipile_hflm,\n",
    "                          tasks=[\"arc_challenge\", \"mmlu\", \"winogrande\", \"hellaswag\", \"lambada\", \"blimp\"],\n",
    "                          num_fewshot=0,\n",
    "                          batch_size=batch_size_hflm,\n",
    "                          device=\"cuda\",\n",
    "                          limit=None)\n",
    "\n",
    "with open('03_eval_160M_minipile_recreation.txt', 'w') as f:\n",
    "    f.write(str(results))\n",
    "\n",
    "print(utils.make_table(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can consider this point a moment of truth for the study project.\n",
    "\n",
    "If benchmark scores indicate significant deviation, a multitude of causes could be the reason, e.g.:\n",
    "- Embedding process may be logically flawed\n",
    "- Switch for `e5-base-4k` may not pay off when looking at embedding accuracy (our gamble for performance didn't work)\n",
    "- Clustering process may be logically flawed\n",
    "- Hand-selected clusters do not match the categories as layed out by the paper (interpretative difference mount to fundamentally different dataset characteristics)\n",
    "- The index-based logical system used for saving memory and processing time may be inconsistent, leading us to sample wildly inaccurately\n",
    "- The paper may not have been clear enough and actually clusters were sampled from with regards to their size, and not flat out equal sample counts across each of them\n",
    "- All or some of the above.\n",
    "\n",
    "However, if benchmark scores *do not* indicate significant deviation, this implies:\n",
    "- The approach as layed out by the paper is reproducible\n",
    "- The pipeline built until now works and can be safely improved upon\n",
    "- The project from here on out may shift towards optimization and generalization, contributing to furthering the working approach\n",
    "\n",
    "I split the results comparison into two parts. The first table compares the just now trained `160M Recreation` against `160M Pile Deduplicated`, while the second table compares `160M Recreation` against `160M MiniPile`.\n",
    "\n",
    "### 160M Pile-Deduplicated vs. 160M Recreation\n",
    "\n",
    "| Benchmark        | Measure    |     | 160M Pile Deduplicated | 160M Recreation             | Percentage Difference of Means | 95% Confidence Interval       | Interpretation                            |\n",
    "| ---------------- | ---------- | --- | ---------------------- | --------------------------- | ------------------------------ | ----------------------------- | ----------------------------------------- |\n",
    "| ARC-Challenge    | acc        | ↑   | **0.1997 ± 0.0117**    | 0.1894 ± 0.0115             | -5.1577                        | (0.0219; -0.0425)             | **Difference not significant**                |\n",
    "| MMLU             | acc        | ↑   | **0.2299 ± 0.0035**    | 0.2295 ± 0.0035             | -0.1740                        | (0.0093; -0.0101)             | **Difference not significant**                |\n",
    "| HellaSwag        | acc        | ↑   | **0.2903 ± 0.0045**    | 0.2604 ± 0.0044             | -10.2997                       | (-0.0176; -0.0422)            | Pile Deduplicated-trained better          |\n",
    "| WinoGrande       | acc        | ↑   | 0.4964 ± 0.0141        | **0.5122 ± 0.0140**         | 3.1829                         | (0.0547; -0.0231)             | **Difference not significant**                |\n",
    "| Lambada (OpenAI) | acc        | ↑   | **0.3689 ± 0.0067**    | 0.0000 ± 0.0000             | -100.0                         | (-0.3558; -0.3820)            | Pile Deduplicated-trained severely better |\n",
    "| Lambada (OpenAI) | perplexity | ↓   | **31.2589 ± 1.1594**   | 1854408.3999 ± 148101.5978  | 5932317.3272                   | (2144656.2727; 1564098.0093)  | Pile Deduplicated-trained severely better |\n",
    "| Lambada (Std)    | acc        | ↑   | **0.2335 ± 0.0059**    | 0.0000 ± 0.0000             | -100.0                         | (-0.2219; -0.2451)            | Pile Deduplicated-trained severely better |\n",
    "| Lambada (Std)    | perplexity | ↓   | **172.7619 ± 7.7265**  | 11927123.2514 ± 1063672.928 | 6903692.5905                   | (14011749.4290; 9842151.5500) | Pile Deduplicated-trained severely better |\n",
    "| BLiMP            | acc        | ↑   | **0.7294 ± 0.0015**    | 0.5481 ± 0.0017             | -24.8560                       | (-0.1769; -0.1857)            | Pile Deduplicated-trained better          |\n",
    "\n",
    "### 160 MiniPile vs. 160M Recreation\n",
    "\n",
    "| Benchmark        | Measure    |     | 160M MiniPile               | 160M Recreation                 | Percentage Difference of Means | 95% Confidence Interval         | Interpretation             |\n",
    "| ---------------- | ---------- | --- | --------------------------- | ------------------------------- | ------------------------------ | ------------------------------- | -------------------------- |\n",
    "| ARC-Challenge    | acc        | ↑   | **0.2125 ± 0.0120**         | 0.1894 ± <br>0.0115             | -10.8706                       | (0.0095; -0.0577)               | **Difference not significant** |\n",
    "| MMLU             | acc        | ↑   | **0.2699 ± 0.0037**         | 0.2295 ± 0.0035                 | -14.9685                       | (-0.0304; -0.0504)              | MiniPile-trained better    |\n",
    "| HellaSwag        | acc        | ↑   | 0.2560 ± 0.0044             | **0.2604 ± 0.0044**             | 1.7188                         | (0.0166; -0.0078)               | **Difference not significant** |\n",
    "| WinoGrande       | acc        | ↑   | 0.4720 ± 0.0140             | **0.5122 ± 0.0140**             | 8.5169                         | (0.0790; 0.0014)                | **Recreation better**          |\n",
    "| Lambada (OpenAI) | acc        | ↑   | 0.0000 ± 0.0000             | 0.0000 ± 0.0000                 | -                              | -                               | -                          |\n",
    "| Lambada (OpenAI) | perplexity | ↓   | 3033175.2693 ± 288926.5827  | **1854408.3999 ± 148101.5978**  | -38.8625                       | (-542407.4980; -1815126.2408)   | **Recreation severely better** |\n",
    "| Lambada (Std)    | acc        | ↑   | 0.0000 ± 0.0000             | 0.0000 ± 0.0000                 | -                              | -                               | -                          |\n",
    "| Lambada (Std)    | perplexity | ↓   | 27067951.3460 ± 2710040.191 | **11927123.2514 ± 1063672.928** | -55.9364                       | (-9434663.1814; -20846993.0080) | **Recreation severely better** |\n",
    "| BLiMP            | acc        | ↑   | 0.5194 ± 0.0018             | **0.5481 ± 0.0017**             | 5.5256                         | (0.0336; 0.0238)                | **Recreation better**          |\n",
    "\n",
    "Not only can we see a match with the original MiniPile across all but one (MMLU) benchmark, but we substantially increased the model's capability on both Lambada benchmarks when evaluated for perplexity, and achieved better results on WinoGrande and BLiMP. \n",
    "\n",
    "This is interesting, because we composed `MiniPile_Recreation` with the exact same amount of samples.<br>\n",
    "While this is the case, the dataset is ~500MB (in text, rest is additional idx column values + metadata) larger, but this difference, which itself can be explained by sampling and embedding expression deviations, has brought down perplexity by ~39% and ~56% respectively.\n",
    "\n",
    "**What does this noticable upwards trend on the perplexities imply?**\n",
    "I consider Lambada (OpenAI) and Lambada (Std) as benchmarks that aim to be useful for checking whether dataset reduction impacts capability to capture even subtle, general grammatical distinctions. The Lambadas realize this via evaluating grammatical knowledge across $67$ distinct linguistic phenomena/subtasks in English.\n",
    "It seems that the data sampled represents nuances of the English language better and in such a way that the trained model could generalize grammatical rules from across the identified clusters much more easily.\n",
    "\n",
    "I conclude that the selection and assembly of `MiniPile_Recreation` has been successful to that extent that it can be tried out for training on Pythia $1.4B$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Evaluate Pythia $1.4\\text{B}$ Pile vs. Pythia $1.4\\text{B}$ MiniPile (recreated)\n",
    "\n",
    "Once again, we will use an exact copy of the training and the test setup previously used for benchmarking Pythia $160\\text{M}$-Pile and $160\\text{M}$-MiniPile-Original.<br>\n",
    "Critically, parameters like the size-specific learning rate and system-wise accomodated batch size have been adjusted, while step count is kept the same.<br>\n",
    "Training is performed with the script `03_train_1.4B_recreation.py`.\n",
    "\n",
    "| Benchmark        | Measure    |     | 1.4B Pile Pretrained | 1.4B MiniPile Reproduction | Percentage Difference of Means | 95% Confidence Interval       | Interpretation                 |\n",
    "| ---------------- | ---------- | --- | -------------------- | -------------------------- | ------------------------------ | ----------------------------- | ------------------------------ |\n",
    "| ARC-Challenge    | acc        | ↑   | **0.2600 ± 0.0130**  | 0.1928 ± 0.0115            | -25.8462                       | (-0.0332; -0.1012)            | 1.4B Pile better               |\n",
    "| MMLU             | acc        | ↑   | **0.2388 ± 0.0036**  | 0.2295 ± 0.0035            | **-3.8945**                    | (0.0005; -0.0191)             | **Difference not significant** |\n",
    "| HellaSwag        | acc        | ↑   | **0.4177 ± 0.0049**  | 0.2584 ± 0.0044            | -38.1374                       | (-0.1464; -0.1722)            | 1.4B Pile better               |\n",
    "| WinoGrande       | acc        | ↑   | **0.5730 ± 0.0140**  | 0.5091 ± 0.0141            | **-11.1518**                   | (-0.0250; -0.1028)            | 1.4B Pile better               |\n",
    "| Lambada (OpenAI) | acc        | ↑   | **0.6202 ± 0.0068**  | 0.0000 ± 0.0000            | -100.0                         | (-0.6069; -0.6335)            | 1.4B Pile better               |\n",
    "| Lambada (OpenAI) | perplexity | ↓   | **6.1041 ± 0.1531**  | 1520707.8702 ± 115261.3664 | 24912792.4854                  | (1746614.0442; 1294789.4880)  | 1.4B Pile much better          |\n",
    "| Lambada (Std)    | acc        | ↑   | **0.4898 ± 0.0070**  | 0.0000 ± 0.0000            | -100.0                         | (-0.4761; -0.5035)            | 1.4B Pile better               |\n",
    "| Lambada (Std)    | perplexity | ↓   | **11.2448 ± 0.3305** | 8651201.8876 ± 735161.5236 | 76935033.4626                  | (10092107.2291; 7210274.0565) | 1.4B Pile much better          |\n",
    "| BLiMP            | acc        | ↑   | **0.8154 ± 0.0013**  | 0.5397 ± 0.0016            | -33.8116                       | (-0.2717; -0.2797)            | 1.4B Pile better               |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Improve the Dataset Creation Process\n",
    "\n",
    "The results and implementation specifics from the reproduction directly affect the investigation for improvements.<br>\n",
    "**The aim is to investigate different ideas for creating a new SuperMiniPile dataset that is ideally smaller and/or more information-retaining.**\n",
    "\n",
    "### Idea 1 - Cluster-Proportionate Sampling\n",
    "\n",
    "The original MiniPile dataset was created by sampling *equal* amounts of documents from each of the non-excluded clusters. This results in a MiniPile that cannot represent the original dataset's cluster distribution anymore, but rather imposes a uniform distribution across the clusters, no matter their size or importance/'weight'.\n",
    "\n",
    "For a first improvement attempt, named 'Proportionate', we keep as close to the reproduction code as possible. But, instead of sampling equal amounts of documents from each remaining cluster, we sample a proportionate amount of documents based on cluster sizes (by document count). This requires to make the amount of data points an upper bound rather than a fixed requirement, as we may not be able to sample the exact amount of documents from each cluster by their size. We just don't want to go over the MiniPile-document count.\n",
    "\n",
    "As a side note, the original script was observed as being relatively memory-demanding and, moreover, cache-guzzling.<br>\n",
    "The parquet data-retrieval process was to blame for that, and got thoroughly improved, ditching efforts to wrestle with Pandas (the library), replacing it with Numpy and using explicit cache management, which in turn lifted needs for deep copying.\n",
    "\n",
    "This idea's distillation script is implemented in `03_distill_pile_embed_idea_1_proportionate.py`.<br>\n",
    "Based on thus assembled $1,010,409$ documents, a Pythia $160\\text{M}$ was trained for evaluation.<br>\n",
    "This idea's trained model can be found here: [https://huggingface.co/Marcus2112/pythia-160m-minipile_proportionate](https://huggingface.co/Marcus2112/pythia-160m-minipile_proportionate)\n",
    "\n",
    "The benchmark results are as follows, compared to Pile, MiniPile and MiniPile Recreation:\n",
    "\n",
    "| Benchmark        | Measure    |     | 160M Pile Deduplicated | 160M MiniPile               | 160M Reproduction           | 160M Proportionate           |\n",
    "| ---------------- | ---------- | --- | ---------------------- | --------------------------- | --------------------------- | ---------------------------- |\n",
    "| ARC-Challenge    | acc        | ↑   | 0.1997 ± 0.0117        | **0.2125 ± 0.0120**         | 0.1894 ± 0.0115             | 0.1928 ± 0.0115              |\n",
    "| MMLU             | acc        | ↑   | 0.2299 ± 0.0035        | **0.2699 ± 0.0037**         | 0.2295 ± 0.0035             | 0.2295 ± 0.0035              |\n",
    "| HellaSwag        | acc        | ↑   | **0.2903 ± 0.0045**    | 0.2560 ± 0.0044             | 0.2604 ± 0.0044             | 0.2613 ± 0.0044              |\n",
    "| WinoGrande       | acc        | ↑   | 0.4964 ± 0.0141        | 0.4720 ± 0.0140             | **0.5122 ± 0.0140**         | 0.5051 ± 0.0141              |\n",
    "| Lambada (OpenAI) | acc        | ↑   | **0.3689 ± 0.0067**    | 0.0000 ± 0.0000             | 0.0000 ± 0.0000             | 0.0000 ± 0.0000              |\n",
    "| Lambada (OpenAI) | perplexity | ↓   | **31.2589 ± 1.1594**   | 3033175.2693 ± 288926.5827  | 1854408.3999 ± 148101.5978  | 2214257.4651 ± 184064.6008   |\n",
    "| Lambada (Std)    | acc        | ↑   | **0.2335 ± 0.0059**    | 0.0000 ± 0.0000             | 0.0000 ± 0.0000             | 0.0000 ± 0.0000              |\n",
    "| Lambada (Std)    | perplexity | ↓   | **172.7619 ± 7.7265**  | 27067951.3460 ± 2710040.191 | 11927123.2514 ± 1063672.928 | 15143084.5983 ± 1387627.8650 |\n",
    "| BLiMP            | acc        | ↑   | **0.7294 ± 0.0015**    | 0.5194 ± 0.0018             | 0.5481 ± 0.0017             | 0.5452 ± 0.0017              |\n",
    "\n",
    "The proportional sampling approach yields a dataset that is ~300 MB smaller than the Recreation dataset, while also containing $91$ entries less.<br>\n",
    "At the same time, proportional sampling yields close to equal results in all but the perplexity benchmarks, where it underperforms compared to the Recreation dataset, yet still beats the original MiniPile by a large margin.\n",
    "\n",
    "> We seem to have lost some highly informative examples from smaller clusters when scaling down their representation to match cluster proportions.\n",
    "\n",
    "*The proportional sampling approach is not a clear improvement over the recreation*, but rather a compromise between the original MiniPile and the Recreation dataset (tending towards the latter in results though). This is interesting, as it implies the uniform sampling approach may have been more effective in capturing the dataset's most speaking examples than the proportional sampling approach. I do not think that is a fault of the proportional sampling approach, but rather a symptom arising from the original dataset's composition. \n",
    "\n",
    "> Even though we incorporate the original dataset topology more directly, this doesn't necessarily come along with a better focus on the most informative examples.\n",
    "\n",
    "Proportional sampling is neither a failure nor a universal solution, and neither is uniform sampling, but the dismissal of either approach would be premature.<br>\n",
    "Instead, sampling styles should be context-dependent.<br>\n",
    "I assume that when performance on specific downstream tasks, such as language modeling, is stated as the primary goal, incorporating adaptive weighting mechanisms that emphasize the contribution of critical clusters might yield superior results.\n",
    "\n",
    "I conclude that, while itself not a clear improvement, the proportional sampling idea is valid for e.g. approaches where it is important to capture the original dataset's cluster distribution more closely. If the main goal is retaining performance, however, future ideas should investigate how to find a content-based weighting factor per cluster.\n",
    "\n",
    "### Idea 2 - Hybrid Loss-based Sampling\n",
    "\n",
    "In order to sample documents considered most representative and informative, the original MiniPile uses a one-shot proxy-based geometric sampling strategy. After all, the pipeline doesn't really 'select' documents by their content, but by their embedding's position in the cluster space, relative to other documents by proxy of the centroid.<br>\n",
    "Beyond that, once clusters have been determined and selected, the pipeline samples randomly across each cluster.<br>\n",
    "The comparison of different subset assembly techniques performed by [(Guo et al. 2022)](https://arxiv.org/abs/2204.08499) concludes that random sampling can be considered a very robust baseline for custom subset selection efforts, adaptable to various tasks.\n",
    "\n",
    "I deduct that employing cluster-wise random sampling, as performed already, while effective, fast and versatile, can not explicitly consider the point-by-point actual degree of informativeness. To an extent, we rely on luck and sampling spread for finding a best MiniPile distillate.\n",
    "\n",
    "Instead, what if we could utilize the heavy lifting by the embedding and clustering steps, while adding into the process an instance for information-based guiding of cluster sample factor weighting and specific document selection, which could thus address the findings of Idea 1 (Proportionate Sampling)?\n",
    "\n",
    "**Idea 2, called Lossi (Loss-informed Sampling)**, utilizes much of the existing pipeline for time and efficiency sake.\n",
    "\n",
    "Lossi as a whole consists of several adaptations.<br>\n",
    "The main idea is to use a small proxy model to determine the informativeness of each cluster and then sample documents from each cluster proportionate to their informativeness. We can do this at two points during dataset assembly, which is why idea 2.1 investiagtes the first point, and idea 2.2 the first with the second point combined. \n",
    "\n",
    "What do I mean by that?\n",
    "\n",
    "- Per cluster: Uniformly sample $n$ (e.g. $1,000$) documents and determine their loss with a small Pythia $70\\text{M}$ proxy model\n",
    "- Use the mean loss as a heuristic for the cluster's informativeness and weight the cluster's representation in the final dataset by this value\n",
    "\n",
    "Note that we select the smallest Pythia model trained half-way through the Pile, which assumes that the Pile is shuffled with regards to cluster assignments. I distinctly chose to use a small proxy model for this. MiniPile was intended for use in constrained academic settings, so we have to make do with small models that are most universally trainable, while not being too small as not to allow for their proxy use. Same goes for why the $70\\text{M}$ model is only trained to half of the full Pile dataset.\n",
    "\n",
    "The distillation script is implemented in `03_distill_pile_embed_idea_2_lossi_1.py` and `03_distill_pile_embed_idea_2_lossi_2.py`.<br>\n",
    "Based on the resulting dataset, a Pythia $160\\text{M}$ was trained for evaluation.<br>\n",
    "This idea's trained model can be found here: [https://huggingface.co/Marcus2112/pythia-160m-minipile_loss-sampled](https://huggingface.co/Marcus2112/pythia-160m-minipile_loss-sampled)\n",
    "\n",
    "The benchmark results are as follows, compared to the original MiniPile and the Recreation:\n",
    "\n",
    "| Benchmark        | Measure    |     | 160M MiniPile                | 160M Lossi                       | Percentage Difference of Means | 95% Confidence Interval         | Interpretation             |\n",
    "| ---------------- | ---------- | --- | ---------------------------- | -------------------------------- | ------------------------------ | ------------------------------- | -------------------------- |\n",
    "| ARC-Challenge    | acc        | ↑   | **0.2125 ± 0.0120**          | 0.1980 ± 0.0116                  | -6.8235                        | (0.0182; -0.0472)               | Different not significant  |\n",
    "| MMLU             | acc        | ↑   | **0.2699 ± 0.0037**          | 0.2295 ± 0.0035                  | -14.9685                       | (-0.0304; -0.0504)              | MiniPile-trained better    |\n",
    "| HellaSwag        | acc        | ↑   | 0.2560 ± 0.0044              | **0.2599 ± 0.0044**              | 1.5234                         | (0.0161; -0.0083)               | Different not significant  |\n",
    "| WinoGrande       | acc        | ↑   | 0.4720 ± 0.0140              | **0.5107 ± 0.0140**              | 8.1992                         | (0.0775; -0.0001)               | Difference not significant |\n",
    "| Lambada (OpenAI) | acc        | ↑   | 0.0000 ± 0.0000              | 0.0000 ± 0.0000                  | -                              | -                               | -                          |\n",
    "| Lambada (OpenAI) | perplexity | ↓   | 3033175.2693 ± 288926.5827   | **2116445.1732 ± 175403.0579**   | -30.2234                       | (-254247.7681; -1579212.4241)   | Lossi 1 severely better    |\n",
    "| Lambada (Std)    | acc        | ↑   | 0.0000 ± 0.0000              | 0.0000 ± 0.0000                  | -                              | -                               | -                          |\n",
    "| Lambada (Std)    | perplexity | ↓   | 27067951.3461 ± 2710040.1910 | **14896599.9251 ± 1366937.5470** | -44.9659                       | (-6222231.2223; -18120471.6197) | Lossi 1 severely better    |\n",
    "| BLiMP            | acc        | ↑   | 0.5194 ± 0.0018              | **0.5492 ± 0.0017**              | 5.7374                         | (0.0347; 0.0249)                | Lossi 1 better             |\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "| Benchmark        | Measure    |     | 160M Reproduction             | 160M Lossi                   | Percentage Difference of Means | 95% Confidence Interval      | Interpretation             |\n",
    "| ---------------- | ---------- | --- | -------------------------------- | ---------------------------- | ------------------------------ | ---------------------------- | -------------------------- |\n",
    "| ARC-Challenge    | acc        | ↑   | 0.1894 ± 0.0115                  | **0.1980 ± 0.0116**          | 4.5407                         | (0.0406; -0.0234)            | Difference not significant |\n",
    "| MMLU             | acc        | ↑   | 0.2295 ± 0.0035                  | 0.2295 ± 0.0035              | 0.0000                         | (0.0097; -0.0097)            | Difference not significant |\n",
    "| HellaSwag        | acc        | ↑   | **0.2604 ± 0.0044**              | 0.2599 ± 0.0044              | -0.1920                        | (0.0117; -0.0127)            | Difference not significant |\n",
    "| WinoGrande       | acc        | ↑   | **0.5122 ± 0.0140**              | 0.5107 ± 0.0140              | -0.2929                        | (0.0373; -0.0403)            | Difference not significant |\n",
    "| Lambada (OpenAI) | acc        | ↑   | 0.0000 ± 0.0000                  | 0.0000 ± 0.0000              | -                              | -                            | -                          |\n",
    "| Lambada (OpenAI) | perplexity | ↓   | **1854408.3999 ± 148101.5978**   | 2116445.1732 ± 175403.0579   | 14.1305                        | (711985.1414; -187911.5948)  | Difference not significant |\n",
    "| Lambada (Std)    | acc        | ↑   | 0.0000 ± 0.0000                  | 0.0000 ± 0.0000              | -                              | -                            | -                          |\n",
    "| Lambada (Std)    | perplexity | ↓   | **11927123.2514 ± 1063672.9280** | 14896599.9251 ± 1366937.5470 | 24.8968                        | (6364250.0618; -425296.7144) | Difference not significant |\n",
    "| BLiMP            | acc        | ↑   | 0.5481 ± 0.0017                  | **0.5492 ± 0.0017**          | 0.2007                         | (0.0058; -0.0036)            | Difference not significant |\n",
    "\n",
    "A clear improvement over the original MiniPile can be observed. However, this can't be attributed to the loss-informed sampling approach, as the Reproduction dataset is remarkably equal in performance. This implies that the measures taken unwittingly during the reproduction process pose a more effective set of improvements than the loss-informed sampling approach itself additionally provides, if it does at all. These measures include:\n",
    "\n",
    "- The embedding process, which during the Recreation uses a slightly worse model than the original MiniPile, but processes larger context sizes for the embeddings (1024 instead of 512)\n",
    "- The clustering process, which may induce different cluster formations based on the different embeddings\n",
    "- The interpretability inherent in the cluster exclusion process, which may have led to a more effective dataset\n",
    "\n",
    "This can imply that changes in the embedding and clustering process may be more effective than the loss-informed sampling approach itself.<br>\n",
    "The proxy model may not be able to capture the full complexity of the dataset, and thus the loss-informed sampling approach may not be able to capture the most informative examples. This in turn would put the loss-informed sampling approach at a disadvantage compared to the uniform sampling approach due to decreased efficiency.\n",
    "\n",
    "### Idea 3 - Density-based Proportionate Cluster Sampling\n",
    "\n",
    "An inherent danger with loss-based informativeness approximation is the potential for skewing the dataset towards harder examples, which could make the dataset less representative of general tasks. While loss can be considered a related indicator, it is not a direct measure of informativeness.\n",
    "\n",
    "Taking a step back with this insight, we should re-focus that MiniPile's goal is to capture the most representative subset of the 'insights' gainable from the original Pile dataset. To achieve that, we can look at diversity, which could improve generalization, especially when requiring broad informational coverage.\n",
    "\n",
    "Specifically, sampling inversely proportional to the cluster density will prioritize sparse regions of the dataset and thus the gaining of a broad coverage of the dataset's information. However, as lunch is never free, over-sampling/representing sparse clusters will likely introduce noise, especially when cluster sparsity correlates with low-quality examples. One could argue that we already excluded the most uninteresting, thus noise-inducing clusters, but the potential for noise itself is in no way mitigated (/mitigatable?). Density-based sampling therefore has to be understood as a trade-off between spread for representation and accidental capturing of noise.\n",
    "\n",
    "And still, density-based sampling could be helpful in capturing a most diverse set of examples, which in turn could be beneficial for generalization.\n",
    "But, we have to find a way to mitigate noise and over-representation at least to some extent.\n",
    "\n",
    "I therefore propose a density-based sampling approach that calculates cluster contribution proportions like so:\n",
    "$$\\text{Cluster Proportion} = \\frac{|C_i|}{|\\bigcup_{j} C_j|} \\cdot (1 - \\omega \\cdot \\rho(C_i))$$\n",
    "\n",
    "where $|C_i|$ is the number of documents in cluster $i$, $|\\bigcup_{j} C_j|$ is the total number of documents in all clusters, and $\\rho(C_i)$ is the density of cluster $i$. The impact of the density is scaled by the hyperparameter $\\omega$, reducing the factor of over-representation of thoroughly sparse clusters.<br>\n",
    "I set $\\omega = 0.5$ with the intention of having neither cluster size nor density completely dominate the proportion calculation.\n",
    "\n",
    "The trained model can be found here: [https://huggingface.co/Marcus2112/pythia-160m-minipile_density-proportioned](https://huggingface.co/Marcus2112/pythia-160m-minipile_density-proportioned)\n",
    "\n",
    "These results emerge when comparing benchmarks against Pythia $160\\text{M}$ MiniPile and Pythia $160\\text{M}$ MiniPile Reproduction:\n",
    "\n",
    "| Benchmark        | Measure    |     | 160M MiniPile                | 160M Density                     | Percentage Difference of Means | 95% Confidence Interval         | Interpretation              |\n",
    "| ---------------- | ---------- | --- | ---------------------------- | -------------------------------- | ------------------------------ | ------------------------------- | --------------------------- |\n",
    "| ARC-Challenge    | acc        | ↑   | **0.2125 ± 0.0120**          | 0.1920 ± 0.0115                  | -9.6471                        | (0.0121; -0.531)                | Difference not significant  |\n",
    "| MMLU             | acc        | ↑   | **0.2699 ± 0.0037**          | 0.2295 ± 0.0035                  | -14.9685                       | (-0.0304; -0.0504)              | MiniPile better             |\n",
    "| HellaSwag        | acc        | ↑   | 0.2560 ± 0.0044              | **0.2604 ± 0.0044**              | 1.7188                         | (0.0166; -0.0078)               | Difference not significant  |\n",
    "| WinoGrande       | acc        | ↑   | 0.4720 ± 0.0140              | **0.5201 ± 0.0140**              | 10.1907                        | (0.0869; 0.0093)                | **Density better**          |\n",
    "| Lambada (OpenAI) | acc        | ↑   | 0.0000 ± 0.0000              | 0.0000 ± 0.0000                  | -                              | -                               | -                           |\n",
    "| Lambada (OpenAI) | perplexity | ↓   | 3033175.2693 ± 288926.5827   | **2099002.0912 ± 170652.6222**   | -30.7985                       | (-276474.4857; -1591871.8705)   | **Density severely better** |\n",
    "| Lambada (Std)    | acc        | ↑   | 0.0000 ± 0.0000              | 0.0000 ± 0.0000                  | -                              | -                               | -                           |\n",
    "| Lambada (Std)    | perplexity | ↓   | 27067951.3461 ± 2710040.1910 | **13347273.6076 ± 1997894.6360** | -50.6898                       | (-7121587.1522; -20319768.3248) | **Density severely better** |\n",
    "| BLiMP            | acc        | ↑   | 0.5194 ± 0.0018              | **0.5501 ± 0.0017**              | 5.9107                         | (0.0356; 0.0258)                | **Density better**          |\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "| Benchmark        | Measure    |     | 160M Reproduction                | 160M Density                 | Percentage Difference of Means | 95% Confidence Interval       | Interpretation             |\n",
    "| ---------------- | ---------- | --- | -------------------------------- | ---------------------------- | ------------------------------ | ----------------------------- | -------------------------- |\n",
    "| ARC-Challenge    | acc        | ↑   | 0.1894 ± 0.0115                  | **0.1920 ± 0.0115**          | 1.3728                         | (0.0345; -0.0293)             | Difference not significant |\n",
    "| MMLU             | acc        | ↑   | 0.2295 ± 0.0035                  | 0.2295 ± 0.0035              | 0.0000                         | (0.0097; -0.0097)             | Difference not significant |\n",
    "| HellaSwag        | acc        | ↑   | 0.2604 ± 0.0044                  | 0.2604 ± 0.0044              | 0.0000                         | (0.0122; -0.0122)             | Difference not significant |\n",
    "| WinoGrande       | acc        | ↑   | 0.5122 ± 0.0140                  | **0.5201 ± 0.0140**          | 1.5424                         | (0.0467; -0.0309)             | Difference not significant |\n",
    "| Lambada (OpenAI) | acc        | ↑   | 0.0000 ± 0.0000                  | 0.0000 ± 0.0000              | -                              | -                             | -                          |\n",
    "| Lambada (OpenAI) | perplexity | ↓   | **1854408.3999 ± 148101.5978**   | 2099002.0912 ± 170652.6222   | 13.1899                        | (687468.6952; -198281.3126)   | Difference not significant |\n",
    "| Lambada (Std)    | acc        | ↑   | 0.0000 ± 0.0000                  | 0.0000 ± 0.0000              | -                              | -                             | -                          |\n",
    "| Lambada (Std)    | perplexity | ↓   | **11927123.2514 ± 1063672.9280** | 13347273.6076 ± 1997894.6360 | 11.9069                        | (5856415.8784; -3016115.1660) | Difference not significant |\n",
    "| BLiMP            | acc        | ↑   | 0.5481 ± 0.0017                  | **0.5501 ± 0.0017**          | 0.3649                         | (0.0067; -0.0027)             | Difference not significant |\n",
    "\n",
    "Other than the loss-based approach and except for the perplexity benchmarks, the density-based sampling approach is on par or marginally better than the Reproduction and thus noticably better than the original MiniPile. This implies that the density-based sampling approach is a valid improvement over the original MiniPile, but not necessarily over the Reproduction. This in turn implies that the Reproduction pipeline itself, as was the case with the loss-based sampling approach, is the largest contributor to the improvements seen in the benchmarks. Although and all be it statistically insignificant, the density-based sampling approach brings a slight improvement over the Reproduction in some areas.\n",
    "\n",
    "**However**, contrary to slightest improvements or equal results seen in the benchmarks, I must conclude that this approach is a real improvement.<br>\n",
    "This is because while benchmarks show near equal results between the Reproduction and the Density-based approach, the datasets themselves are not of the same size:\n",
    "- MiniPile Reproduction: $1,010,500$ documents at $3.76$ GB,\n",
    "- MiniPile Density-Proportioned: $946,465$ documents at $3.25$ GB.\n",
    "\n",
    "The dataset contains $64,035$ documents less (~6.34%), which amount to $0.51$ GB (~13.56%) less of a footprint.<br>\n",
    "(For the training split specifically, this means a reduction from $1,000,000$ to $936,630$, meaning $63,370$ documents less, or $~6.34%$)\n",
    "Still, it produces slightly better or equal results (except for perplexity) compared to the Reproduction and much better results (except on MMLU) than the original MiniPile. Therefore, I conclude that applying weighted density-based sampling is indeed a valid improvement over the Reproduction and the original distilled subset, at least for The Pile dataset as basis.\n",
    "\n",
    "### Idea 3.1 - Changing $\\omega$\n",
    "\n",
    "The above described density-based sampling approach uses a hyperparameter $\\omega$ to scale the impact of the density on the cluster proportion calculation like so:\n",
    "$$\\text{Cluster Proportion} = \\frac{|C_i|}{|\\bigcup_{j} C_j|} \\cdot (1 - \\omega \\cdot \\rho(C_i))$$\n",
    "\n",
    "In the above attempt, $\\omega$ was set to $0.5$ with an intent of balancing the impact of cluster size and density. However, this value was chosen arbitrarily and may not be optimal. Thus, the impact of choosing a different value for $\\omega$ should be investigated for potential improvements and further insights.\n",
    "\n",
    "The higher we set $\\omega$, the more we increase the impact of the density on the cluster proportion calculation. In theory, this could lead to a more diverse dataset, but also to a higher risk of over-representing sparse clusters and thus introducing noise.\n",
    "\n",
    "We've seen the results of cluster-proportionate sampling, i.e. $\\omega = 0$, and density-based sampling, i.e. $\\omega = 0.5$. Now, we will investigate the impact of setting $\\omega = 0.75$ on the dataset creation process.\n",
    "The trained model can be found here: [https://huggingface.co/Marcus2112/pythia-160m-minipile_low-density](https://huggingface.co/Marcus2112/pythia-160m-minipile_low-density)\n",
    "\n",
    "The benchmark results are as follows, where $\\omega = 0.5$ is called 'Density' and $\\omega = 0.75$ is called 'Low Density':\n",
    "\n",
    "| Benchmark        | Measure    |     | 160M Density                 | 160M Low Density             | Percentage Difference of Means | 95% Confidence Interval       | Interpretation             |\n",
    "| ---------------- | ---------- | --- | ---------------------------- | ---------------------------- | ------------------------------ | ----------------------------- | -------------------------- |\n",
    "| ARC-Challenge    | acc        | ↑   | 0.1920 ± 0.0115              | 0.1886 ± 0.0114              | -1.7708                        | (0.0283; -0.0351)             | Difference not significant |\n",
    "| MMLU             | acc        | ↑   | 0.2295 ± 0.0035              | 0.2295 ± 0.0035              | 0.0000                         | -                             | -                          |\n",
    "| HellaSwag        | acc        | ↑   | 0.2604 ± 0.0044              | 0.2508 ± 0.0044              | -3.6866                        | (0.0026; -0.0218)             | Difference not significant |\n",
    "| WinoGrande       | acc        | ↑   | 0.5201 ± 0.0140              | 0.5067 ± 0.0141              | -2.5764                        | (0.0255; -0.0523)             | Difference not significant |\n",
    "| Lambada (OpenAI) | acc        | ↑   | 0.0000 ± 0.0000              | 0.0000 ± 0.0000              | 0.0000                         | -                             | -                          |\n",
    "| Lambada (OpenAI) | perplexity | ↓   | 2099002.0912 ± 170652.6222   | 2287598.5548 ± 192724.6151   | 8.8951                         | (693139.8095; -315946.8823)   | Difference not significant |\n",
    "| Lambada (Std)    | acc        | ↑   | 0.0000 ± 0.0000              | 0.0000 ± 0.0000              | 0.0000                         | -                             | -                          |\n",
    "| Lambada (Std)    | perplexity | ↓   | 13347273.6076 ± 1997894.6360 | 16223747.0588 ± 1503858.3054 | 21.5510                        | (7777717.0232; -2024770.1208) | Difference not significant |\n",
    "| BLiMP            | acc        | ↑   | 0.5501 ± 0.0017              | 0.5504 ± 0.0170              | 0.0545                         | (0.0338; -0.0332)             | Difference not significant |\n",
    "\n",
    "Apart from BLiMP results, all benchmark report insignificantly lower scores for the 'Low Density' dataset compared to the 'Density' dataset.<br>\n",
    "This implies that overly enforcing sampling from low-density clusters may hurt overall dataset representation and thus generalization abilities. The 'Density' dataset, with $\\omega = 0.5$, is therefore the better choice of these two approaches for the dataset creation process.\n",
    "\n",
    "### Idea 4 - Increasing $k$ for Clustering\n",
    "\n",
    "Given the results of ideas 1 to 3, we can see that the reproduction in itself already is a strong improvement over the original MiniPile. It is such a strong improvement in fact, that the sampling and arrangement ideas seemingly only marginally improve the dataset over the reproduction, if at all. This implies that the clustering process itself should be investigated for potential improvements.\n",
    "\n",
    "Revisiting the K-Means clustering process, it became evident that the number of clusters $k$ chosen by the paper was chosen somewhat arbitrarily. While the deduplicated Pile consists of $220$ data subsets, this doesn't mean that $k = 220$ is the optimal choice for the clustering process, neglecting e.g. the potential for capturing of finer group structures within subsets, which would in turn enable more detailed document selection.\n",
    "\n",
    "To investigate if a higher number of clusters could lead to a more representative dataset, $k$ was increased from $220$ to $440$.\n",
    "We can't however couple the doubling of $k$ with the expectation of doubling the number of clusters to be excluded. Clusters may be excluded only if they fit to the paper's original criteria. By these measures, a total of $70$ clusters were excluded from the dataset. Other parameters were kept unchanged.\n",
    "\n",
    "For dataset distillation, the reproduction code with an increased $k$ value was used.<br>\n",
    "The trained model can be found here: \n",
    "\n",
    "The benchmark results are as follows, where $k = 220$ is called 'Reproduction' and $k = 440$ is called 'Increased $k$':\n",
    "\n",
    "### Cancelled Ideas\n",
    "\n",
    "The project's constraints for time and resource availability needed to be considered during the pursuit of different improvement ideas.\n",
    "As MiniPile itself is designed purposefully to be applied in academic settings, ideally, its creation should be feasible within this same setting as well.\n",
    "However, techniques like HDBSCAN clustering, which could potentially improve the clustering process, require more time and resources than could be deemed feasible for this project. The following ideas have been cancelled due to these constraints, but have been added in the `cancelled_ideas` folder and may be worth investigating in future projects:\n",
    "\n",
    "- **HDBSCAN clustering:** Replacing K-Means with HDBSCAN could help in identifying complex-shaped clusters, as well as outliers\n",
    "- **Lossi 3:** The loss-based sampling approach could have been improved further by using the proxy model during a second loss-based selection strategy. Where the cluster proportions where governed by losses already, the samples themselves could have been, too. For that, we would sample $1.5\\times$ the proportion per cluster and measure each example's loss with the proxy. From that, from a list sorted by loss in descending order, we would pick the top as $0.8\\times$ of the proportion and the lower as $0.2\\times$ of the proportion. This would have been a more fine-grained approach to the loss-based sampling idea, but loss itself is was deemed not a suitable measure, because it tends to favor complexity over information content, even with the $0.2\\times$ lower complexity proportion.\n",
    "- **Density-based sampling with $\\omega = 0.25$:** This approach was discarded to save time. In effect, the cluster size-proportioned sampling approach provided a reference for an $\\omega = 0.00$ setting already. More interest was therefore put into lower-density-based sampling, resulting in the tests with $\\omega=0.75$.\n",
    "\n",
    "### Theoretical Ideas\n",
    "\n",
    "While the above ideas were cancelled during the implementation phase, others were put aside during brainstorming for similar reasons (feasibility within constraints).\n",
    "\n",
    "#### Sparse Sampling of Embeddings with Similarity Hashing\n",
    "\n",
    "This idea arose from the Lossi 3 cancellation and aims to improve the dataset creation process specifically without the need for a proxy model. This idea applies Locality Sensitive Hashing to the clusters resulting from the K-Means clustering step. Per cluster, LSH would hash embeddings into buckets based on similarity, potentially capturing more detailed relationships between documents than would be possible by K-Means clusters alone. Note that I would keep K-Means in place and would not replace it with LSH, as K-Means poses an overall low-cost grouping option allowing for human-guided filtering. Also, using LSH on cluster-level may help with resource usage. (This choice can admittedly be seen as a shortcoming as we don't leverage LSH's full potential of capturing relationships even across clusters, yet clustwe-wise handling would be more likely to execute.)\n",
    "\n",
    "Per cluster, LSH hashes embeddings into similarity-based buckets (e.g. $16$ buckets, arbitrarily). We'd attain $16$ sub-clusters per cluster by similarity and it is across these that we would sample from the cluster. Sampling could thus be more informed of inner-cluster topology. Still, sampling itself would not solely rely on LSH, but treat it as one of three configurably weighted factors:\n",
    "\n",
    "- **Proportional Sampling:** As in Idea 1, we would sample documents proportional to cluster sizes, enabling capture of original dataset distribution.\n",
    "- **Random Exploration:** Introduces 'total' inner-cluster randomness to capture some potentially unique or median examples.\n",
    "- **Semantic Diversity Sampling:** Prioritizes semantically diverse examples using LSH-based similarity groups, aiming at capturing more detailed relationships between documents.\n",
    "\n",
    "Rest of assembly would equal the implemented ideas.<br>\n",
    "I've read that the python framework `faiss` is praised for being efficient in LSH-based similarity hashing. Maybe this would be a good starting point.<br>\n",
    "This is what I stumbled across while researching LSH and its applications and what might be interesting:\n",
    "- https://www.pinecone.io/learn/series/faiss/locality-sensitive-hashing/\n",
    "- https://arxiv.org/pdf/2208.05648\n",
    "- https://arxiv.org/pdf/1408.2927\n",
    "- https://mediatum.ub.tum.de/doc/1655492/ptz317jfxiatpjlwxrpcuimu1.2022-11-01_einreichung_mediatum_titelblatt_neu.pdf (although not as directly related I fear)\n",
    "\n",
    "#### Double-Proxied Cross-Entropy-based Sampling\n",
    "\n",
    "This idea would be closely aligned with the findings of [Extracting representative subset from extensive text data for\n",
    "training pre-trained language models (Suzuki, et al. 2023)](https://www.sciencedirect.com/science/article/pii/S0306457322003508). The authors claim that \"[...] the representative subset obtained using [a] likelihood difference score can achieve the 90% performance level even when the size of the dataset is reduced to approximately two to three orders of magnitude smaller than the original dataset\".<br>\n",
    "\n",
    "The approach scores and ranks data samples based on the likelihood difference between two differently pretrained language models (PreLMs):\n",
    "1. An in-domain PreLM, trained on domain-specific data, i.e. a conceptually interrelated subset of the larger dataset\n",
    "2. A non-domain PreLM, trained on a general domain, i.e. the full dataset\n",
    "\n",
    "For each document $X$, the likelihood difference score $S(X)$ is calculated as:\n",
    "$$S(X)=\\overline{H}^{(I)}(X) - \\overline{H}^{(N)}(X)$$\n",
    "$\\overline{H}^{(I)}$ and $\\overline{H}^{(N)}$ are the normalized per-word cross-entropy losses from in-domain PreLM and non-domain PreLM, respectively.\n",
    "\n",
    "Essentially, we would:\n",
    "1. Calculate $S(X)$ for all documents in the dataset.\n",
    "2. Rank documents in ascending order of $S(X)$.\n",
    "3. Deduplicate redundant samples (I'd argue this is not necessary for The Pile Deduplicated).\n",
    "4. Selecting the top-$K$ ranked samples as representative dataset (RepSet).\n",
    "\n",
    "It would be interesting to follow up on the paper's claims, but I'd fear this is a resource-intense and in the end still loss-based approach, overrepresenting compliated and not representing/informative documents in the final assembly. Further, this approach poses a catch 22, because in order to train more cost-effective, we have to train the non-domain PreLM on the full dataset. For a constrained academic setting, this might not be possible, even if the proxy model size is downscaled.\n",
    "\n",
    "#### Semantic Deduplication as Post-Processing for the distilled dataset\n",
    "\n",
    "An idea for a post-processing step would be to analyze the now smaller subset for semantic duplicates. I'd suggest the works of [SemDeDup: Data-efficient learning at web-scale through semantic deduplication (Abbas, et al. 2023)](https://arxiv.org/pdf/2303.09540) for this semantic deduplication. It is claimed that for the LAION dataset, \"[...] SemDeDup can remove 50% of the data with minimal performance loss\". This could be a valuable step to further reduce the dataset size without losing performance.\n",
    "\n",
    "While this idea is interesting, I was hesitant to allocate resources to it, mostly because reducing the distillate solely by the perspective of semantic similarity might remove syntactically informative, yet semantically similar examples. This could lead to a loss of diversity in the dataset, which is not necessarily desirable. However, testing could be done relatively flexibly, as this can be put into action post-assembly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Evaluate Pythia $1.4\\text{B}$ Pretrained vs. Pythia $1.4\\text{B}$ SuperMiniPile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "def average_pool(last_hidden_states, attention_mask):\n",
    "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_embedding(text, tokenizer, model):\n",
    "    inputs = tokenizer(f\"query: {text}\", max_length=1024, padding=\"max_length\", truncation=True, return_tensors='pt')\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = average_pool(outputs.last_hidden_state, inputs['attention_mask'])\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "    return embeddings.cpu().numpy()[0]\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_path = \"/mnt/data/e5-base-4k\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n",
    "model = AutoModel.from_pretrained(model_path, attn_implementation=\"sdpa\")\n",
    "model.eval()\n",
    "model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Directory containing parquet files\n",
    "parquet_dir = Path(\"/mnt/data/Pile_Deduplicated_Embd\")\n",
    "\n",
    "for i in range(3):\n",
    "    parquet_file = parquet_dir / f\"shard_{i:09d}.parquet\"\n",
    "    if not parquet_file.exists():\n",
    "        print(f\"File {parquet_file} not found. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    dataset = load_dataset(\"parquet\", data_files=str(parquet_file))[\"train\"]\n",
    "    print(f\"Iterating over shard {i}....\")\n",
    "    for entry in tqdm(dataset):\n",
    "        stored_embedding = np.array(entry['embedding'])\n",
    "        text = entry['text']\n",
    "        # Generate new embedding\n",
    "        new_embedding = get_embedding(text, tokenizer, model)\n",
    "        # Convert both for fp16 precision\n",
    "        stored_embedding_fp16 = stored_embedding.astype(np.float16)\n",
    "        new_embedding_fp16 = new_embedding.astype(np.float16)\n",
    "        cosine_similarity = np.dot(stored_embedding_fp16, new_embedding_fp16) / (np.linalg.norm(stored_embedding_fp16) * np.linalg.norm(new_embedding_fp16))\n",
    "        # 1.0 as in identical, 0.0 as in orthogonal\n",
    "        if cosine_similarity != 1.0:\n",
    "            print(f\"Text: {text}\")\n",
    "            print(f\"Cosine similarity: {cosine_similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Specify the path to your Parquet file\n",
    "data_files_1 = {\"train\": \"/vol/tmp/koppelmm/Pile_Deduplicated_Embd/shard_000000001.parquet\"}\n",
    "data_files_2 = {\"train\": \"/vol/tmp/koppelmm/Pile_Deduplicated_Embd/shard_000000002.parquet\"}\n",
    "\n",
    "# Load the dataset\n",
    "dataset_1 = load_dataset(\"parquet\", data_files=data_files_1, split=\"train\")\n",
    "dataset_2 = load_dataset(\"parquet\", data_files=data_files_2, split=\"train\")\n",
    "\n",
    "# Print the number of entries in the dataset\n",
    "print(f\"Number of entries in shard 0: {len(dataset_1)} ({128*8192})\")\n",
    "print(f\"Number of entries in shard 1: {len(dataset_2)} ({128*8192})\")\n",
    "\n",
    "# Print the first entry from each dataset\n",
    "first_entry_1 = dataset_1[0]\n",
    "first_entry_2 = dataset_2[0]\n",
    "\n",
    "print(\"First entry in shard 0:\")\n",
    "print(first_entry_1)\n",
    "\n",
    "print(\"First entry in shard 1:\")\n",
    "print(first_entry_2)\n",
    "\n",
    "# Check if the first entries are different\n",
    "if first_entry_1 == first_entry_2:\n",
    "    print(\"The first entries are identical.\")\n",
    "else:\n",
    "    print(\"The first entries are different.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Unexpected heap contents: [{'value': 95}, {'value': 96}, {'value': 97}, {'value': 98}, {'value': 99}] vs. expected [{'value': 5}, {'value': 6}, {'value': 7}, {'value': 8}, {'value': 9}]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 36\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m result \u001b[38;5;241m==\u001b[39m expected, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected heap contents: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll tests passed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m \u001b[43mtest_update_heap\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 21\u001b[0m, in \u001b[0;36mtest_update_heap\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m result \u001b[38;5;241m=\u001b[39m [entry[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(heap, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m0\u001b[39m])]\n\u001b[1;32m     19\u001b[0m expected \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m: i} \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m)]\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m result \u001b[38;5;241m==\u001b[39m expected, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected heap contents: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m vs. expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Test descending order heap (keep smallest items based on `value`)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m heap \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mAssertionError\u001b[0m: Unexpected heap contents: [{'value': 95}, {'value': 96}, {'value': 97}, {'value': 98}, {'value': 99}] vs. expected [{'value': 5}, {'value': 6}, {'value': 7}, {'value': 8}, {'value': 9}]"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "\n",
    "def update_heap(heap, item, key_func, max_size=5, reverse=False):\n",
    "    key = key_func(item)\n",
    "    heapq.heappush(heap, (key if not reverse else -key, item))\n",
    "    if len(heap) > max_size:\n",
    "        heapq.heappop(heap)\n",
    "\n",
    "def test_update_heap():\n",
    "    # Test ascending order heap (keep largest items based on `value`)\n",
    "    heap = []\n",
    "    items = [{'value': i} for i in range(100)]  # Items with values from 0 to 9\n",
    "\n",
    "    for item in items:\n",
    "        update_heap(heap, item, key_func=lambda x: x['value'], max_size=5, reverse=False)\n",
    "\n",
    "    # Extract the final items from the heap, sorted by `value`\n",
    "    result = [entry[1] for entry in sorted(heap, key=lambda x: x[0])]\n",
    "    expected = [{'value': i} for i in range(5, 10)]\n",
    "\n",
    "    assert result == expected, f\"Unexpected heap contents: {result} vs. expected {expected}\"\n",
    "\n",
    "    # Test descending order heap (keep smallest items based on `value`)\n",
    "    heap = []\n",
    "    for item in items:\n",
    "        update_heap(heap, item, key_func=lambda x: x['value'], max_size=5, reverse=True)\n",
    "\n",
    "    # Extract the final items from the heap, sorted by `value`\n",
    "    result = [entry[1] for entry in sorted(heap, key=lambda x: x[0], reverse=True)]\n",
    "    expected = [{'value': i} for i in range(5)]\n",
    "\n",
    "    assert result == expected, f\"Unexpected heap contents: {result}\"\n",
    "\n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "test_update_heap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import cpu_count\n",
    "\n",
    "print(cpu_count() // 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minipile",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
